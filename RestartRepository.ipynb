{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # if this isn't installed, use pip install beautifulsoup4\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "import progressbar # if this isn't installed, use pip install progressbar2\n",
    "import random\n",
    "from selenium import webdriver # if not installed, do pip install selenium\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateProxies():\n",
    "    # Get list of US-based proxy IPs and ports using selenium\n",
    "\n",
    "    IPurl = \"https://www.us-proxy.org/\" # <-- the robots.txt file for this site allows full access for all user-agents\n",
    "\n",
    "    # Specify incognito options for Chrome\n",
    "    option = webdriver.ChromeOptions()\n",
    "    option.add_argument(\"--incognito\")\n",
    "\n",
    "    # Create new Chrome instance\n",
    "    browser = webdriver.Chrome(options=option)\n",
    "\n",
    "    # Minimize window\n",
    "    browser.minimize_window()\n",
    "\n",
    "    # Go to desired website\n",
    "    IPurl = \"https://www.us-proxy.org/\" # <-- the robots.txt file for this site allows full access for all user-agents\n",
    "    browser.get(IPurl)\n",
    "\n",
    "    # Filter by https only\n",
    "    https_button = browser.find_elements_by_xpath(\"//*[@id='proxylisttable']/tfoot/tr/th[7]/select/option[3]\")[0]\n",
    "    https_button.click()\n",
    "\n",
    "    # Set to 80 results\n",
    "    maxnum_button = browser.find_elements_by_xpath(\"//*[@id='proxylisttable_length']/label/select/option[3]\")[0]\n",
    "    maxnum_button.click()\n",
    "\n",
    "    # Grab IP's and Ports from the resulting table\n",
    "    rows = browser.find_elements_by_xpath(\"//*[@id='proxylisttable']/tbody/tr\")\n",
    "\n",
    "    proxies = set() # using a set ensures there aren't duplicates\n",
    "    for row in rows:\n",
    "        row = row.text.split(' ')\n",
    "\n",
    "        if row[3].strip().lower() != 'transparent': # don't want to include our real proxy when navigating KSL\n",
    "            proxies.add(''.join(['http://', ':'.join([row[0].strip(), row[1].strip()])]))\n",
    "\n",
    "    # Close browser when done\n",
    "    browser.close()\n",
    "\n",
    "    return proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function for the scraping done for each search page\n",
    "\n",
    "def carscraper(**kwargs):\n",
    "    '''VARIABLE INPUTS:\n",
    "    url: should be of the form \"https://cars.ksl.com/search/newUsed/Used;Certified/perPage/96/page/0\"\n",
    "    rooturl: should be something like \"https://cars.ksl.com\"\n",
    "    maxts: the maximum timestamp of the all_cars repository\n",
    "    use_proxy: a boolean or binary to indicate if a proxy should be used\n",
    "    curr_proxy: a string indicating the current proxy IP from last function call\n",
    "    proxydict: a dictionary of proxy IPs and associated user-agents to cycle through\n",
    "    refreshmin: the number of minutes to wait before updating the proxy pool\n",
    "    \n",
    "    ***NOTE: This function is meant to work with a pool of proxy IPs and a various spoofed user-agents'''\n",
    "    \n",
    "    # Need to spoof a user-agent in order to get past crawler block\n",
    "    user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36'\n",
    "    \n",
    "    # the following were pulled manually on 3/12/20 from https://www.whatismybrowser.com/guides/the-latest-user-agent/\n",
    "    user_agents = ['Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36',\n",
    "                   'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36',\n",
    "                   'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36',\n",
    "                   'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:54.0) Gecko/20100101 Firefox/74.0',\n",
    "                   'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.13; rv:61.0) Gecko/20100101 Firefox/74.0',\n",
    "                   'Mozilla/5.0 (X11; Linux i586; rv:31.0) Gecko/20100101 Firefox/74.0',\n",
    "                   'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0 Safari/605.1.15',\n",
    "                   'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36 Edg/80.0.361.62',\n",
    "                   'Mozilla/5.0 (Windows NT 10.0; Trident/7.0; rv:11.0) like Gecko']    \n",
    "    \n",
    "    \n",
    "    # Parse the kwargs\n",
    "    \n",
    "    \n",
    "    if 'url' in kwargs.keys():\n",
    "        if isinstance(kwargs['url'],str):\n",
    "            url = kwargs['url']\n",
    "        else:\n",
    "            raise TypeError(f'Expected string for url but got {type(kwargs[\"url\"])}.')\n",
    "    else:\n",
    "        raise ValueError('url is a required input for carscraper().')\n",
    "        \n",
    "    if 'rooturl' in kwargs.keys():\n",
    "        if isinstance(kwargs['rooturl'],str):\n",
    "            rooturl = kwargs['rooturl']\n",
    "        else:\n",
    "            raise TypeError(f'Expected string for rooturl but got {type(kwargs[\"rooturl\"])}.')\n",
    "    else:\n",
    "        raise ValueError('rooturl is a required input for carscraper().')\n",
    "        \n",
    "    if 'maxts' in kwargs.keys():\n",
    "        if isinstance(kwargs['maxts'],np.int64) or isinstance(kwargs['maxts'],int):\n",
    "            maxts = kwargs['maxts']\n",
    "        else:\n",
    "            raise TypeError(f'Expected np.int64 or int for maxts but got {type(kwargs[\"maxts\"])}.')\n",
    "    else:\n",
    "        raise ValueError('maxts is a required input for carscraper().')\n",
    "        \n",
    "    if 'use_proxy' in kwargs.keys():\n",
    "        if isinstance(kwargs['use_proxy'],int) or isinstance(kwargs['use_proxy'],bool):\n",
    "            use_proxy = kwargs['use_proxy']\n",
    "        else:\n",
    "            raise TypeError(f'Expected int or bool for use_proxy but got {type(kwargs[\"use_proxy\"])}.')\n",
    "    else:\n",
    "        # default is to NOT use proxy\n",
    "        use_proxy = False\n",
    "        \n",
    "    if use_proxy:\n",
    "        # The following inputs are only useful when using a proxy\n",
    "        \n",
    "        if 'proxydict' in kwargs.keys():\n",
    "            if isinstance(kwargs['proxydict'],dict):\n",
    "                proxydict = kwargs['proxydict']\n",
    "            else:\n",
    "                print(f'Expected dict type for proxydict but got {type(kwargs[\"proxydict\"])}. Generating new proxydict...')\n",
    "                newproxies = generateProxies()\n",
    "                proxydict = {i:random.choice(user_agents) for i in newproxies}\n",
    "        else:\n",
    "            print('No proxydict found. Generating...')\n",
    "            newproxies = generateProxies()\n",
    "            proxydict = {i:random.choice(user_agents) for i in newproxies}\n",
    "\n",
    "        if 'refreshmin' in kwargs.keys():\n",
    "            if isinstance(kwargs['refreshmin'],int) or isinstance(kwargs['refreshmin'],float):\n",
    "                refreshmin = kwargs['refreshmin']\n",
    "            else:\n",
    "                refreshmin = 15\n",
    "                print(f'Expected int or float for refreshmin but got {type(kwargs[\"refreshmin\"])}. Set to default value of {refreshmin}.')\n",
    "        else:\n",
    "            refreshmin = 15\n",
    "            print(f'No refreshmin found. Set to default value of {refreshmin}.')\n",
    "        \n",
    "    \n",
    "    \n",
    "    if use_proxy:\n",
    "        tstart = time.time() # set a start time to use for refreshing proxy list (if needed)    \n",
    "\n",
    "        if 'currproxy' in kwargs.keys():\n",
    "            if isinstance(kwargs['currproxy'],str):\n",
    "                currproxy = kwargs['currproxy']\n",
    "            else:\n",
    "                proxy_pool = cycle(proxydict) # make a pool of proxies \n",
    "                currproxy = next(proxy_pool) # grab the next proxy in cycle\n",
    "        else:\n",
    "            proxy_pool = cycle(proxydict) # make a pool of proxies \n",
    "            currproxy = next(proxy_pool) # grab the next proxy in cycle                \n",
    "\n",
    "\n",
    "        attempts = len(proxydict) # for now, limit the total number of attempts to one per proxy. This will prevent endless while loop\n",
    "        chkproxy = 1\n",
    "        while chkproxy and attempts:\n",
    "            if (time.time() - tstart) > 60*refreshmin: # check if it's been more than refreshmin minutes since proxy_pool updated\n",
    "                print('Refreshing proxy pool...')\n",
    "\n",
    "                currproxies = set(proxydict.keys())\n",
    "                newproxies = generateProxies()\n",
    "                newproxies = newproxies.difference(currproxies)\n",
    "\n",
    "                if newproxies:\n",
    "                    newdict = {i:random.choice(user_agents) for i in newproxies}\n",
    "                    proxydict.update(newdict)\n",
    "                    proxy_pool = cycle(proxydict)\n",
    "                    currproxy = next(proxy_pool)\n",
    "                    print('Proxy pool updated!')\n",
    "\n",
    "            try:\n",
    "                resp = requests.get(url,proxies={\"http\":currproxy, \"https\":currproxy},headers={'User-Agent': proxydict[currproxy]}, timeout=20)\n",
    "                print(f'Proxy success for {currproxy}')\n",
    "                print()\n",
    "                chkproxy = 0\n",
    "                attempts += 1\n",
    "            except:\n",
    "                prevproxy = currproxy\n",
    "                currproxy = next(proxy_pool)\n",
    "                print(f'Proxy error for {prevproxy}! Next up is {currproxy}')\n",
    "                attempts -= 1\n",
    "                print(f'Attempts remaining: {attempts}')\n",
    "                \n",
    "    else:\n",
    "        # don't use the proxy\n",
    "        resp = requests.get(url, headers = {'User-Agent': user_agent})\n",
    "        \n",
    "    html = resp.content\n",
    "    pgsoup = BeautifulSoup(html)\n",
    "    \n",
    "    # Check if there are additional pages of results\n",
    "    if pgsoup.find(\"a\", {\"title\" : \"Go forward 1 page\"}):\n",
    "        moreresults = 1\n",
    "    else:\n",
    "        moreresults = 0\n",
    "    \n",
    "    links = pgsoup.select(\"div.title > a.link\") # grab all 96 (or up to 96) links\n",
    "    tstamps = pgsoup.select(\"div.listing-detail-line script\") # grab all 96 (or up to 96) timestamps\n",
    "\n",
    "    # Loop through links and scrape data for each new listing\n",
    "    all_cars = []\n",
    "    with progressbar.ProgressBar(max_value=len(links)) as bar:\n",
    "        for idx, link in enumerate(links): # *** only load first x results for now to avoid ban before implementing spoofing\n",
    "\n",
    "            # Reset all fields to None before next loop\n",
    "            price=year=make=model=body=mileage=title_type=city=state=seller=None\n",
    "            trim=ext_color=int_color=transmission=liters=cylinders=fuel_type=n_doors=ext_condition=int_condition=drive_type=None\n",
    "\n",
    "            # We're going to want to strip the \"?ad_cid=[number]\" from the end of these links as they're not needed to load the page properly\n",
    "            # Regular expressions should come in handy here\n",
    "\n",
    "            cutidx = re.search('(\\?ad_cid=.+)',link['href']).start()\n",
    "            currlink = link['href'][:cutidx]\n",
    "\n",
    "            # Somewhere here we should do a check to make sure that the timestamp for currlink is newer than our newest file in our repository\n",
    "            # That is, compare the timestamps with a simple conditional, where if the conditional is not met, this loop breaks to avoid useless computation time\n",
    "\n",
    "            # Generate full link for the current listing\n",
    "            fulllink = '/'.join([rooturl.rstrip('/'), currlink.lstrip('/')])\n",
    "\n",
    "            if use_proxy:\n",
    "                attempts = len(proxydict) # for now, limit the total number of attempts to one per proxy. This will prevent endless while loop\n",
    "                chkproxy = 1\n",
    "                while chkproxy and attempts:\n",
    "                    if (time.time() - tstart) > 60*refreshmin: # check if it's been more than refreshmin minutes since proxy_pool updated\n",
    "                        print('Refreshing proxy pool...')\n",
    "\n",
    "                        currproxies = set(proxydict.keys())\n",
    "                        newproxies = generateProxies()\n",
    "                        newproxies = newproxies.difference(currproxies)\n",
    "\n",
    "                        if newproxies:\n",
    "                            newdict = {i:random.choice(user_agents) for i in newproxies}\n",
    "                            proxydict.update(newdict)\n",
    "                            proxy_pool = cycle(proxydict)\n",
    "                            currproxy = next(proxy_pool)\n",
    "                            print('Proxy pool updated!')\n",
    "\n",
    "                    try:\n",
    "                        resp = requests.get(fulllink,proxies={\"http\":currproxy, \"https\":currproxy},headers={'User-Agent': proxydict[currproxy]}, timeout=20)\n",
    "                        print(f'Proxy success for {currproxy}')\n",
    "                        print()\n",
    "                        chkproxy = 0\n",
    "                        attempts += 1\n",
    "                    except:\n",
    "                        prevproxy = currproxy\n",
    "                        currproxy = next(proxy_pool)\n",
    "                        print(f'Proxy error for {prevproxy}! Next up is {currproxy}')\n",
    "                        attempts -= 1\n",
    "                        print(f'Attempts remaining: {attempts}')\n",
    "                        \n",
    "            else:\n",
    "                # don't use the proxy\n",
    "                resp = requests.get(fulllink, headers = {'User-Agent': user_agent})\n",
    "            \n",
    "            \n",
    "            lsthtml = resp.content\n",
    "            lstsoup = BeautifulSoup(lsthtml)\n",
    "            \n",
    "            # Check if link is still good (i.e. listing is still active)\n",
    "            if lstsoup.title.text.strip().lower() == 'not found':\n",
    "                print('Bad link. Skipping...')\n",
    "                bar.update(idx)\n",
    "            else:\n",
    "\n",
    "                # Get timestamp\n",
    "                tstamp = int(re.search('(\\d+)',tstamps[idx].text).group(0))\n",
    "\n",
    "                # Check if timestamp is newer than maxts\n",
    "                if tstamp <= maxts:\n",
    "                    print('************ Found end of new data ************')\n",
    "                    moreresults = 0\n",
    "                    break\n",
    "\n",
    "                # Get listing price\n",
    "                price = lstsoup.select('h3.price')[0].text.strip().replace('$','').replace(',','')\n",
    "\n",
    "                # Get seller's location\n",
    "                if lstsoup.select('h2.location > a'):\n",
    "                    location = lstsoup.select('h2.location > a')[0].text.strip()\n",
    "                    city, state = location.split(',')\n",
    "                    city = city.strip()\n",
    "                    state = state.strip()\n",
    "\n",
    "                # Get seller type (dealer or owner)\n",
    "                sellerstr = lstsoup.select('div.fsbo')[0].text.strip()\n",
    "                if re.search('(Dealer)', sellerstr):\n",
    "                    seller = 'Dealer'\n",
    "                elif re.search('(Owner)', sellerstr):\n",
    "                    seller = 'Owner'\n",
    "                    \n",
    "                # Get number of photos\n",
    "                if lstsoup.select('div.slider-uninitialized > p'):\n",
    "                    picstr = lstsoup.select('div.slider-uninitialized > p')[0].text.strip()\n",
    "                    n_pics = int(re.search('(\\d+)',picstr).group())\n",
    "                else:\n",
    "                    if lstsoup.find(id='widgetPhoto').p:\n",
    "                        picstr = lstsoup.find(id='widgetPhoto').p.text.strip()\n",
    "                        n_pics = int(re.search('(\\d+)',picstr).group())\n",
    "                    else:\n",
    "                        n_pics = 0\n",
    "\n",
    "                # Get table of car specs\n",
    "                specs = lstsoup.select('ul.listing-specifications')\n",
    "\n",
    "                for li in specs[0].find_all('li'):\n",
    "                    lititle = li.select('span.title')[0].text.strip().strip(':')\n",
    "                    livalue = li.select('span.value')[0].text.strip().strip(':')\n",
    "\n",
    "                    if livalue.lower() == 'not specified':\n",
    "                        livalue = None\n",
    "\n",
    "                    # Now a bunch of if-else statements to determine which column to add data to\n",
    "                    # There might be a more sophisticated way to do this, perhaps with a tuple or a dictionary?\n",
    "                    if lititle.lower() == 'year':\n",
    "                        if livalue:\n",
    "                            year = int(livalue)\n",
    "                        else:\n",
    "                            year = livalue\n",
    "                    elif lititle.lower() == 'make':\n",
    "                        make = livalue\n",
    "                    elif lititle.lower() == 'model':\n",
    "                        model = livalue\n",
    "                    elif lititle.lower() == 'body':\n",
    "                        body = livalue\n",
    "                    elif lititle.lower() == 'mileage':\n",
    "                        if livalue:\n",
    "                            mileage = int(livalue.replace(',',''))\n",
    "                        else:\n",
    "                            mileage = livalue\n",
    "                    elif lititle.lower() == 'title type':\n",
    "                        title_type = livalue\n",
    "\n",
    "                    # Below this are non-required specs    \n",
    "                    elif lititle.lower() == 'trim':\n",
    "                        trim = livalue\n",
    "                    elif lititle.lower() == 'exterior color':\n",
    "                        if livalue:\n",
    "                            ext_color = livalue.lower()\n",
    "                        else:\n",
    "                            ext_color = livalue\n",
    "                    elif lititle.lower() == 'interior color':\n",
    "                        if livalue:\n",
    "                            int_color = livalue.lower()\n",
    "                        else:\n",
    "                            int_color = livalue\n",
    "                    elif lititle.lower() == 'transmission':\n",
    "                        transmission = livalue\n",
    "                    elif lititle.lower() == 'liters':\n",
    "                        try:\n",
    "                            liters = float(livalue)\n",
    "                        except:\n",
    "                            if livalue:\n",
    "                                str1 = re.search('^(.*?)L',livalue).group(0).strip().replace(' ','')\n",
    "                                if re.search('^(\\D+)',str1):\n",
    "                                    idxend = re.search('^(\\D+)',str1).end()\n",
    "                                    livalue = str1[idxend:-1]\n",
    "                                    if re.search('(\\D+)',livalue): # check if still other pollutants\n",
    "                                        idxend = re.search('(\\D+)',livalue).end()\n",
    "                                        livalue = livalue[idxend:]\n",
    "                                else:\n",
    "                                    livalue = str1[:-1]\n",
    "                                try:\n",
    "                                    livalue = float(livalue)\n",
    "                                except:\n",
    "                                    print(url)\n",
    "                                    print('****')\n",
    "                                    print(link)\n",
    "                            else:\n",
    "                                liters = livalue\n",
    "                    elif lititle.lower() == 'cylinders':\n",
    "                        if livalue:\n",
    "                            cylinders = int(livalue)\n",
    "                        else:\n",
    "                            cylinders = livalue\n",
    "                    elif lititle.lower() == 'fuel type':\n",
    "                        fuel_type = livalue\n",
    "                    elif lititle.lower() == 'number of doors':\n",
    "                        if livalue:\n",
    "                            n_doors = int(livalue)\n",
    "                        else:\n",
    "                            n_doors = livalue\n",
    "                    elif lititle.lower() == 'exterior condition':\n",
    "                        ext_condition = livalue\n",
    "                    elif lititle.lower() == 'interior condition':\n",
    "                        int_condition = livalue\n",
    "                    elif lititle.lower() == 'drive type':\n",
    "                        drive_type = livalue\n",
    "                    elif (lititle.lower() == 'vin') | (lititle.lower() == 'stock number') | (lititle.lower() == 'dealer license'):\n",
    "                        None # Don't want to save these\n",
    "                    else:\n",
    "                        None\n",
    "                        print(f'Unmatched param {lititle}: {livalue}') # <-- could take advantage of some or all of these\n",
    "\n",
    "                curr_car = pd.DataFrame({\"timestamp\":[tstamp],\n",
    "                                         \"lastpull_ts\":[int(time.time())],\n",
    "                                         \"link\":[fulllink],\n",
    "                                         \"price\":[price],\n",
    "                                         \"year\":[year],\n",
    "                                         \"make\":[make],\n",
    "                                         \"model\":[model],\n",
    "                                         \"body\":[body],\n",
    "                                         \"mileage\":[mileage],\n",
    "                                         \"title_type\":[title_type],\n",
    "                                         \"city\":[city],\n",
    "                                         \"state\":[state],\n",
    "                                         \"seller\":[seller],\n",
    "                                         \"trim\":[trim],\n",
    "                                         \"ext_color\":[ext_color],\n",
    "                                         \"int_color\":[int_color],\n",
    "                                         \"transmission\":[transmission],\n",
    "                                         \"liters\":[liters],\n",
    "                                         \"cylinders\":[cylinders],\n",
    "                                         \"fuel_type\":[fuel_type],\n",
    "                                         \"n_doors\":[n_doors],\n",
    "                                         \"ext_condition\":[ext_condition],\n",
    "                                         \"int_condition\":[int_condition],\n",
    "                                         \"drive_type\":[drive_type],\n",
    "                                         \"n_pics\":[n_pics]})\n",
    "                try:\n",
    "                    all_cars = pd.concat([all_cars, curr_car])\n",
    "                except:\n",
    "                    all_cars = curr_car\n",
    "\n",
    "                bar.update(idx)\n",
    "\n",
    "    if type(all_cars) is pd.core.frame.DataFrame: # make sure that some data was actually scraped\n",
    "        all_cars = all_cars.reset_index()\n",
    "        del all_cars['index']\n",
    "        all_cars.fillna(value=pd.np.nan, inplace=True)\n",
    "    if use_proxy:\n",
    "        return all_cars, moreresults, currproxy, proxydict\n",
    "    else:\n",
    "        return all_cars, moreresults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regenerate repository from scratch\n",
    "\n",
    "# determine whether or not to use proxy IPs. Can use boolean or int for this\n",
    "use_proxy = False\n",
    "\n",
    "# Define root url for KSL cars\n",
    "rooturl = \"https://cars.ksl.com\"\n",
    "\n",
    "# Note the url below specifies that we're looking for 96 per page and the default sort of newest to oldest posting\n",
    "# This note about newest to oldest is useful so that we can avoid scraping repeat listings based on their timestamps\n",
    "# Also note that this url does NOT have a page number associated with it. This is added in the while loop below\n",
    "lurl = \"https://cars.ksl.com/search/newUsed/Used;Certified/perPage/96/page/\"\n",
    "\n",
    "count = 0\n",
    "all_cars = []\n",
    "moreresults = 1\n",
    "while moreresults:\n",
    "    url = lurl + str(count)\n",
    "    try:\n",
    "        if use_proxy:\n",
    "            curr_cars, moreresults, currproxy, proxydict = carscraper(url=url, rooturl=rooturl, maxts=0, use_proxy=use_proxy, currproxy=currproxy, refreshmin = 15, proxydict = proxydict)\n",
    "        else:\n",
    "            curr_cars, moreresults = carscraper(url=url, rooturl=rooturl, maxts=0, use_proxy=False)\n",
    "        \n",
    "    except:\n",
    "        if use_proxy:\n",
    "            curr_cars, moreresults, currproxy, proxydict = carscraper(url=url, rooturl=rooturl, maxts=0, use_proxy=use_proxy, refreshmin = 15)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    \n",
    "    count += 1    \n",
    "    print(f'More results? {moreresults}')\n",
    "    print(f'Next page is {url}')\n",
    "    if type(curr_cars) is pd.core.frame.DataFrame: # make sure real data was returned\n",
    "        try:\n",
    "            all_cars = pd.concat([all_cars, curr_cars], ignore_index=True)\n",
    "        except:\n",
    "            all_cars = curr_cars\n",
    "    else:\n",
    "        print('No car data found!')\n",
    "    \n",
    "all_cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe to csv\n",
    "\n",
    "all_cars.to_csv('data/all_cars.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
