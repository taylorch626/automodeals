{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automodeals\n",
    "\n",
    "### NOTE: Prior to running this notebook, ensure the following packages are installed and that ipyleaflet's extension for jupyter is enabled (otherwise maps will not display):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # must be version 1.0.1 or later!\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "import statsmodels.formula.api as sm\n",
    "import seaborn as sns\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from ipyleaflet import Map, basemaps, GeoJSON, Popup, FullScreenControl, CircleMarker, LayerGroup\n",
    "# if above module isn't installed, do BOTH of the following:\n",
    "# pip install ipyleaflet\n",
    "# jupyter nbextension enable --py --sys-prefix ipyleaflet\n",
    "\n",
    "from ipywidgets import HTML # pip install ipywidgets\n",
    "from uszipcode import SearchEngine # pip install uszipcode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Information\n",
    "\n",
    "Title: KSL AutomoDeals\n",
    "<br>\n",
    "Names: Chantel Charlebois, Taylor Hansen, Michael Paskett\n",
    "<br>\n",
    "E-mails: chantel.charlebois@utah.edu, taylor.c.hansen@utah.edu, michael.paskett@utah.edu\n",
    "<br>\n",
    "UIDS: u1043299, u0642850, u1144000\n",
    "\n",
    "### Background and Motivation\n",
    "\n",
    "Almost every person in Utah (and some neighboring states) buying a used car will visit KSL Cars classifieds to look for their new wheels. There are a few resources for understanding the rough value of a used car, such as Kelly Blue Book (https://www.kbb.com) and CarGurus (https://www.cargurus.com/), but such services may not fully incorporate the complex auto market of a local area. By storing and analyzing the prices, details, and options for a certain model or class of vehicle, a prospective buyer can evaluate how good the listed price for a vehicle actually is. With such a model, the user can estimate how much a specific car is really worth, and determine if the vehicle is worthy of a test drive.\n",
    "\n",
    "### Project Objectives\n",
    "\n",
    "Questions:\n",
    "* How well can we predict the price of a newly-listed car based on the attributes available in an a ksl advertisement?\n",
    "* Which attributes are most influential in determining the vehicle price?\n",
    "* What nearby areas have the best price of cars?\n",
    "\n",
    "Aims:\n",
    "* Create a regression model that will predict the expected price of a car based on several attributes, such as:\n",
    "    * year,  seller type (dealer/private), mileage, color, title (clean/salvaged), transmission type, cylinders, fuel type, number of doors, exterior/interior condition, listing date, page views rate\n",
    "* Create a clustered map of ‚Äúgood deals‚Äù in different regions\n",
    "\n",
    "Benefits:\n",
    "* This project could benefit anyone in the market for a used car in the KSL area, helping them to be informed of the potential value of a car they are interested in.\n",
    "\n",
    "### Ethical Considerations\n",
    "\n",
    "Stakeholders:\n",
    "* The creators (us)\n",
    "* The seller\n",
    "* The prospective buyer\n",
    "* KSL\n",
    "\n",
    "Our incentive as creators and prospective buyers is to find good deals without having to manually spend hours searching through KSL for a good deal. For other prospective buyers, the same applies. The sellers have competing interests, as they would like to sell their car for as much as possible. KSL also has a stake in this project, as it makes revenue from ads and from sellers paying for better listings in order to make their vehicle more prominent.\n",
    "\n",
    "### Data\n",
    "\n",
    "We scraped our data from queries of used cars listed on [cars.ksl.com](https://cars.ksl.com/). We read the robots.txt files for both ksl.com and cars.ksl.com to confirm that there were no restrictions for crawling their website. We also reviewed the terms and conditions and similarly found no indication for rules on crawling. There used to be an undocumented API for interfacing with KSL (as of four years ago), but it is no longer publicly accessible, so we manually scraped the data with BeautifulSoup.\n",
    "\n",
    "To avoid consistently using too much bandwidth on their website, our initial plan was to start with ‚Äúhistorical‚Äù data collection by saving .html pages over the course of the project so that we could parse them offline. However, we found that KSL listings are dynamically loaded by JavaScript, and the html files were bloated with other associated files which would be too unwieldy to download on a daily basis (on the order of 5,000 cars/day). Instead, we opted for a live crawler approach to get our data.\n",
    "\n",
    "After an initial crawl through the website to get as many used cars listings as possible, we automated the crawler to go in every night at a randomly scheduled time and pull any newly listed cars not already in our repository. The code associated with this crawler was developed initially in various Jupyter notebooks before porting over to .py files for automation. The code is not included in the present notebook given its length. The crawler/scraping functions are described in more detail in the Project Milestone.\n",
    "\n",
    "We also used CarGurus website https://www.cargurus.com/Cars/sell-car/?pid=SellMyCarDesktopHeader and their API to request an expected car price based on each car's Make, Model, Mileage, Year, and Zip Code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KSL\n",
    "\n",
    "\n",
    "##### A typical search page from KSL Cars:\n",
    "Links for each newly listed individual car were scraped from pages like this.\n",
    "<br>\n",
    "<img src='screenshots\\KSL_search_page.png'>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "***\n",
    "\n",
    "<br>\n",
    "\n",
    "##### A typical individual listing from KSL cars:\n",
    "A pandas dataframe was filled with car info scraped from pages like this.\n",
    "<br>\n",
    "<img src='screenshots\\KSL_listing.png'>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "***\n",
    "\n",
    "<br>\n",
    "\n",
    "##### Crawler Automation:\n",
    "The crawler was scheduled to run every night and scrape data for any newly listed cars.\n",
    "<br>\n",
    "<img src='screenshots\\automating_repo_update.png'>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "***\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Price from CarGurus and Zip Codes\n",
    "Example of the form used to request the expected car price from CarGurus using their API.\n",
    "<img src=\"screenshots\\cargurus.png\">\n",
    "\n",
    "All code to get the expected price is commented out because it takes too long to run. Code also found in the GetExpectedPrice.ipynb notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# import numpy as np\n",
    "# import bs4\n",
    "# import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Read in scraped car data\n",
    "# data_file = os.path.join(os.getcwd(),\"data\",\"all_cars.csv\") \n",
    "# cars = pd.read_csv(data_file)\n",
    "# pd.set_option(\"display.max_columns\",None) \n",
    "# cars['mileage'] = cars['mileage'].astype('Int64')\n",
    "# cars['year'] = cars['year'].astype('Int64')\n",
    "# cars.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Create dictionary of zip codes for all towns to look up expected price\n",
    "# # Website to look up zipcodes\n",
    "# # http://localistica.com/usa/ut/salt%20lake%20city/zipcodes/all-zipcodes/\n",
    "\n",
    "# def get_most_populated_zip_code(city):\n",
    "#     try:\n",
    "#         r = requests.get(f'http://localistica.com/search.aspx?q={city.lower().replace(\" \", \"+\")}')\n",
    "#         url = bs4.BeautifulSoup(r.text).find(\"a\", id=\"ctl09_hlZipCodesCount\")['href']\n",
    "#         return int(bs4.BeautifulSoup(requests.get(url).text).find(id=\"dgZipCodes\").find_all(\"tr\")[1].td.a.text)\n",
    "#     except:\n",
    "#         return None\n",
    "    \n",
    "\n",
    "# # get unique cities in dataframe\n",
    "# all_cities = [car for car in cars.city.unique() if type(car) == str]\n",
    "# keyList = [x + \", \" + cars[cars.city == x].iloc[0]['state'] for x in all_cities]\n",
    "# # look up zipcode\n",
    "# zip_codes = {key: get_most_populated_zip_code(key + \" \" + cars[cars.city == key].iloc[0]['state']) for key in all_cities}\n",
    "# # hard code the ones it missed\n",
    "# zip_codes.update({'St. Anthony': 83445, 'Provo Canyon': 84604})\n",
    "# # check for missing zip codes\n",
    "# print(len([k for k,v in zip_codes.items() if v == None]))\n",
    "# zip_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # list of all cars in CarGurus database\n",
    "# all_cars = requests.get(\"https://www.cargurus.com/Cars/getCarPickerReferenceDataAJAX.action?showInactive=false&useInventoryService=false&quotableCarsOnly=false&localCountryCarsOnly=true&outputFormat=REACT\").json()\n",
    "\n",
    "# # gets CarGuru make and model id to find price for individual make and model\n",
    "# def get_cargurus_maker_and_model_ids(all_cars, car_make, car_model):\n",
    "#     try:\n",
    "#         all_models = [x for x in all_cars.get('allMakerModels').get('makers') if x.get('name') == car_make][0]\n",
    "#         return (all_models.get('id'), [x for x in all_models.get('models') if x.get('name') == car_model][0].get('id'))\n",
    "#     except (IndexError, AttributeError):\n",
    "#         return (None, None)\n",
    "    \n",
    "# # gets the entity id which includes the make, model, and year of car\n",
    "# def get_entity_id(maker_id, model_id, car_year):\n",
    "#     try:\n",
    "#         all_entities = requests.get(f\"https://www.cargurus.com/Cars/getSelectedMakerModelCarsAJAX.action?showInactive=false&useInventoryService=false&quotableCarsOnly=false&localCountryCarsOnly=true&outputFormat=REACT&maker={maker_id}\").json()\n",
    "#         model_entity_ids = [car for car in all_entities.get('models') if car.get('id') == model_id][0]\n",
    "#         return [ids for ids in model_entity_ids.get('cars') if ids.get('year') == car_year][0].get('id')\n",
    "#     except (IndexError, AttributeError):\n",
    "#         return None\n",
    "    \n",
    "# # gets the estimated listing price of the car based on entity id and the mileage\n",
    "# def get_price(car_make, car_model, car_year, car_mileage, car_zip_code, all_cars):\n",
    "#     maker_id, model_id = get_cargurus_maker_and_model_ids(all_cars, car_make, car_model)\n",
    "#     if not model_id or pd.isna(car_mileage):\n",
    "#         return None\n",
    "    \n",
    "#     entity_id = get_entity_id(maker_id, model_id, car_year)\n",
    "    \n",
    "#     if not entity_id:\n",
    "#         return None \n",
    "#     # data needed to request CarGurus report\n",
    "#     data = {\n",
    "#         'carDescription.radius': 75,\n",
    "#         'selectedEntity': entity_id,\n",
    "#         'carDescription.transmission': \"\",\n",
    "#         'carDescription.mileage': car_mileage,\n",
    "#         'carDescription.postalCode': car_zip_code,\n",
    "#         'carDescription.engineId': \"\",\n",
    "#         'carDescription.vin': \"\",\n",
    "#         'carDescription.vinType': \"\",\n",
    "#         'forPrivateListing': True,\n",
    "#         'inventoryListingId' : \"\"\n",
    "#     }\n",
    "    \n",
    "#     res = requests.post(\"https://www.cargurus.com/Cars/generateReportJsonAjax.action\", data=data)\n",
    "#     res.raise_for_status()\n",
    "#     try:\n",
    "#         return res.json().get(\"priceDetails\").get(\"privateListingPrice\") #private listing price from CarGurus report\n",
    "#     except AttributeError:\n",
    "#         raise Exception(res.json())\n",
    "\n",
    "\n",
    "# for index, row in cars.iterrows():\n",
    "#     if row[\"city\"] in zip_codes.keys():\n",
    "#         try:\n",
    "#             expected_price = get_price(row[\"make\"], row[\"model\"], row[\"year\"], row[\"mileage\"], zip_codes.get(row[\"city\"]), all_cars)\n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "#             time.sleep(5)\n",
    "#             continue\n",
    "#         # change expected prices that are 0 to none\n",
    "#         if expected_price == None or expected_price < 1:\n",
    "#             expected_price = None \n",
    "#             print(index)\n",
    "#         cars.loc[index, \"expected_price\"] = expected_price\n",
    "#     else:\n",
    "#         cars.loc[index, \"expected_price\"] = None\n",
    "\n",
    "# # Add zip codes to a column\n",
    "# for index, row in cars.iterrows():\n",
    "#     cars.loc[index, \"zip_code\"] = zip_codes.get(row[\"city\"])\n",
    "    \n",
    "# # save cars dataframe to pickle\n",
    "# cars.to_pickle(\"./cars.pkl\")\n",
    "\n",
    "# pickle_cars.info() # 36,325 expected prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing\n",
    "\n",
    "Each KSL listing page has a fairly consistent format making scraping feasible for the large number of pages we analyzed. When creating a listing, the user is required to list the year, VIN, make, model, body style, mileage, title type, asking price, and location. Together, these are the only features we can guarantee to extract from each page. Of course, many listings have many more details listed than these which we also incorporated into our analysis (as shown in the example listing above).\n",
    "\n",
    "As mentioned above, data was scraped with BeautifulSoup and structured into a pandas dataframe. Numerical variables for categorical variables were generated and concatenated to this dataframe to facilitate use of these variables. Subsequent processing was done using built-in pandas masking to get relevant rows from the dataframe for new queries when searching recently listed used cars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Analysis\n",
    "\n",
    "We visualized our data in multiple ways to check our data scraping procedures and make sure we did not incorrectly classify our data. The first basic check we did was scroll through the data frame for any obvious errors using the display command. We then used the describe command to look at the descriptive statistics of each column in our dataframe. Next we visualized the data using a scatterplot matrix in order to check the histograms of each parameter for outliers and general trends. We also used the scatterplot matrix to explore correlations between different parameters. We also visualized a heat map of the correlation matrix to determine which parameters were strongly correlated. This information was used to identify potential strong predictors for the multiple linear regression and determine if any parameters were potential confounders.\n",
    "\n",
    "Basic outline:\n",
    "1. Scroll through the dataframe for any obvious errors\n",
    "2. Describe command to verify the descriptive statistics\n",
    "3. Visualize using the scatterplot matrix to explore histograms of each parameter and look for outliers/trends\n",
    "4. Heat map of the correlation matrix to determine which parameters are strongly correlated to identify potential strong predictors for the multiple linear regression or potential confounders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 10)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in scraped car data\n",
    "data_file = os.path.join(os.getcwd(),\"data\",\"all_cars.csv\")\n",
    "cars = pd.read_csv(data_file)\n",
    "# cars = pd.read_pickle('cars.pkl')\n",
    "pd.set_option(\"display.max_rows\",None,\"display.max_columns\",None) \n",
    "\n",
    "# Recast data\n",
    "cars['mileage'] = cars['mileage'].astype('float')\n",
    "cars['year'] = cars['year'].astype('float')\n",
    "\n",
    "# Clean price data and recast\n",
    "ugly_cars = cars[cars['price'].str.contains('MSRP')]\n",
    "ugly_cars.index\n",
    "for index, car in ugly_cars.iterrows():\n",
    "    if '|' not in car['price']:\n",
    "        cars.at[index,'price'] = None\n",
    "    else:\n",
    "        cars.at[index,'price'] = car['price'].split('|')[0].strip()\n",
    "cars['price'] = cars['price'].astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(cars.describe())\n",
    "display(cars.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Data clean up\n",
    "# Check year\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "cars[\"year\"].plot.hist(bins=100)\n",
    "plt.title('Original Year Histogram')\n",
    "plt.xlabel('Year')\n",
    "# any cars with a year less than 1920 changed to None\n",
    "cars.loc[(cars.year < 1920),'year']=None \n",
    "plt.subplot(122)\n",
    "cars[\"year\"].plot.hist(bins=100)\n",
    "plt.title('Cleaned Year Histogram')\n",
    "plt.xlabel('Year')\n",
    "\n",
    "# Check price\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "cars[\"price\"].plot.hist(bins=100)\n",
    "plt.title('Original Price Histogram')\n",
    "plt.xlabel('Price ($)')\n",
    "# any cars with a price less than $100 or greater than $500,000 changed to None\n",
    "cars.loc[(cars.price < 100),'price']=None \n",
    "# expcars = cars['price'] > 500000\n",
    "# expcars = cars[expcars]\n",
    "# display(expcars)\n",
    "cars.loc[(cars.price > 500000),'price']=None \n",
    "plt.subplot(122)\n",
    "cars[\"price\"].plot.hist(bins=100)\n",
    "plt.title('Cleaned Price Histogram')\n",
    "plt.xlabel('Price ($)')\n",
    "\n",
    "# Check Mileage\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "cars[\"mileage\"].plot.hist(bins=100)\n",
    "plt.title('Original Mileage Histogram')\n",
    "plt.xlabel('Miles')\n",
    "# any cars with a mileage greater than 500000 changed to None\n",
    "cars.loc[(cars.mileage > 500000),'mileage']=None \n",
    "plt.subplot(122)\n",
    "cars[\"mileage\"].plot.hist(bins=100)\n",
    "plt.title('Cleaned Mileage Histogram')\n",
    "plt.xlabel('Miles')\n",
    "\n",
    "# Check Liters\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "cars[\"liters\"].plot.hist(bins=100)\n",
    "plt.title('Original Liters Histogram')\n",
    "plt.xlabel('Engine Size (Liters)')\n",
    "# any cars with a liters greater than 100 changed to None\n",
    "cars.loc[(cars.liters > 100),'liters']=None \n",
    "plt.subplot(122)\n",
    "cars[\"liters\"].plot.hist(bins=100)\n",
    "plt.title('Cleaned Liters Histogram')\n",
    "plt.xlabel('Engine Size (Liters)')\n",
    "\n",
    "# Check fav_per_view\n",
    "# any cars with inf or Nan changed to None\n",
    "cars.loc[(cars.fav_per_view > 2),'fav_per_view']=None \n",
    "\n",
    "print('Cleaned Descriptive Statistics')\n",
    "display(cars.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Explore Categorical Variables\n",
    "print(cars['body'].value_counts(), '\\n')\n",
    "print(cars['title_type'].value_counts(), '\\n')\n",
    "print(cars['seller'].value_counts(), '\\n')\n",
    "#print(cars['ext_color'].value_counts(), '\\n')\n",
    "print(cars['transmission'].value_counts(), '\\n')\n",
    "print(cars['fuel_type'].value_counts(), '\\n')\n",
    "print(cars['ext_condition'].value_counts(), '\\n')\n",
    "print(cars['int_condition'].value_counts(), '\\n')\n",
    "print(cars['drive_type'].value_counts(), '\\n')\n",
    "\n",
    "# create numerical categorical variables\n",
    "print('\\n Title types')\n",
    "# set clean titles to 1 and other titles to 0\n",
    "cars[\"title_num\"] = cars[\"title_type\"].map({'Rebuilt/Reconstructed Title':0, 'Salvage Title':0, 'Dismantled Title':0, 'Clean Title':1})\n",
    "print(cars['title_num'].value_counts(), '\\n')\n",
    "\n",
    "print('Seller types')\n",
    "cars[\"seller_num\"] = cars[\"seller\"].map({'Dealer':0, 'Owner':1})\n",
    "print(cars['seller_num'].value_counts(), '\\n')\n",
    "\n",
    "print('Transmission types')\n",
    "cars[\"transmission_num\"] = cars[\"transmission\"].map({'Automatic':0, 'Automanual':0, 'Manual':1, 'CVT':0})\n",
    "print(cars['transmission_num'].value_counts(), '\\n')\n",
    "\n",
    "print('Drive types: 1 = 4 Wheel Drive, 0 = 2 Wheel Drive')\n",
    "cars[\"drive_num\"] = cars[\"drive_type\"].map({'4-Wheel Drive':1, '2-Wheel Drive':0, 'FWD':0, 'AWD':1})\n",
    "print(cars['drive_num'].value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning Interpretation\n",
    "Looking at the histograms and descriptive statistics we had to clean the data and remove outliers. We changed the data to missing if the car year was older than 1920, the price was less than 100 dollars or greater than 500000 dollars, the mileage was greater than 500,000 miles, or the liters was greater than 100. This cleaned up most of the outliers and the describe() values look much more realistic. We then converted the categorical variables of title_type, seller, transmission, and drive_type to numerical categorical variables of 0 or 1 so that we could include them in our regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter Matrix & Correlation Matrix for All Cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check histograms and scatter matrix for outliers\n",
    "# Scatter Matrix\n",
    "scatter_matrix(cars[[\"price\", \"year\", \"mileage\", \"liters\", \"cylinders\", \"n_doors\", \"n_pics\",'views','favorites','view_rate','favorite_rate']], alpha = 0.2, figsize=(15, 10))\n",
    "print()\n",
    "\n",
    "# Correlation Matrix\n",
    "# automatically ignores missing values\n",
    "fig = plt.figure(figsize=(15,10));\n",
    "ax = fig.add_subplot(111)\n",
    "plt.pcolor(cars.corr(),vmin=-1,vmax=1, cmap=plt.cm.get_cmap('BrBG'))\n",
    "labels = ['lastpull_ts','price','year','mileage','liters','cylinders','n_doors','n_pics','expected_price','zip_code','views','favorites','workingURL','view_rate','favorite_rate','fav_per_view','title_num', 'seller_num', 'transmission_num', 'drive_num'] \n",
    "plt.xticks([i+0.5 for i in range(len(labels))],labels=labels,rotation=90)\n",
    "plt.yticks([i+0.5 for i in range(len(labels))],labels=labels)\n",
    "plt.title(\"Correlation Matrix\");\n",
    "plt.colorbar();\n",
    "display(cars.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Exploration Interpretation for All Cars\n",
    "The scatter matrix and correlation matrix for all the cars show that the strongest correlations with price are mileage ~ -0.56, year ~ 0.47, and drive_num ~ 0.42. All of these variables are what we would expect to correlate. Other variables that may be confounders because they correlate with each other are cylinders x liters, views x favorites, views x view_rate, views x favorite_rate, favorites x favorite_rate which also make sense that they would be correlated because they are so closely related."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression\n",
    "Aim: predict the price of a newly-listed used car\n",
    "<br>\n",
    "Dependent variable:list price \n",
    "<br>\n",
    "Possible independent variables: year, seller type (dealer/private), mileage, color, title (clean/salvaged), transmission type, cylinders, fuel type, number of doors, exterior/interior condition, listing date, page views per day. \n",
    "<br>\n",
    "We will use the Python package statsmodels to perform all regression analyses. \n",
    "1. Multiple linear regression first using the parameters that had strong correlations with list price. \n",
    "2. Based off of this initial model we will adjust our multiple linear regression to only include parameters that have significant p-values for their individual coefficients. \n",
    "Significance level: ùù∞=0.05\n",
    "\n",
    "Expected Outcomes:\n",
    "* Our final model should have a p-value < 0.05 for the F-statistic of the overall model. \n",
    "* We are aiming to explain at least 70% of the variance with our model and hope to get an R-squared value of 0.70 or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# multiple linear regression\n",
    "# set missing option to drop any missing data\n",
    "print('Multiple linear regression for all variables across all cars')\n",
    "df_all_ols = sm.ols(formula=\"price ~ year + mileage + liters + cylinders + n_doors + n_pics + views + favorites + view_rate + favorite_rate + fav_per_view\", data=cars, missing='drop').fit()\n",
    "display(df_all_ols.summary())\n",
    "\n",
    "# remove correlated variables\n",
    "# correlated variables - views x favorites, views x view_rate, favorites x favorite_rate, views x favorite_rate, view_rate x favorite_rate\n",
    "print('Multiple linear regression with highly correlated variables removed (favorites and views and their rates)')\n",
    "df_all_ols = sm.ols(formula=\"price ~ year + mileage + liters + cylinders + n_doors + n_pics + favorites + fav_per_view\", data=cars, missing='drop').fit()\n",
    "display(df_all_ols.summary())\n",
    "\n",
    "# remove insignificant variables\n",
    "print('Multiple linear regression with significant predictor variables')\n",
    "df_all_ols = sm.ols(formula=\"price ~ year + mileage + liters + cylinders + fav_per_view\", data=cars, missing='drop').fit()\n",
    "display(df_all_ols.summary())\n",
    "\n",
    "# remove insignificant variables\n",
    "print('Multiple linear regression with significant predictor variables round 2')\n",
    "df_all_ols = sm.ols(formula=\"price ~ year + mileage + liters + fav_per_view\", data=cars, missing='drop').fit()\n",
    "display(df_all_ols.summary())\n",
    "\n",
    "# Compare expected price from cargurus to actual list price\n",
    "print('Simple linear regression of expected price')\n",
    "df_ols = sm.ols(formula=\"price ~ expected_price\", data=cars, missing='drop').fit()\n",
    "display(df_ols.summary())\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.regplot(x='expected_price', y='price', data=cars)\n",
    "plt.title('All Cars Simple Linear Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find the most common car make\n",
    "cars['make_model'] = cars['make'] + ' ' + cars['model']\n",
    "\n",
    "print('Most common models of cars:')\n",
    "print(cars['make_model'].value_counts()[:5].sort_values(ascending=False))\n",
    "# the 1500 model includes Ram, GMC so second most common car is Silverado 1500\n",
    "\n",
    "# Find all Ford F150, most popular car\n",
    "mask = cars['make_model'] == 'Ford F-150'\n",
    "F150 = cars[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Ford F150\n",
    "# Scatter Matrix\n",
    "scatter_matrix(F150[[\"price\", \"year\", \"mileage\", \"liters\", \"cylinders\", \"n_doors\", \"n_pics\",'views','favorites','view_rate','favorite_rate']], alpha = 0.2, figsize=(15, 10))\n",
    "print()\n",
    "\n",
    "# Correlation Matrix\n",
    "# automatically ignores missing values\n",
    "fig = plt.figure(figsize=(15,10));\n",
    "ax = fig.add_subplot(111)\n",
    "plt.pcolor(F150.corr(),vmin=-1,vmax=1, cmap=plt.cm.get_cmap('BrBG'))\n",
    "labels = ['lastpull_ts','price','year','mileage','liters','cylinders','n_doors','n_pics','expected_price','zip_code','views','favorites','workingURL','view_rate','favorite_rate','fav_per_view','title_num', 'seller_num', 'transmission_num', 'drive_num'] # check labels for final df\n",
    "plt.xticks([i+0.5 for i in range(len(labels))],labels=labels,rotation=90)\n",
    "plt.yticks([i+0.5 for i in range(len(labels))],labels=labels)\n",
    "plt.title(\"Correlation Matrix\");\n",
    "plt.colorbar();\n",
    "display(F150.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Ford F150 Multiple Linear Regression\n",
    "print('Multiple linear regression Ford F150')\n",
    "df_ols = sm.ols(formula=\"price ~ year + mileage + liters + cylinders + n_doors + n_pics + views + favorites + title_num + seller_num + transmission_num + drive_num\", data=F150, missing='drop').fit()\n",
    "display(df_ols.summary())\n",
    "print('We explained 85% of the variance with this initial model including all variables \\n \\n')\n",
    "\n",
    "print('Multiple linear regression Ford F150 remove insignificant variables')\n",
    "df_ols = sm.ols(formula=\"price ~ year + mileage + liters + cylinders + n_pics + views + favorites + drive_num\", data=F150, missing='drop').fit()\n",
    "display(df_ols.summary())\n",
    "print('We explained 76% of the variance with our second model that doesn not include variables that were not signficiant in the previous model \\n \\n')\n",
    "\n",
    "print('Simple linear regression of expected price Ford F150')\n",
    "df_ols = sm.ols(formula=\"price ~ expected_price\", data=F150, missing='drop').fit()\n",
    "display(df_ols.summary())\n",
    "print('70% of the variance is explained when comparing the expected price to the list price. The model we developed above is explains more variance than the CarGurus expected price. \\n \\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Second most commont type of car\n",
    "\n",
    "# Find all Chevy Silverado 1500, second most popular car\n",
    "mask = cars['make_model'] == 'Chevrolet Silverado 1500'\n",
    "silverado_1500 = cars[mask]\n",
    "\n",
    "# Chevy Silverado 1500\n",
    "# Scatter Matrix\n",
    "scatter_matrix(silverado_1500[[\"price\", \"year\", \"mileage\", \"liters\", \"cylinders\", \"n_doors\", \"n_pics\",'views','favorites','view_rate','favorite_rate']], alpha = 0.2, figsize=(15, 10))\n",
    "print()\n",
    "\n",
    "# Correlation Matrix\n",
    "# automatically ignores missing values\n",
    "fig = plt.figure(figsize=(15,10));\n",
    "ax = fig.add_subplot(111)\n",
    "plt.pcolor(silverado_1500.corr(),vmin=-1,vmax=1, cmap=plt.cm.get_cmap('BrBG'))\n",
    "labels = ['lastpull_ts','price','year','mileage','liters','cylinders','n_doors','n_pics','expected_price','zip_code','views','favorites','workingURL','view_rate','favorite_rate','fav_per_view','title_num', 'seller_num', 'transmission_num', 'drive_num'] # check labels for final df\n",
    "plt.xticks([i+0.5 for i in range(len(labels))],labels=labels,rotation=90)\n",
    "plt.yticks([i+0.5 for i in range(len(labels))],labels=labels)\n",
    "plt.title(\"Correlation Matrix\");\n",
    "plt.colorbar();\n",
    "print('Correlations for the Chevy Silverado 1500')\n",
    "display(silverado_1500.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Silverado 1500 Multiple Linear Regression\n",
    "print('Multiple linear regression Chevy Silverado 1500')\n",
    "df_ols = sm.ols(formula=\"price ~ year + mileage + liters + cylinders + n_doors + n_pics + views + favorites + title_num + seller_num + transmission_num + drive_num\", data=silverado_1500, missing='drop').fit()\n",
    "display(df_ols.summary())\n",
    "print(' We explained 94% of the variance when including all variables in this model to predict the list price of Chevy Silverado 1500. \\n \\n')\n",
    "\n",
    "print('Multiple linear regression Chevy Silverado 1500 remove insignificant variables')\n",
    "df_ols = sm.ols(formula=\"price ~ year + mileage + liters + drive_num\", data=silverado_1500, missing='drop').fit()\n",
    "display(df_ols.summary())\n",
    "print(' Our model explains 89% of the variance after removing the unsiginificant variables. \\n \\n')\n",
    "\n",
    "print('Simple linear regression of expected price Chevy Silverado 1500')\n",
    "df_ols = sm.ols(formula=\"price ~ expected_price\", data=silverado_1500, missing='drop').fit()\n",
    "display(df_ols.summary())\n",
    "print('77% of the variance is explained when comparing the expected price to the list price. The model we developed above is explains more variance than the CarGurus expected price. \\n \\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression Interpretation\n",
    "For the most popular car, the Ford F-150, we were able to make a multiple linear regression model that explained 76% of the variance in list price using the independent variables of year, mileage, liters, cylinders, n_pics, views, favorites, and drive (4-wheel drive vs 2-wheel drive). This explained more of the variance than just comparing the list price to the car gurus expected price.\n",
    "\n",
    "For the second most popular car, the Chevy Silverado 1500, our multiple linear regression model explained 89% of the variance in list price using the variables year, mileage, liters, and drive. It is interesting that this model explained more of the variance with less variables. When we included all of the variables we were able to explain 94% of the variance, but this model is probably overfit. \n",
    "\n",
    "These regression models for individual cars explain a lot of the variance in this dataset and could be used to predict the expected price of cars on ksl based on their year, mileage, liters, cylinders, and drive. These expected list prices could be compared to the list price to alert potential buyers to good/bad deals instead of using a website like CarGurus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering\n",
    "We plan to cluster what we classify as a ‚Äúgood deal‚Äù in its respective geographical location and create clusters showing areas in Utah where cars are generally sold for a good deal. We're going to create a heat map that displays the average percent difference between the cargurus expected price and the list price to find geographical locations of good deals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = SearchEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in a city, state column\n",
    "cars['citystate_abb'] = cars[['city', 'state']].apply(lambda x: ', '.join(x.astype(str)), axis=1)\n",
    "# Combine make and model for new column\n",
    "cars['make_model'] = cars[['make', 'model']].apply(lambda x: ' '.join(x.astype(str)), axis=1)\n",
    "\n",
    "# only get non-null expected price rows\n",
    "good_cars = cars[cars.expected_price.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load zip code coordinates\n",
    "with open(os.path.join(os.getcwd(),'maps','zip_coord_api.pkl'),'rb') as handle:\n",
    "    zip_coord = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse the ZIP:lat/long dictionary\n",
    "coor_zip = {str(round(v[0], 3))+str(round(v[1], 3)): k for k, v in zip_coord.items()}\n",
    "\n",
    "if len(coor_zip) != len(zip_coord):\n",
    "    raise ValueError('the reversed ZIP code dictionary had a different number of keys than the original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the data by ZIP code and generate some summary statistics for the map\n",
    "good_cars = good_cars.copy()\n",
    "good_cars['price_diff'] =  good_cars['price'] / good_cars['expected_price']\n",
    "good_cars['price_cat'] = pd.qcut(good_cars['price_diff'], 3, labels=[\"green\", \"orange\", \"red\"])\n",
    "good_cars['price_abs_cat'] = pd.qcut(good_cars['price'], 3, labels=[\"green\", \"orange\", \"red\"])\n",
    "good_cars['price_label'] = pd.qcut(good_cars['price_diff'], 3, labels=[\"Good\", \"Average\", \"Bad\"])\n",
    "good_cars['price_abs_label'] = pd.qcut(good_cars['price'], 3, labels=[\"Cheap\", \"Average\", \"Expensive\"])\n",
    "\n",
    "good_cars['zip_code'] = good_cars['zip_code'].astype(int)\n",
    "zip_group = good_cars.groupby('zip_code')\n",
    "\n",
    "zip_group_mode = zip_group.agg(pd.Series.mode)\n",
    "zip_group_median = zip_group.agg(pd.Series.median)\n",
    "zip_group_count = zip_group.agg('count').iloc[:,0]\n",
    "zip_group_count_log = np.floor(np.log(zip_group_count)+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate an interactive map that categorizes how good city prices are based on $\\frac{KSL\\,listed\\,price}{CarGurus\\,expected\\,price}$\n",
    "\n",
    "* Color encodes whether cars in that area are generally a good deal\n",
    "    * Red = bad\n",
    "    * Orange = average\n",
    "    * Green = good\n",
    "* Circle size encodes how many cars are listed for a given location\n",
    "* Click on each colored circle to display summary statistics for a given city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an interactive map that categorizes how good city prices are based on the ratio of KSL listed price over CarGurus expected price\n",
    "\n",
    "utah_center = [39.3210, -111.0937]\n",
    "zoom = 6\n",
    "m = Map(basemap=basemaps.OpenStreetMap.Mapnik, center=utah_center, zoom=zoom)\n",
    "m.add_control(FullScreenControl())\n",
    "\n",
    "def click_disp(event, type, coordinates):\n",
    "    # use coordinates to reverse look up data associated with those coordinates\n",
    "    try:\n",
    "        currzip = coor_zip[str(round(coordinates[0], 3))+str(round(coordinates[1], 3))]\n",
    "    except:\n",
    "        currzip = int(search.by_coordinates(40.4, -112, radius=30, returns=1)[0].zipcode)\n",
    "    currdata = good_cars.loc[good_cars['zip_code'] == currzip]\n",
    "    currlabel = zip_group_mode.loc[currzip, 'citystate_abb']\n",
    "    currtotal = \"{:,}\".format(zip_group_count.loc[currzip])\n",
    "    currmedprice = \"{:,}\".format(int(zip_group_median.loc[currzip, 'price']))\n",
    "    currmedmile = \"{:,}\".format(int(zip_group_median.loc[currzip, 'mileage']))\n",
    "    currmedyear = str(int(zip_group_median.loc[currzip, 'year']))\n",
    "    if isinstance(zip_group_mode.loc[currzip, 'make_model'], str):\n",
    "        currcommcar = zip_group_mode.loc[currzip, 'make_model']\n",
    "        mid_msg = '<tr><td>Most Common Car:&emsp;</td><td>' + currcommcar + '</td></tr>'\n",
    "    else:\n",
    "        currcommcar = zip_group_mode.loc[currzip, 'make_model'][0]\n",
    "        mid_msg = '<tr><td>Most Common Car:&emsp;</td><td>' + currcommcar + '</td></tr>'\n",
    "        for currcommcar in zip_group_mode.loc[currzip, 'make_model'][1:]:\n",
    "            mid_msg += '<tr><td>&emsp;</td><td>' + currcommcar + '</td></tr>'\n",
    "    if isinstance(zip_group_mode.loc[currzip, 'price_label'], str):\n",
    "        currdeal = zip_group_mode.loc[currzip, 'price_label']\n",
    "    else:\n",
    "        currdeal = zip_group_mode.loc[currzip, 'price_label'][0]\n",
    "    \n",
    "    # remove old popup layer\n",
    "    if isinstance(m.layers[-1], Popup):\n",
    "        m.remove_layer(m.layers[-1])\n",
    "\n",
    "    # add a popup layer on hover over a city\n",
    "    message = HTML()\n",
    "    \n",
    "    upper_msg = ('<h4><strong>' + currlabel + '</strong></h4>' +\n",
    "                     '<table>' +\n",
    "                         '<tr><td>Total Cars:&emsp;</td><td>' + currtotal + '</td></tr>' +\n",
    "                         '<tr><td>Median Price:&emsp;</td><td>$' + currmedprice + '</td></tr>' +\n",
    "                         '<tr><td>Median Mileage:&emsp;</td><td>' + currmedmile + '</td></tr>' +\n",
    "                         '<tr><td>Median Year:&emsp;</td><td>' + currmedyear + '</td></tr>')\n",
    "    lower_msg = ('<tr><td>Overall Market:&emsp;</td><td>' + currdeal + '</td></tr>' +\n",
    "                     '</table>')\n",
    "\n",
    "    message.value = upper_msg + mid_msg + lower_msg\n",
    "    \n",
    "    popup = Popup(location=coordinates, child=message, close_button=False, auto_close=True, close_on_escape_key=False)\n",
    "\n",
    "    m.add_layer(popup) # add the new layer\n",
    "\n",
    "# create a layer group \n",
    "layer_group = LayerGroup()\n",
    "for zipp, coord in zip_coord.items():\n",
    "    circle = CircleMarker()\n",
    "    circle.location = coord\n",
    "    circle.radius = int(zip_group_count_log[zipp])\n",
    "    circle.weight = 2\n",
    "    circle.opacity = 0.8\n",
    "    if isinstance(zip_group_mode.loc[zipp,'price_cat'], str):\n",
    "        color = zip_group_mode.loc[zipp,'price_cat']\n",
    "    else:\n",
    "        color = zip_group_mode.loc[zipp,'price_cat'][0]\n",
    "    circle.color = color\n",
    "    circle.fill_color = color\n",
    "    circle.fill_opacity = 0.3\n",
    "    layer_group.add_layer(circle)\n",
    "    circle.on_click(click_disp)\n",
    "\n",
    "    \n",
    "\n",
    "m.add_layer(layer_group)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate a different interactive map that categorizes how good city prices are based on all KSL listed prices\n",
    "\n",
    "* Color encodes how cars in that area generally compare to others on KSL\n",
    "    * Red = expensive\n",
    "    * Orange = average\n",
    "    * Green = cheap\n",
    "* Circle size encodes how many cars are listed for a given location\n",
    "* Click on each colored circle to display summary statistics for a given city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a different interactive map that categorizes how good city prices are based on all KSL listed prices\n",
    "\n",
    "utah_center = [39.3210, -111.0937]\n",
    "zoom = 6\n",
    "m2 = Map(basemap=basemaps.OpenStreetMap.Mapnik, center=utah_center, zoom=zoom)\n",
    "m2.add_control(FullScreenControl())\n",
    "\n",
    "def click_disp(event, type, coordinates):\n",
    "    # use coordinates to reverse look up data associated with those coordinates\n",
    "    try:\n",
    "        currzip = coor_zip[str(round(coordinates[0], 3))+str(round(coordinates[1], 3))]\n",
    "    except:\n",
    "        currzip = int(search.by_coordinates(40.4, -112, radius=30, returns=1)[0].zipcode)\n",
    "    currdata = good_cars.loc[good_cars['zip_code'] == currzip]\n",
    "    currlabel = zip_group_mode.loc[currzip, 'citystate_abb']\n",
    "    currtotal = \"{:,}\".format(zip_group_count.loc[currzip])\n",
    "    currmedprice = \"{:,}\".format(int(zip_group_median.loc[currzip, 'price']))\n",
    "    currmedmile = \"{:,}\".format(int(zip_group_median.loc[currzip, 'mileage']))\n",
    "    currmedyear = str(int(zip_group_median.loc[currzip, 'year']))\n",
    "    if isinstance(zip_group_mode.loc[currzip, 'make_model'], str):\n",
    "        currcommcar = zip_group_mode.loc[currzip, 'make_model']\n",
    "        mid_msg = '<tr><td>Most Common Car:&emsp;</td><td>' + currcommcar + '</td></tr>'\n",
    "    else:\n",
    "        currcommcar = zip_group_mode.loc[currzip, 'make_model'][0]\n",
    "        mid_msg = '<tr><td>Most Common Car:&emsp;</td><td>' + currcommcar + '</td></tr>'\n",
    "        for currcommcar in zip_group_mode.loc[currzip, 'make_model'][1:]:\n",
    "            mid_msg += '<tr><td>&emsp;</td><td>' + currcommcar + '</td></tr>'\n",
    "    if isinstance(zip_group_mode.loc[currzip, 'price_abs_label'], str):\n",
    "        currdeal = zip_group_mode.loc[currzip, 'price_abs_label']\n",
    "    else:\n",
    "        currdeal = zip_group_mode.loc[currzip, 'price_abs_label'][0]\n",
    "        \n",
    "    # remove old popup layer\n",
    "    if isinstance(m2.layers[-1], Popup):\n",
    "        m2.remove_layer(m2.layers[-1])\n",
    "\n",
    "    # add a popup layer on hover over a city\n",
    "    message = HTML()\n",
    "    \n",
    "    upper_msg = ('<h4><strong>' + currlabel + '</strong></h4>' +\n",
    "                     '<table>' +\n",
    "                         '<tr><td>Total Cars:&emsp;</td><td>' + currtotal + '</td></tr>' +\n",
    "                         '<tr><td>Median Price:&emsp;</td><td>$' + currmedprice + '</td></tr>' +\n",
    "                         '<tr><td>Median Mileage:&emsp;</td><td>' + currmedmile + '</td></tr>' +\n",
    "                         '<tr><td>Median Year:&emsp;</td><td>' + currmedyear + '</td></tr>')\n",
    "    lower_msg = ('<tr><td>Overall Market:&emsp;</td><td>' + currdeal + '</td></tr>' +\n",
    "                     '</table>')\n",
    "\n",
    "    message.value = upper_msg + mid_msg + lower_msg\n",
    "    \n",
    "    popup = Popup(location=coordinates, child=message, close_button=False, auto_close=True, close_on_escape_key=False)\n",
    "\n",
    "    m2.add_layer(popup) # add the new layer\n",
    "\n",
    "# create a layer group \n",
    "layer_group = LayerGroup()\n",
    "for zipp, coord in zip_coord.items():\n",
    "    circle = CircleMarker()\n",
    "    circle.location = coord\n",
    "    circle.radius = int(zip_group_count_log[zipp])\n",
    "    circle.weight = 2\n",
    "    circle.opacity = 0.8\n",
    "    if isinstance(zip_group_mode.loc[zipp,'price_abs_cat'], str):\n",
    "        color = zip_group_mode.loc[zipp,'price_abs_cat']\n",
    "    else:\n",
    "        color = zip_group_mode.loc[zipp,'price_abs_cat'][0]\n",
    "    circle.color = color\n",
    "    circle.fill_color = color\n",
    "    circle.fill_opacity = 0.3\n",
    "    layer_group.add_layer(circle)\n",
    "    circle.on_click(click_disp)\n",
    "\n",
    "    \n",
    "\n",
    "m2.add_layer(layer_group)\n",
    "m2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Schedule\n",
    "\n",
    "#### February 24th - 28th\n",
    "* Check data accessibility (robots.txt and terms of conditions) \n",
    "* Basic info due Wed Feb 26th\n",
    "* Project Proposal due Fri Feb 28th\n",
    "#### March 2nd - 6th\n",
    "* Download html files for all recent listings from ksl\n",
    "* Begin data scraping and create one dataframe with each row as a listing\n",
    "* Get/give peer feedback March 5th\n",
    "* Written feedback from staff by March 8th\n",
    "#### March 9th - 13th (Spring Break)\n",
    "* Finish data scraping \n",
    "* Exploratory analysis\n",
    "* Describe \n",
    "\n",
    "#### March 16th - 20th\n",
    "* Exploratory analysis\n",
    "    * Scatter Matrix\n",
    "        * Interpret histograms - check if there are any outliers that could be an error from scraping\n",
    "        * Interpret correlations\n",
    "    * Heatmap of Correlation Matrix\n",
    "        * Interpret Correlations\n",
    "\n",
    "#### March 23rd - 27th\n",
    "* Write up project milestone\n",
    "* Project milestone due March 29th \n",
    "* Acquired, cleaned data, EDA, Sketches of your analysis methods, Submit zip file with Jupyter Notebook, data, other resources.\n",
    "\n",
    "#### March 30th - April 3rd \n",
    "* Get staff feedback\n",
    "* Begin testbed for good deal predictions based on relation to scraped historical dataset\n",
    "\n",
    "#### April 6th - April 10th\n",
    "* Finalize predictive model for new listings\n",
    "* Script and film project video\n",
    "\n",
    "#### April 13th - 17th\n",
    "* Polish up repository in preparation for final submission\n",
    "* Edit and finalize project video\n",
    "* Project Due Sunday April 19th\n",
    "* Project awards April 21st\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peer Feedback\n",
    "Our Reviewers: Kyle Cornwall, Shushanna Mkrtychyan\n",
    "\n",
    "* This is pretty similar to cargurus.com and KBB. How is this different than those existing sites?\n",
    "\n",
    "* How do you know if a car has been in an accident?\n",
    "\n",
    "* Look for granularity of NADAguides and devise ways that we can \"beat\" that model.\n",
    "\n",
    "* Consider doing feature transformation when doing regression.\n",
    "\n",
    "* Can you enhance the dataset with some other website?\n",
    "\n",
    "* What features do other car valuation websites use to generate their price predictions?\n",
    "\n",
    "* Can you get Carfax info from VIN? (without breaking the bank)\n",
    "\n",
    "* Three potential classes when predicting a value (good, average, bad)\n",
    "\n",
    "* Might need to downselect the number of cars we can predict prices for since our dataset size could be limited (i.e. top 20 most frequent cars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video\n",
    "\n",
    "Add link to final video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
