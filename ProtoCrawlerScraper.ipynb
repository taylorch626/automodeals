{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do:\n",
    "* Carve up carscraper() function into subfunctions where possible:\n",
    "    * Checking for validity of \\*\\*kwargs could be a separate function\n",
    "    * Checking for lititle and livalue type could be a separate function\n",
    "* Keep a database or dictionary of IP:user-agent combos to avoid using different user-agents for the same IP?\n",
    "* Figure out how to use API for getting user-agents (to avoid IP ban from user-agent website)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # if this isn't installed, use pip install beautifulsoup4\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "import progressbar # if this isn't installed, use pip install progressbar2\n",
    "import random\n",
    "from selenium import webdriver # if not installed, do pip install selenium\n",
    "from itertools import cycle\n",
    "import os\n",
    "# import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure chromedriver.exe has been added to the `PATH` before running the generateProxies() function. You can do so using [this guide](https://zwbetz.com/download-chromedriver-binary-and-add-to-your-path-for-automated-functional-testing/). The driver itself is located in the repository at `~/automodeals/selenium/chromedriver.exe`\n",
    "\n",
    "**Importantly**, make sure that the chromedriver version used (e.g. 80) is the same as the full Chrome version you have installed (e.g. 80).\n",
    "\n",
    "Chromedriver can be downloaded [here](https://sites.google.com/a/chromium.org/chromedriver/downloads)\n",
    "\n",
    "Chrome version can be found [here](https://www.whatismybrowser.com/detect/what-version-of-chrome-do-i-have)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateProxies():\n",
    "    # Get list of US-based proxy IPs and ports using selenium\n",
    "\n",
    "    IPurl = \"https://www.us-proxy.org/\" # <-- the robots.txt file for this site allows full access for all user-agents\n",
    "\n",
    "    # Specify incognito options for Chrome\n",
    "    option = webdriver.ChromeOptions()\n",
    "    option.add_argument(\"--incognito\")\n",
    "\n",
    "    # Create new Chrome instance\n",
    "    browser = webdriver.Chrome(options=option)\n",
    "\n",
    "    # Minimize window\n",
    "    browser.minimize_window()\n",
    "\n",
    "    # Go to desired website\n",
    "    IPurl = \"https://www.us-proxy.org/\" # <-- the robots.txt file for this site allows full access for all user-agents\n",
    "    browser.get(IPurl)\n",
    "\n",
    "    # Filter by https only\n",
    "    https_button = browser.find_elements_by_xpath(\"//*[@id='proxylisttable']/tfoot/tr/th[7]/select/option[3]\")[0]\n",
    "    https_button.click()\n",
    "\n",
    "    # Set to 80 results\n",
    "    maxnum_button = browser.find_elements_by_xpath(\"//*[@id='proxylisttable_length']/label/select/option[3]\")[0]\n",
    "    maxnum_button.click()\n",
    "\n",
    "    # Grab IP's and Ports from the resulting table\n",
    "    rows = browser.find_elements_by_xpath(\"//*[@id='proxylisttable']/tbody/tr\")\n",
    "\n",
    "    proxies = set() # using a set ensures there aren't duplicates\n",
    "    for row in rows:\n",
    "        row = row.text.split(' ')\n",
    "\n",
    "        if row[3].strip().lower() != 'transparent': # don't want to include our real proxy when navigating KSL\n",
    "            proxies.add(''.join(['http://', ':'.join([row[0].strip(), row[1].strip()])]))\n",
    "\n",
    "    # Close browser when done\n",
    "    browser.close()\n",
    "\n",
    "    return proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function for the scraping done for each search page\n",
    "\n",
    "def carscraper(**kwargs):\n",
    "    '''VARIABLE INPUTS:\n",
    "    url: should be of the form \"https://cars.ksl.com/search/newUsed/Used;Certified/perPage/96/page/0\"\n",
    "    rooturl: should be something like \"https://cars.ksl.com\"\n",
    "    maxts: the maximum timestamp of the all_cars repository\n",
    "    use_proxy: a boolean or binary to indicate if a proxy should be used\n",
    "    curr_proxy: a string indicating the current proxy IP from last function call\n",
    "    proxydict: a dictionary of proxy IPs and associated user-agents to cycle through\n",
    "    refreshmin: the number of minutes to wait before updating the proxy pool\n",
    "    \n",
    "    ***NOTE: This function is meant to work with a pool of proxy IPs and various spoofed user-agents'''\n",
    "    \n",
    "    # Need to spoof a user-agent in order to get past crawler block\n",
    "    user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36'\n",
    "    \n",
    "    # the following were pulled manually on 3/12/20 from https://www.whatismybrowser.com/guides/the-latest-user-agent/\n",
    "    user_agents = ['Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36',\n",
    "                   'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36',\n",
    "                   'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36',\n",
    "                   'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:54.0) Gecko/20100101 Firefox/74.0',\n",
    "                   'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.13; rv:61.0) Gecko/20100101 Firefox/74.0',\n",
    "                   'Mozilla/5.0 (X11; Linux i586; rv:31.0) Gecko/20100101 Firefox/74.0',\n",
    "                   'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0 Safari/605.1.15',\n",
    "                   'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36 Edg/80.0.361.62',\n",
    "                   'Mozilla/5.0 (Windows NT 10.0; Trident/7.0; rv:11.0) like Gecko']    \n",
    "    \n",
    "    \n",
    "    # Parse the kwargs\n",
    "\n",
    "    \n",
    "    if 'url' in kwargs.keys():\n",
    "        if isinstance(kwargs['url'],str):\n",
    "            url = kwargs['url']\n",
    "        else:\n",
    "            raise TypeError(f'Expected string for url but got {type(kwargs[\"url\"])}.')\n",
    "    else:\n",
    "        raise ValueError('url is a required input for carscraper().')\n",
    "        \n",
    "    if 'rooturl' in kwargs.keys():\n",
    "        if isinstance(kwargs['rooturl'],str):\n",
    "            rooturl = kwargs['rooturl']\n",
    "        else:\n",
    "            raise TypeError(f'Expected string for rooturl but got {type(kwargs[\"rooturl\"])}.')\n",
    "    else:\n",
    "        raise ValueError('rooturl is a required input for carscraper().')\n",
    "        \n",
    "    if 'maxts' in kwargs.keys():\n",
    "        if isinstance(kwargs['maxts'],np.int64) or isinstance(kwargs['maxts'],int) or isinstance(kwargs['maxts'],float):\n",
    "            maxts = kwargs['maxts']\n",
    "        else:\n",
    "            raise TypeError(f'Expected np.int64 or int for maxts but got {type(kwargs[\"maxts\"])}.')\n",
    "    else:\n",
    "        raise ValueError('maxts is a required input for carscraper().')\n",
    "        \n",
    "    if 'use_proxy' in kwargs.keys():\n",
    "        if isinstance(kwargs['use_proxy'],int) or isinstance(kwargs['use_proxy'],bool):\n",
    "            use_proxy = kwargs['use_proxy']\n",
    "        else:\n",
    "            raise TypeError(f'Expected int or bool for use_proxy but got {type(kwargs[\"use_proxy\"])}.')\n",
    "    else:\n",
    "        # default is to NOT use proxy\n",
    "        use_proxy = False\n",
    "        \n",
    "    if use_proxy:\n",
    "        # The following inputs are only useful when using a proxy\n",
    "        \n",
    "        if 'proxydict' in kwargs.keys():\n",
    "            if isinstance(kwargs['proxydict'],dict):\n",
    "                proxydict = kwargs['proxydict']\n",
    "            else:\n",
    "                print(f'Expected dict type for proxydict but got {type(kwargs[\"proxydict\"])}. Generating new proxydict...')\n",
    "                newproxies = generateProxies()\n",
    "                proxydict = {i:random.choice(user_agents) for i in newproxies}\n",
    "        else:\n",
    "            print('No proxydict found. Generating...')\n",
    "            newproxies = generateProxies()\n",
    "            proxydict = {i:random.choice(user_agents) for i in newproxies}\n",
    "\n",
    "        if 'refreshmin' in kwargs.keys():\n",
    "            if isinstance(kwargs['refreshmin'],int) or isinstance(kwargs['refreshmin'],float):\n",
    "                refreshmin = kwargs['refreshmin']\n",
    "            else:\n",
    "                refreshmin = 15\n",
    "                print(f'Expected int or float for refreshmin but got {type(kwargs[\"refreshmin\"])}. Set to default value of {refreshmin}.')\n",
    "        else:\n",
    "            refreshmin = 15\n",
    "            print(f'No refreshmin found. Set to default value of {refreshmin}.')\n",
    "        \n",
    "    \n",
    "    \n",
    "    if use_proxy:\n",
    "        tstart = time.time() # set a start time to use for refreshing proxy list (if needed)    \n",
    "\n",
    "        if 'currproxy' in kwargs.keys():\n",
    "            if isinstance(kwargs['currproxy'],str):\n",
    "                currproxy = kwargs['currproxy']\n",
    "            else:\n",
    "                proxy_pool = cycle(proxydict) # make a pool of proxies \n",
    "                currproxy = next(proxy_pool) # grab the next proxy in cycle\n",
    "        else:\n",
    "            proxy_pool = cycle(proxydict) # make a pool of proxies \n",
    "            currproxy = next(proxy_pool) # grab the next proxy in cycle                \n",
    "\n",
    "\n",
    "        attempts = len(proxydict) # for now, limit the total number of attempts to one per proxy. This will prevent endless while loop\n",
    "        chkproxy = 1\n",
    "        while chkproxy and attempts:\n",
    "            if (time.time() - tstart) > 60*refreshmin: # check if it's been more than refreshmin minutes since proxy_pool updated\n",
    "                print('Refreshing proxy pool...')\n",
    "\n",
    "                currproxies = set(proxydict.keys())\n",
    "                newproxies = generateProxies()\n",
    "                newproxies = newproxies.difference(currproxies)\n",
    "\n",
    "                if newproxies:\n",
    "                    newdict = {i:random.choice(user_agents) for i in newproxies}\n",
    "                    proxydict.update(newdict)\n",
    "                    proxy_pool = cycle(proxydict)\n",
    "                    currproxy = next(proxy_pool)\n",
    "                    print('Proxy pool updated!')\n",
    "\n",
    "            try:\n",
    "                resp = requests.get(url,proxies={\"http\":currproxy, \"https\":currproxy},headers={'User-Agent': proxydict[currproxy]}, timeout=20)\n",
    "                print(f'Proxy success for {currproxy}')\n",
    "                print()\n",
    "                chkproxy = 0\n",
    "                attempts += 1\n",
    "            except:\n",
    "                prevproxy = currproxy\n",
    "                currproxy = next(proxy_pool)\n",
    "                print(f'Proxy error for {prevproxy}! Next up is {currproxy}')\n",
    "                attempts -= 1\n",
    "                print(f'Attempts remaining: {attempts}')\n",
    "                \n",
    "    else:\n",
    "        # don't use the proxy\n",
    "        resp = requests.get(url, headers = {'User-Agent': user_agent})\n",
    "        \n",
    "    html = resp.content\n",
    "    pgsoup = BeautifulSoup(html)\n",
    "    \n",
    "    # Check if there are additional pages of results\n",
    "    if pgsoup.find(\"a\", {\"title\" : \"Go forward 1 page\"}):\n",
    "        moreresults = 1\n",
    "    else:\n",
    "        moreresults = 0\n",
    "    \n",
    "    links = pgsoup.select(\"div.title > a.link\") # grab all 96 (or up to 96) links\n",
    "#     tstamps = pgsoup.select(\"div.listing-detail-line script\") # grab all 96 (or up to 96) timestamps\n",
    "    # ^^^ this line no longer works as of Mar 17, 2020 due to timestamp being used on KSL backend rather than with frontend js\n",
    "\n",
    "    # Loop through links and scrape data for each new listing\n",
    "    all_cars = []\n",
    "    with progressbar.ProgressBar(max_value=len(links)) as bar:\n",
    "        for idx, link in enumerate(links): # *** only load first x results for now to avoid ban before implementing spoofing\n",
    "\n",
    "            # Reset all fields to None before next loop\n",
    "            price=year=make=model=body=mileage=title_type=city=state=seller=None\n",
    "            trim=ext_color=int_color=transmission=liters=cylinders=fuel_type=n_doors=ext_condition=int_condition=drive_type=None\n",
    "\n",
    "            # We're going to want to strip the \"?ad_cid=[number]\" from the end of these links as they're not needed to load the page properly\n",
    "            # Regular expressions should come in handy here\n",
    "\n",
    "            cutidx = re.search('(\\?ad_cid=.+)',link['href']).start()\n",
    "            currlink = link['href'][:cutidx]\n",
    "\n",
    "            # Somewhere here we should do a check to make sure that the timestamp for currlink is newer than our newest file in our repository\n",
    "            # That is, compare the timestamps with a simple conditional, where if the conditional is not met, this loop breaks to avoid useless computation time\n",
    "\n",
    "            # Generate full link for the current listing\n",
    "            fulllink = '/'.join([rooturl.rstrip('/'), currlink.lstrip('/')])\n",
    "\n",
    "            if use_proxy:\n",
    "                attempts = len(proxydict) # for now, limit the total number of attempts to one per proxy. This will prevent endless while loop\n",
    "                chkproxy = 1\n",
    "                while chkproxy and attempts:\n",
    "                    if (time.time() - tstart) > 60*refreshmin: # check if it's been more than refreshmin minutes since proxy_pool updated\n",
    "                        print('Refreshing proxy pool...')\n",
    "\n",
    "                        currproxies = set(proxydict.keys())\n",
    "                        newproxies = generateProxies()\n",
    "                        newproxies = newproxies.difference(currproxies)\n",
    "\n",
    "                        if newproxies:\n",
    "                            newdict = {i:random.choice(user_agents) for i in newproxies}\n",
    "                            proxydict.update(newdict)\n",
    "                            proxy_pool = cycle(proxydict)\n",
    "                            currproxy = next(proxy_pool)\n",
    "                            print('Proxy pool updated!')\n",
    "\n",
    "                    try:\n",
    "                        resp = requests.get(fulllink,proxies={\"http\":currproxy, \"https\":currproxy},headers={'User-Agent': proxydict[currproxy]}, timeout=20)\n",
    "                        print(f'Proxy success for {currproxy}')\n",
    "                        print()\n",
    "                        chkproxy = 0\n",
    "                        attempts += 1\n",
    "                    except:\n",
    "                        prevproxy = currproxy\n",
    "                        currproxy = next(proxy_pool)\n",
    "                        print(f'Proxy error for {prevproxy}! Next up is {currproxy}')\n",
    "                        attempts -= 1\n",
    "                        print(f'Attempts remaining: {attempts}')\n",
    "                        \n",
    "            else:\n",
    "                # don't use the proxy\n",
    "                resp = requests.get(fulllink, headers = {'User-Agent': user_agent})\n",
    "            \n",
    "            \n",
    "            lsthtml = resp.content\n",
    "            lstsoup = BeautifulSoup(lsthtml)\n",
    "            \n",
    "            # Check if link is still good (i.e. listing is still active)\n",
    "            if lstsoup.title.text.strip().lower() == 'not found':\n",
    "                print('Bad link. Skipping...')\n",
    "                bar.update(idx)\n",
    "            else:\n",
    "\n",
    "                # Get timestamp <-- no longer works as of March 17, 2020 due to removal of frontend js calculation of display date\n",
    "#                 tstamp = int(re.search('(\\d+)',tstamps[idx].text).group(0))\n",
    "\n",
    "                # Get post date\n",
    "                poststr = lstsoup.select('h2.location')[0].text.strip()\n",
    "                poststr = re.search(r'Posted\\s([\\w\\s\\d,]+)',poststr)\n",
    "                poststr = poststr.group(1)\n",
    "                postdate = datetime.strptime(poststr, '%B %d, %Y') # Convert to type to datetime\n",
    "                \n",
    "                # Check if date is newer than maxts (with some leniency for same day)\n",
    "                if datetime.timestamp(postdate) < maxts:\n",
    "                    print('************ Found end of new data ************')\n",
    "                    moreresults = 0\n",
    "                    break\n",
    "\n",
    "                # Get listing price\n",
    "                price = lstsoup.select('h3.price')[0].text.strip().replace('$','').replace(',','')\n",
    "\n",
    "                # Get seller's location\n",
    "                if lstsoup.select('h2.location > a'):\n",
    "                    location = lstsoup.select('h2.location > a')[0].text.strip()\n",
    "                    city, state = location.split(',')\n",
    "                    city = city.strip()\n",
    "                    state = state.strip()\n",
    "\n",
    "                # Get seller type (dealer or owner)\n",
    "                sellerstr = lstsoup.select('div.fsbo')[0].text.strip()\n",
    "                if re.search('(Dealer)', sellerstr):\n",
    "                    seller = 'Dealer'\n",
    "                elif re.search('(Owner)', sellerstr):\n",
    "                    seller = 'Owner'\n",
    "                    \n",
    "                # Get number of photos\n",
    "                if lstsoup.select('div.slider-uninitialized > p'):\n",
    "                    picstr = lstsoup.select('div.slider-uninitialized > p')[0].text.strip()\n",
    "                    n_pics = int(re.search('(\\d+)',picstr).group())\n",
    "                else:\n",
    "                    if lstsoup.find(id='widgetPhoto').p:\n",
    "                        picstr = lstsoup.find(id='widgetPhoto').p.text.strip()\n",
    "                        n_pics = int(re.search('(\\d+)',picstr).group())\n",
    "                    else:\n",
    "                        n_pics = 0\n",
    "\n",
    "                # Get table of car specs\n",
    "                specs = lstsoup.select('ul.listing-specifications')\n",
    "\n",
    "                for li in specs[0].find_all('li'):\n",
    "                    lititle = li.select('span.title')[0].text.strip().strip(':')\n",
    "                    livalue = li.select('span.value')[0].text.strip().strip(':')\n",
    "\n",
    "                    if livalue.lower() == 'not specified':\n",
    "                        livalue = None\n",
    "\n",
    "                    # Now a bunch of if-else statements to determine which column to add data to\n",
    "                    # There might be a more sophisticated way to do this, perhaps with a tuple or a dictionary?\n",
    "                    if lititle.lower() == 'year':\n",
    "                        if livalue:\n",
    "                            year = int(livalue)\n",
    "                        else:\n",
    "                            year = livalue\n",
    "                    elif lititle.lower() == 'make':\n",
    "                        make = livalue\n",
    "                    elif lititle.lower() == 'model':\n",
    "                        model = livalue\n",
    "                    elif lititle.lower() == 'body':\n",
    "                        body = livalue\n",
    "                    elif lititle.lower() == 'mileage':\n",
    "                        if livalue:\n",
    "                            mileage = int(livalue.replace(',',''))\n",
    "                        else:\n",
    "                            mileage = livalue\n",
    "                    elif lititle.lower() == 'title type':\n",
    "                        title_type = livalue\n",
    "\n",
    "                    # Below this are non-required specs    \n",
    "                    elif lititle.lower() == 'trim':\n",
    "                        trim = livalue\n",
    "                    elif lititle.lower() == 'exterior color':\n",
    "                        if livalue:\n",
    "                            ext_color = livalue.lower()\n",
    "                        else:\n",
    "                            ext_color = livalue\n",
    "                    elif lititle.lower() == 'interior color':\n",
    "                        if livalue:\n",
    "                            int_color = livalue.lower()\n",
    "                        else:\n",
    "                            int_color = livalue\n",
    "                    elif lititle.lower() == 'transmission':\n",
    "                        transmission = livalue\n",
    "                    elif lititle.lower() == 'liters':\n",
    "                        try:\n",
    "                            liters = float(livalue)\n",
    "                        except:\n",
    "                            if livalue:\n",
    "                                try:\n",
    "                                    str1 = re.search('^(.*?)L',livalue).group(0).strip().replace(' ','')\n",
    "                                    if re.search('^(\\D+)',str1):\n",
    "                                        idxend = re.search('^(\\D+)',str1).end()\n",
    "                                        livalue = str1[idxend:-1]\n",
    "                                        if re.search('(\\D+)',livalue): # check if still other pollutants\n",
    "                                            idxend = re.search('(\\D+)',livalue).end()\n",
    "                                            livalue = livalue[idxend:]\n",
    "                                    else:\n",
    "                                        livalue = str1[:-1]\n",
    "                                    try:\n",
    "                                        livalue = float(livalue)\n",
    "                                    except:\n",
    "                                        # save to error log\n",
    "                                        err_df = pd.DataFrame({'timestamp':[datetime.fromtimestamp(time.time())], 'link':[fulllink], 'liters_str':[livalue]})\n",
    "\n",
    "                                        # Check to see if liters_error_log already exists\n",
    "                                        if os.path.isfile('errors/liters_error_log.csv'):\n",
    "#                                             print('found file. appending to existing file')\n",
    "                                            err_df.to_csv('errors/liters_error_log.csv', mode='a', index=False, header=False)\n",
    "                                        else:\n",
    "#                                             print('no file found. creating new file')\n",
    "                                            err_df.to_csv('errors/liters_error_log.csv', index=False)\n",
    "                                except:\n",
    "                                    # save to error log\n",
    "                                    err_df = pd.DataFrame({'timestamp':[datetime.fromtimestamp(time.time())], 'link':[fulllink], 'liters_str':[livalue]})\n",
    "                                    \n",
    "                                    # Check to see if liters_error_log already exists\n",
    "                                    if os.path.isfile('errors/liters_error_log.csv'):\n",
    "#                                         print('found file. appending to existing file')\n",
    "                                        err_df.to_csv('errors/liters_error_log.csv', mode='a', index=False, header=False)\n",
    "                                    else:\n",
    "#                                         print('no file found. creating new file')\n",
    "                                        err_df.to_csv('errors/liters_error_log.csv', index=False)                                   \n",
    "                                    \n",
    "                                    pass\n",
    "#                                     print('couldn't parse liters info')\n",
    "                            else:\n",
    "                                liters = livalue\n",
    "                    elif lititle.lower() == 'cylinders':\n",
    "                        if livalue:\n",
    "                            cylinders = int(livalue)\n",
    "                        else:\n",
    "                            cylinders = livalue\n",
    "                    elif lititle.lower() == 'fuel type':\n",
    "                        fuel_type = livalue\n",
    "                    elif lititle.lower() == 'number of doors':\n",
    "                        if livalue:\n",
    "                            n_doors = int(livalue)\n",
    "                        else:\n",
    "                            n_doors = livalue\n",
    "                    elif lititle.lower() == 'exterior condition':\n",
    "                        ext_condition = livalue\n",
    "                    elif lititle.lower() == 'interior condition':\n",
    "                        int_condition = livalue\n",
    "                    elif lititle.lower() == 'drive type':\n",
    "                        drive_type = livalue\n",
    "                    elif (lititle.lower() == 'vin'):\n",
    "                        VIN = livalue\n",
    "                    elif (lititle.lower() == 'stock number') | (lititle.lower() == 'dealer license'):\n",
    "                        None # Don't want to save these\n",
    "                    else:\n",
    "                        None\n",
    "                        print(f'Unmatched param {lititle}: {livalue}') # <-- could take advantage of some or all of these\n",
    "\n",
    "                curr_car = pd.DataFrame({\"post_date\":[postdate],\n",
    "                                         \"lastpull_ts\":[int(time.time())],\n",
    "                                         \"link\":[fulllink],\n",
    "                                         \"price\":[price],\n",
    "                                         \"year\":[year],\n",
    "                                         \"make\":[make],\n",
    "                                         \"model\":[model],\n",
    "                                         \"body\":[body],\n",
    "                                         \"mileage\":[mileage],\n",
    "                                         \"title_type\":[title_type],\n",
    "                                         \"city\":[city],\n",
    "                                         \"state\":[state],\n",
    "                                         \"seller\":[seller],\n",
    "                                         \"trim\":[trim],\n",
    "                                         \"ext_color\":[ext_color],\n",
    "                                         \"int_color\":[int_color],\n",
    "                                         \"transmission\":[transmission],\n",
    "                                         \"liters\":[liters],\n",
    "                                         \"cylinders\":[cylinders],\n",
    "                                         \"fuel_type\":[fuel_type],\n",
    "                                         \"n_doors\":[n_doors],\n",
    "                                         \"ext_condition\":[ext_condition],\n",
    "                                         \"int_condition\":[int_condition],\n",
    "                                         \"drive_type\":[drive_type],\n",
    "                                         \"VIN\":[VIN],\n",
    "                                         \"n_pics\":[n_pics]})\n",
    "                try:\n",
    "                    all_cars = pd.concat([all_cars, curr_car])\n",
    "                except:\n",
    "                    all_cars = curr_car\n",
    "\n",
    "                bar.update(idx)\n",
    "\n",
    "    if type(all_cars) is pd.core.frame.DataFrame: # make sure that some data was actually scraped\n",
    "        all_cars = all_cars.reset_index()\n",
    "        del all_cars['index']\n",
    "        all_cars.fillna(value=pd.np.nan, inplace=True)\n",
    "    if use_proxy:\n",
    "        return all_cars, moreresults, currproxy, proxydict\n",
    "    else:\n",
    "        return all_cars, moreresults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function to remove duplicated dataframe rows (based on url link) before saving to .csv\n",
    "\n",
    "def removeduplicates(df):\n",
    "    \n",
    "    oldsize = df.shape[0]\n",
    "    bool_series = df['link'].duplicated()\n",
    "    if bool_series.any():\n",
    "        df = df[~bool_series]\n",
    "        newsize = df.shape[0]\n",
    "        df = df.reset_index()\n",
    "        del df['index']\n",
    "        print(f'Removed {oldsize - newsize} duplicates before saving')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a multi-page test using carscraper function\n",
    "\n",
    "# determine whether or not to use proxy IPs. Can use boolean or int for this\n",
    "use_proxy = False\n",
    "\n",
    "# set cap for number of search pages to load (i.e. pages with up to 96 listings)\n",
    "maxpg = 2\n",
    "\n",
    "# Define root url for KSL cars\n",
    "rooturl = \"https://cars.ksl.com\"\n",
    "\n",
    "# Note the url below specifies that we're looking for 96 per page and the default sort of newest to oldest posting\n",
    "# This note about newest to oldest is useful so that we can avoid scraping repeat listings based on their timestamps\n",
    "# Also note that this url does NOT have a page number associated with it. This is added in the while loop below\n",
    "lurl = \"https://cars.ksl.com/search/newUsed/Used;Certified/perPage/96/page/\"\n",
    "\n",
    "count = 0\n",
    "all_cars = []\n",
    "while count < maxpg:\n",
    "    url = lurl + str(count)\n",
    "    try:\n",
    "        if use_proxy:\n",
    "            curr_cars, moreresults, currproxy, proxydict = carscraper(url=url, rooturl=rooturl, maxts=0, use_proxy=use_proxy, currproxy=currproxy, refreshmin = 15, proxydict = proxydict)\n",
    "        else:\n",
    "            curr_cars, moreresults = carscraper(url=url, rooturl=rooturl, maxts=0, use_proxy=False)\n",
    "        \n",
    "    except:\n",
    "        if use_proxy:\n",
    "            curr_cars, moreresults, currproxy, proxydict = carscraper(url=url, rooturl=rooturl, maxts=0, use_proxy=use_proxy, refreshmin = 15)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    \n",
    "    count += 1    \n",
    "#     print(f'More results? {moreresults}')\n",
    "    if type(curr_cars) is pd.core.frame.DataFrame: # make sure real data was returned\n",
    "        try:\n",
    "            all_cars = pd.concat([all_cars, curr_cars], ignore_index=True)\n",
    "        except:\n",
    "            all_cars = curr_cars\n",
    "    else:\n",
    "        print('No car data found!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any duplicate rows\n",
    "\n",
    "all_cars = removeduplicates(all_cars)\n",
    "all_cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe to csv\n",
    "\n",
    "all_cars.to_csv('data/all_cars.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframe\n",
    "all_cars = pd.read_csv('data/all_cars.csv')\n",
    "\n",
    "# Convert post_date back to datetime\n",
    "all_cars['post_date'] = pd.to_datetime(all_cars['post_date'])\n",
    "\n",
    "# get most recent timestamp from the dataframe\n",
    "rep_ts = datetime.timestamp(all_cars['post_date'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now scrape for more cars and check for timestamp\n",
    "\n",
    "# Define root url for KSL cars\n",
    "rooturl = \"https://cars.ksl.com\"\n",
    "\n",
    "lurl = \"https://cars.ksl.com/search/newUsed/Used;Certified/perPage/96/page/\"\n",
    "\n",
    "count = 0\n",
    "newer_cars = []\n",
    "moreresults = 1\n",
    "while moreresults:\n",
    "    url = lurl + str(count)\n",
    "    \n",
    "    try:\n",
    "        if use_proxy:\n",
    "            curr_cars, moreresults, currproxy, proxydict = carscraper(url=url, rooturl=rooturl, maxts=rep_ts, use_proxy=use_proxy, currproxy=currproxy, refreshmin = 15, proxydict = proxydict)\n",
    "        else:\n",
    "            curr_cars, moreresults = carscraper(url=url, rooturl=rooturl, maxts=rep_ts, use_proxy=use_proxy)\n",
    "    except:\n",
    "        if use_proxy:\n",
    "            curr_cars, moreresults, currproxy, proxydict = carscraper(url=url, rooturl=rooturl, maxts=0, use_proxy=use_proxy, refreshmin = 15)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    count += 1    \n",
    "#     print(f'More results? {moreresults}')\n",
    "    if type(curr_cars) is pd.core.frame.DataFrame: # make sure real data was returned\n",
    "        try:\n",
    "            newer_cars = pd.concat([newer_cars, curr_cars], ignore_index=True)\n",
    "        except:\n",
    "            newer_cars = curr_cars\n",
    "    else:\n",
    "        print('No newer car data found!')\n",
    "    \n",
    "# add newer_cars\n",
    "if type(newer_cars) is pd.core.frame.DataFrame:\n",
    "    all_cars_UPDATED = pd.concat([newer_cars, all_cars], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any duplicate rows\n",
    "\n",
    "all_cars_UPDATED = removeduplicates(all_cars_UPDATED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save updated dataframe to csv\n",
    "\n",
    "all_cars_UPDATED.to_csv('data/all_cars_UPDATED.csv', index=False) # in later iterations, remove the \"_UPDATED\" part of filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load updated dataframe\n",
    "all_cars_UPDATED = pd.read_csv('data/all_cars_UPDATED.csv') # in later iterations, remove the \"_UPDATED\" part of filename\n",
    "all_cars_UPDATED\n",
    "\n",
    "# Convert post_date back to datetime\n",
    "all_cars_UPDATED['post_date'] = pd.to_datetime(all_cars_UPDATED['post_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sandbox code before implementing in main loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_listing_info(cars_df, **kwargs):\n",
    "    '''Updates a cars_dataframe with 6 new columns (views, favorites, \n",
    "    workingURL, view_rate, favorite_rate, fav_per_view). \n",
    "    REQUIRED INPUTS:\n",
    "    cars_df: data frame with information as pulled from carscraper()\n",
    "    VARIABLE INPUTS:\n",
    "    min_age: int specifying minimum age (days) listing must be before updating information. Default 3\n",
    "    min_last_pull: int specifying minimum time (days) since last pull for new information. Default 1'''\n",
    "    \n",
    "    if 'min_age' in kwargs.keys():\n",
    "        if isinstance(kwargs['min_age'],int):\n",
    "            min_age = kwargs['min_age']\n",
    "        else:\n",
    "            raise TypeError(f'Expected int for min_age but got {type(kwargs[\"min_age\"])}.')\n",
    "    else:\n",
    "        min_age = 3\n",
    "    if 'min_last_pull' in kwargs.keys():\n",
    "        if isinstance(kwargs['min_last_pull'],int):\n",
    "            min_last_pull = kwargs['min_last_pull']\n",
    "        else:\n",
    "            raise TypeError(f'Expected int for min_last_pull but got {type(kwargs[\"min_last_pull\"])}.')\n",
    "    else:\n",
    "        min_last_pull = 1\n",
    "\n",
    "    # new columns to add\n",
    "    cars_df['views'] = np.NaN\n",
    "    cars_df['favorites'] = np.NaN\n",
    "    cars_df['workingURL'] = 1\n",
    "    cars_df['view_rate'] = np.NaN\n",
    "    cars_df['favorite_rate'] = np.NaN\n",
    "    cars_df['fav_per_view'] = np.NaN\n",
    "\n",
    "    # find ads more than x days old (time.time() is in seconds)\n",
    "    curr_time = int(time.time())\n",
    "    min_dt = min_age*60*60*24 # time in seconds for use with datetime\n",
    "    old_ads = cars_df['timestamp'] < (curr_time - min_dt)\n",
    "\n",
    "    # find ads that haven't been pulled for more than x days\n",
    "    min_last_pull = 2\n",
    "    min_last_pull_dt = min_last_pull*60*60*24 # time in seconds for use with datetime\n",
    "    no_recent_update = cars_df['lastpull_ts'] < (curr_time - min_last_pull_dt)\n",
    "\n",
    "    # subselect ads that need updating based on previous criteria and having a working URL last time it was checked\n",
    "    cars_need_update = cars_df[old_ads & no_recent_update & cars_df['workingURL']]\n",
    "\n",
    "    user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'\n",
    "\n",
    "    # iterate through, pulling new information from each ad\n",
    "    last_pull = []\n",
    "    views = []\n",
    "    favorites = []\n",
    "    working_url = []\n",
    "    with progressbar.ProgressBar(max_value=len(cars_need_update.index)) as bar:\n",
    "        for i, ad in cars_need_update.iterrows():\n",
    "            ad_response = requests.get(ad['link'], headers = {'User-Agent': user_agent})\n",
    "            pull_ts = int(time.time())\n",
    "            last_pull.append(pull_ts)\n",
    "            ad_soup = BeautifulSoup(ad_response.content)\n",
    "\n",
    "            # Check if link is still good (i.e. listing is still active)\n",
    "            if ad_soup.title.text.strip().lower() == 'not found':\n",
    "                working_url.append(0)\n",
    "                views.append(None)\n",
    "                favorites.append(None)\n",
    "            else:\n",
    "                working_url.append(1)\n",
    "\n",
    "                # get views\n",
    "                viewcount = int(ad_soup.select('span.vdp-info-value')[1].text.split()[0])\n",
    "                views.append(viewcount)\n",
    "\n",
    "                # get favorites\n",
    "                favoritecount = int(ad_soup.select('span.vdp-info-value')[2].text.split()[0])\n",
    "                favorites.append(favoritecount)\n",
    "            bar.update(i)\n",
    "\n",
    "    cars_updated = cars_need_update\n",
    "    cars_updated['views'] = views\n",
    "    cars_updated['favorites'] = favorites\n",
    "    cars_updated['lastpull_ts'] = last_pull\n",
    "    cars_updated['workingURL'] = working_url\n",
    "    cars_updated['fav_per_view'] = cars_updated['favorites'] / cars_updated['views']\n",
    "    # rates calculated per day\n",
    "    cars_updated['view_rate'] = cars_updated['views'] / (cars_updated['lastpull_ts'] - cars_updated['timestamp']) * 60*60*24\n",
    "    cars_updated['favorite_rate'] = cars_updated['favorites'] / (cars_updated['lastpull_ts'] - cars_updated['timestamp']) * 60*60*24\n",
    "\n",
    "    cars_df.update(cars_updated)\n",
    "    \n",
    "    return cars_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_df = pd.read_csv('data/archive/all_cars.csv')\n",
    "# cars_df = cars_df.iloc[:20,:]\n",
    "cars_df = update_listing_info(cars_df,min_age=0,min_last_pull=0)\n",
    "cars_df.to_csv('data/archive/all_cars_new_info.csv')\n",
    "cars_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_df = pd.read_csv('data/archive/all_cars.csv')\n",
    "cars_df = cars_df.iloc[:20,:]\n",
    "cars_df = update_listing_info(cars_df,min_age=0,min_last_pull=0)\n",
    "cars_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_listing_info(cars_df):\n",
    "    '''Updates a cars_dataframe with 6 new columns (views, favorites, \n",
    "    workingURL, view_rate, favorite_rate, fav_per_view). Can easily be modified to accept different arguments'''\n",
    "\n",
    "    # new columns to add\n",
    "    cars_df['views'] = np.NaN\n",
    "    cars_df['favorites'] = np.NaN\n",
    "    cars_df['workingURL'] = 1\n",
    "    cars_df['view_rate'] = np.NaN\n",
    "    cars_df['favorite_rate'] = np.NaN\n",
    "    cars_df['fav_per_view'] = np.NaN\n",
    "\n",
    "    # find ads more than x days old (time.time() is in seconds)\n",
    "    curr_time = int(time.time())\n",
    "    min_days = 3\n",
    "    min_dt = min_days*60*60*24 # time in seconds for use with datetime\n",
    "    old_ads = cars_df['timestamp'] < (curr_time - min_dt)\n",
    "\n",
    "    # find ads that haven't been pulled for more than x days\n",
    "    min_last_pull = 2\n",
    "    min_last_pull_dt = min_last_pull*60*60*24 # time in seconds for use with datetime\n",
    "    no_recent_update = cars_df['lastpull_ts'] < (curr_time - min_last_pull_dt)\n",
    "\n",
    "    # subselect ads that need updating based on previous criteria and having a working URL last time it was checked\n",
    "    cars_need_update = cars_df[old_ads & no_recent_update & cars_df['workingURL']]\n",
    "#     cars_need_update = cars_need_update[:3] # temporary for development\n",
    "\n",
    "    user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'\n",
    "\n",
    "    # iterate through, pulling new information from each ad\n",
    "    last_pull = []\n",
    "    views = []\n",
    "    favorites = []\n",
    "    working_url = []\n",
    "    for _, ad in cars_need_update.iterrows():\n",
    "        ad_response = requests.get(ad['link'], headers = {'User-Agent': user_agent})\n",
    "        pull_ts = int(time.time())\n",
    "        last_pull.append(pull_ts)\n",
    "        ad_soup = BeautifulSoup(ad_response.content)\n",
    "\n",
    "        # Check if link is still good (i.e. listing is still active)\n",
    "        if ad_soup.title.text.strip().lower() == 'not found':\n",
    "            working_url.append(0)\n",
    "            views.append(None)\n",
    "            favorites.append(None)\n",
    "        else:\n",
    "            working_url.append(1)\n",
    "\n",
    "            # get views\n",
    "            viewcount = int(ad_soup.select('span.vdp-info-value')[1].text.split()[0])\n",
    "            views.append(viewcount)\n",
    "\n",
    "            # get favorites\n",
    "            favoritecount = int(ad_soup.select('span.vdp-info-value')[2].text.split()[0])\n",
    "            favorites.append(favoritecount)\n",
    "\n",
    "    cars_updated = cars_need_update\n",
    "    cars_updated['views'] = views\n",
    "    cars_updated['favorites'] = favorites\n",
    "    cars_updated['lastpull_ts'] = last_pull\n",
    "    cars_updated['workingURL'] = working_url\n",
    "    cars_updated['fav_per_view'] = cars_updated['favorites'] / cars_updated['views']\n",
    "    # rates calculated per day\n",
    "    cars_updated['view_rate'] = cars_updated['views'] / (cars_updated['lastpull_ts'] - cars_updated['timestamp']) * 60*60*24\n",
    "    cars_updated['favorite_rate'] = cars_updated['favorites'] / (cars_updated['lastpull_ts'] - cars_updated['timestamp']) * 60*60*24\n",
    "\n",
    "    cars_df.update(cars_updated)\n",
    "    \n",
    "    return cars_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_df = pd.read_csv('data/all_cars_UPDATED.csv')\n",
    "cars_df = update_listing_info(cars_df)\n",
    "cars_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log errors when trying to parse liters info (which tends to be quite variable for some reason)\n",
    "\n",
    "link1 = 'https://cars.ksl.com/car001'\n",
    "link2 = 'https://cars.ksl.com/car002'\n",
    "\n",
    "text1 = 'car 001: this would be the place that the liters info could be parsed from for car 001'\n",
    "text2 = 'car 002: this would be the place that the liters info could be parsed from for car 002'\n",
    "\n",
    "df1 = pd.DataFrame({'timestamp':[datetime.fromtimestamp(time.time())], 'link':[link1], 'liters_str':[text1]})\n",
    "df2 = pd.DataFrame({'timestamp':[datetime.fromtimestamp(time.time())], 'link':[link2], 'liters_str':[text2]})\n",
    "\n",
    "# Save dataframe to csv\n",
    "df1.to_csv('errors/liters_error_log.csv', index=False)\n",
    "\n",
    "# Check to see if liters_error_log already exists\n",
    "\n",
    "if os.path.isfile('errors/liters_error_log.csv'):\n",
    "    print('found file. appending to existing file')\n",
    "    df2.to_csv('errors/liters_error_log.csv', mode='a', index=False, header=False)\n",
    "else:\n",
    "    print('no file found. creating new file')\n",
    "    df2.to_csv('errors/liters_error_log.csv', index=False)\n",
    "    \n",
    "pd.read_csv('errors/liters_error_log.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function to determine how long to run restart repository function\n",
    "\n",
    "def determinemaxpg():\n",
    "    url = \"https://cars.ksl.com/search/newUsed/Used;Certified/perPage/96/page/0\"\n",
    "    \n",
    "    # Need to spoof a user-agent in order to get past crawler block\n",
    "    user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36'\n",
    "    \n",
    "    resp = requests.get(url, headers = {'User-Agent': user_agent})\n",
    "        \n",
    "    html = resp.content\n",
    "    pgsoup = BeautifulSoup(html)\n",
    "    \n",
    "    # Get last page of results (useful when restarting repository from scratch to know when to stop)\n",
    "    return int(pgsoup.find(attrs={\"title\": \"Go to last page\"}).text.strip()) # Note that this is 1 more than number from href for this page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy code to figure out how to find which proxies are new when refreshing list\n",
    "set1 = set(['a','b','c','d','e'])\n",
    "set2 = set(['b','c','e','f','h'])\n",
    "# set2 = set(['a','b','c','d','e'])\n",
    "newset = set2.difference(set1) # return elements in set2 that aren't in set1\n",
    "if newset:\n",
    "    print(newset)\n",
    "else:\n",
    "    print('No new elements in set 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding a way to try out a new proxy without moving on to the next href until success\n",
    "\n",
    "url = \"https://cars.ksl.com/search/newUsed/Used;Certified/perPage/96/page/0\"\n",
    "# url = \"https://httpbin.org/ip\"\n",
    "\n",
    "# user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'\n",
    "\n",
    "# the following were pulled manually on 3/12/20 from https://www.whatismybrowser.com/guides/the-latest-user-agent/\n",
    "user_agents = ['Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36',\n",
    "               'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36',\n",
    "               'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36',\n",
    "               'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:54.0) Gecko/20100101 Firefox/74.0',\n",
    "               'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.13; rv:61.0) Gecko/20100101 Firefox/74.0',\n",
    "               'Mozilla/5.0 (X11; Linux i586; rv:31.0) Gecko/20100101 Firefox/74.0',\n",
    "               'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0 Safari/605.1.15',\n",
    "               'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36 Edg/80.0.361.62',\n",
    "               'Mozilla/5.0 (Windows NT 10.0; Trident/7.0; rv:11.0) like Gecko']\n",
    "\n",
    "if not proxies:\n",
    "    proxies = generateProxies()\n",
    "\n",
    "# make a dictionary of proxies and user-agents using dictionary comprehension\n",
    "proxydict = {i:random.choice(user_agents) for i in proxies}\n",
    "\n",
    "proxy_pool = cycle(proxydict)\n",
    "currproxy = next(proxy_pool)\n",
    "\n",
    "attempts = len(proxies) # for now, limit the total number of attempts to one per proxy. This will prevent endless while loop\n",
    "for i in range(5):\n",
    "    chkproxy = 1\n",
    "    while chkproxy and attempts:\n",
    "        try:\n",
    "            resp = requests.get(url,proxies={\"http\":currproxy, \"https\":currproxy},headers={'User-Agent': proxydict[currproxy]}, timeout=20)\n",
    "            html = resp.content\n",
    "            print()\n",
    "            if url == \"https://httpbin.org/ip\":\n",
    "                print(html)\n",
    "            else:\n",
    "                pgsoup = BeautifulSoup(html)\n",
    "                links = pgsoup.select(\"div.title > a.link\") # grab all 96 (or up to 96) links\n",
    "                print(f'Number of links found: {len(links)}')\n",
    "            chkproxy = 0\n",
    "        except:\n",
    "            prevproxy = currproxy\n",
    "            currproxy = next(proxy_pool)\n",
    "            print(f'Proxy error for {prevproxy}! Next up is {currproxy}')\n",
    "            attempts -= 1\n",
    "            print(f'Attempts remaining: {attempts}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with proxy and user-agent combos\n",
    "\n",
    "url = \"https://cars.ksl.com/search/newUsed/Used;Certified/perPage/96/page/0\"\n",
    "# url = \"https://httpbin.org/ip\"\n",
    "\n",
    "# user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'\n",
    "\n",
    "# the following were pulled manually on 3/12/20 from https://www.whatismybrowser.com/guides/the-latest-user-agent/\n",
    "user_agents = ['Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36',\n",
    "               'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36',\n",
    "               'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36',\n",
    "               'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:54.0) Gecko/20100101 Firefox/74.0',\n",
    "               'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.13; rv:61.0) Gecko/20100101 Firefox/74.0',\n",
    "               'Mozilla/5.0 (X11; Linux i586; rv:31.0) Gecko/20100101 Firefox/74.0',\n",
    "               'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0 Safari/605.1.15',\n",
    "               'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36 Edg/80.0.361.62',\n",
    "               'Mozilla/5.0 (Windows NT 10.0; Trident/7.0; rv:11.0) like Gecko']\n",
    "\n",
    "proxies = generateProxies()\n",
    "\n",
    "# make a dictionary of proxies and user-agents using dictionary comprehension\n",
    "proxydict = {i:random.choice(user_agents) for i in proxies}\n",
    "\n",
    "proxy_pool = cycle(proxydict)\n",
    "for i in range(30):\n",
    "    currproxy = next(proxy_pool)\n",
    "    try:\n",
    "        resp = requests.get(url,proxies={\"http\":currproxy, \"https\":currproxy},headers={'User-Agent': proxydict[currproxy]}, timeout=15)\n",
    "        html = resp.content\n",
    "        print()\n",
    "        if url == \"https://httpbin.org/ip\":\n",
    "            print(html)\n",
    "        else:\n",
    "            pgsoup = BeautifulSoup(html)\n",
    "            links = pgsoup.select(\"div.title > a.link\") # grab all 96 (or up to 96) links\n",
    "            print(f'Number of links found: {len(links)}')\n",
    "        print(f'Success! proxy used: {currproxy}')\n",
    "        print()\n",
    "    except:\n",
    "        print(f'Proxy error! proxy used: {currproxy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figuring out number of photos on listing page\n",
    "# url = 'https://cars.ksl.com/listing/6269343' # 19 photos (seller)\n",
    "# url = 'https://cars.ksl.com/listing/6101875' # 26 photos (dealer)\n",
    "# url = 'https://cars.ksl.com/listing/6304284' # no photos (dealer)\n",
    "url = 'https://cars.ksl.com/listing/6302069' # 1 photo (seller)\n",
    "\n",
    "user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36'\n",
    "\n",
    "resp = requests.get(url, headers = {'User-Agent': user_agent})\n",
    "lsthtml = resp.content\n",
    "lstsoup = BeautifulSoup(lsthtml)\n",
    "    \n",
    "if lstsoup.select('div.slider-uninitialized > p'):\n",
    "    picstr = lstsoup.select('div.slider-uninitialized > p')[0].text.strip()\n",
    "    n_pics = int(re.search('(\\d+)',picstr).group())\n",
    "else:\n",
    "    if lstsoup.find(id='widgetPhoto').p:\n",
    "        picstr = lstsoup.find(id='widgetPhoto').p.text.strip()\n",
    "        n_pics = int(re.search('(\\d+)',picstr).group())\n",
    "    else:\n",
    "        n_pics = 0\n",
    "        \n",
    "print(f'Number of photos found: {n_pics}')\n",
    "print(type(n_pics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a multi-page test using carscraper function\n",
    "\n",
    "# set cap for number of search pages to load (i.e. pages with up to 96 listings)\n",
    "maxpg = 2\n",
    "\n",
    "# Define root url for KSL cars\n",
    "rooturl = \"https://cars.ksl.com\"\n",
    "\n",
    "# Note the url below specifies that we're looking for 96 per page and the default sort of newest to oldest posting\n",
    "# This note about newest to oldest is useful so that we can avoid scraping repeat listings based on their timestamps\n",
    "# Also note that this url does NOT have a page number associated with it. This is added in the while loop below\n",
    "lurl = \"https://cars.ksl.com/search/newUsed/Used;Certified/perPage/96/page/\"\n",
    "\n",
    "count = 0\n",
    "all_cars = []\n",
    "while count < maxpg:\n",
    "    url = lurl + str(count)\n",
    "#     curr_cars, moreresults = carscraper(url, rooturl, 0)\n",
    "    \n",
    "#     curr_cars, moreresults = carscraperproxy(url, rooturl, 0)\n",
    "    try:\n",
    "        curr_cars, moreresults, proxydict = carscraperproxy(url, rooturl, 0, refreshmin = 15, proxydict = proxydict)\n",
    "    except:\n",
    "        curr_cars, moreresults, proxydict = carscraperproxy(url, rooturl, 0, refreshmin = 15)\n",
    "    \n",
    "    count += 1    \n",
    "#     print(f'More results? {moreresults}')\n",
    "    if type(curr_cars) is pd.core.frame.DataFrame: # make sure real data was returned\n",
    "        try:\n",
    "            all_cars = pd.concat([curr_cars, all_cars], ignore_index=True)\n",
    "        except:\n",
    "            all_cars = curr_cars\n",
    "    else:\n",
    "        print('No car data found!')\n",
    "    \n",
    "all_cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Playing around with determining if car listing is still good (or if it's been removed)\n",
    "testurl = \"https://cars.ksl.com/listing/9999999\"\n",
    "\n",
    "user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36'\n",
    "\n",
    "resp = requests.get(testurl, headers = {'User-Agent': user_agent})\n",
    "lsthtml = resp.content\n",
    "lstsoup = BeautifulSoup(lsthtml)\n",
    "\n",
    "if lstsoup.title.text.strip().lower() == 'not found':\n",
    "    print('bad link')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Playing around with checking time to see when to refresh proxy list\n",
    "tstart = time.time()\n",
    "time.sleep(5) # time in seconds\n",
    "tend = time.time() - tstart\n",
    "tend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Playing around with timestamps for use when checking for new data\n",
    "\n",
    "# print(datetime.fromtimestamp(all_cars['timestamp'][0]).isoformat())\n",
    "print(datetime.now())\n",
    "currtime = time.time()\n",
    "print(currtime)\n",
    "print(datetime.fromtimestamp(currtime).isoformat())\n",
    "print()\n",
    "print(datetime.fromtimestamp(time.time()))\n",
    "print(datetime.today())\n",
    "print()\n",
    "testdate = datetime(1971,4,25).date() # <-- super handy to get rid of annoying time info!\n",
    "print(testdate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below this is old code, some of which is no longer useful. Proceed with caution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "### DEPRECATED CODE: All functionality built into new optional proxy-based function ###\n",
    "#######################################################################################\n",
    "\n",
    "# Make a function for the scraping done for each search page\n",
    "\n",
    "def carscraper(url, rooturl, maxts):\n",
    "    '''INPUTS:\n",
    "    url should be of the form \"https://cars.ksl.com/search/newUsed/Used;Certified/perPage/96/page/0\"\n",
    "    rooturl should be something like \"https://cars.ksl.com\"\n",
    "    maxts is the maximum timestamp of the all_cars repository\n",
    "    \n",
    "    ***NOTE: This function is meant to work with original IP address (as opposed to proxy) and a single spoofed user-agent'''\n",
    "    \n",
    "    # Need to spoof a user-agent in order to get past crawler block\n",
    "    user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36'\n",
    "\n",
    "    resp = requests.get(url, headers = {'User-Agent': user_agent})\n",
    "    html = resp.content\n",
    "    pgsoup = BeautifulSoup(html)\n",
    "    \n",
    "    # Check if there are additional pages of results\n",
    "    if pgsoup.find(\"a\", {\"title\" : \"Go forward 1 page\"}):\n",
    "        moreresults = 1\n",
    "    else:\n",
    "        moreresults = 0\n",
    "    \n",
    "    links = pgsoup.select(\"div.title > a.link\") # grab all 96 (or up to 96) links\n",
    "    tstamps = pgsoup.select(\"div.listing-detail-line script\") # grab all 96 (or up to 96) timestamps\n",
    "\n",
    "    # Loop through links and scrape data for each new listing\n",
    "    all_cars = []\n",
    "    with progressbar.ProgressBar(max_value=len(links)) as bar:\n",
    "        for idx, link in enumerate(links): # *** only load first x results for now to avoid ban before implementing spoofing\n",
    "\n",
    "            # Reset all fields to None before next loop\n",
    "            price=year=make=model=body=mileage=title_type=city=state=seller=None\n",
    "            trim=ext_color=int_color=transmission=liters=cylinders=fuel_type=n_doors=ext_condition=int_condition=drive_type=None\n",
    "\n",
    "            # We're going to want to strip the \"?ad_cid=[number]\" from the end of these links as they're not needed to load the page properly\n",
    "            # Regular expressions should come in handy here\n",
    "\n",
    "            cutidx = re.search('(\\?ad_cid=.+)',link['href']).start()\n",
    "            currlink = link['href'][:cutidx]\n",
    "\n",
    "            # Somewhere here we should do a check to make sure that the timestamp for currlink is newer than our newest file in our repository\n",
    "            # That is, compare the timestamps with a simple conditional, where if the conditional is not met, this loop breaks to avoid useless computation time\n",
    "\n",
    "            # Open listing link and pull html from it\n",
    "            fulllink = '/'.join([rooturl.rstrip('/'), currlink.lstrip('/')])\n",
    "\n",
    "            resp = requests.get(fulllink, headers = {'User-Agent': user_agent})\n",
    "            lsthtml = resp.content\n",
    "            lstsoup = BeautifulSoup(lsthtml)\n",
    "            \n",
    "            # Check if link is still good (i.e. listing is still active)\n",
    "            if lstsoup.title.text.strip().lower() == 'not found':\n",
    "                print('Bad link. Skipping...')\n",
    "                bar.update(idx)\n",
    "            else:\n",
    "\n",
    "                # Get timestamp\n",
    "                tstamp = int(re.search('(\\d+)',tstamps[idx].text).group(0))\n",
    "\n",
    "                # Check if timestamp is newer than maxts\n",
    "                if tstamp <= maxts:\n",
    "                    print('************ Found end of new data ************')\n",
    "#                     print(f'var type of all_cars is: {type(all_cars)}')\n",
    "                    moreresults = 0\n",
    "                    break\n",
    "#                 else:\n",
    "#                     print(f'New car found: {idx} in link {fulllink}')\n",
    "\n",
    "                # Get listing price\n",
    "                price = lstsoup.select('h3.price')[0].text.strip().replace('$','').replace(',','')\n",
    "\n",
    "                # Get seller's location\n",
    "                if lstsoup.select('h2.location > a'):\n",
    "                    location = lstsoup.select('h2.location > a')[0].text.strip()\n",
    "                    city, state = location.split(',')\n",
    "                    city = city.strip()\n",
    "                    state = state.strip()\n",
    "\n",
    "                # Get seller type (dealer or owner)\n",
    "                sellerstr = lstsoup.select('div.fsbo')[0].text.strip()\n",
    "                if re.search('(Dealer)', sellerstr):\n",
    "                    seller = 'Dealer'\n",
    "                elif re.search('(Owner)', sellerstr):\n",
    "                    seller = 'Owner'\n",
    "                    \n",
    "                # Get number of photos\n",
    "                if lstsoup.select('div.slider-uninitialized > p'):\n",
    "                    picstr = lstsoup.select('div.slider-uninitialized > p')[0].text.strip()\n",
    "                    n_pics = int(re.search('(\\d+)',picstr).group())\n",
    "                else:\n",
    "                    if lstsoup.find(id='widgetPhoto').p:\n",
    "                        picstr = lstsoup.find(id='widgetPhoto').p.text.strip()\n",
    "                        n_pics = int(re.search('(\\d+)',picstr).group())\n",
    "                    else:\n",
    "                        n_pics = 0\n",
    "\n",
    "                # Get table of car specs\n",
    "                specs = lstsoup.select('ul.listing-specifications')\n",
    "\n",
    "                for li in specs[0].find_all('li'):\n",
    "                    lititle = li.select('span.title')[0].text.strip().strip(':')\n",
    "                    livalue = li.select('span.value')[0].text.strip().strip(':')\n",
    "\n",
    "                    if livalue.lower() == 'not specified':\n",
    "                        livalue = None\n",
    "\n",
    "                    # Now a bunch of if-else statements to determine which column to add data to\n",
    "                    # There might be a more sophisticated way to do this, perhaps with a tuple or a dictionary?\n",
    "                    if lititle.lower() == 'year':\n",
    "                        if livalue:\n",
    "                            year = int(livalue)\n",
    "                        else:\n",
    "                            year = livalue\n",
    "                    elif lititle.lower() == 'make':\n",
    "                        make = livalue\n",
    "                    elif lititle.lower() == 'model':\n",
    "                        model = livalue\n",
    "                    elif lititle.lower() == 'body':\n",
    "                        body = livalue\n",
    "                    elif lititle.lower() == 'mileage':\n",
    "                        if livalue:\n",
    "                            mileage = int(livalue.replace(',',''))\n",
    "                        else:\n",
    "                            mileage = livalue\n",
    "                    elif lititle.lower() == 'title type':\n",
    "                        title_type = livalue\n",
    "\n",
    "                    # Below this are non-required specs    \n",
    "                    elif lititle.lower() == 'trim':\n",
    "                        trim = livalue\n",
    "                    elif lititle.lower() == 'exterior color':\n",
    "                        if livalue:\n",
    "                            ext_color = livalue.lower()\n",
    "                        else:\n",
    "                            ext_color = livalue\n",
    "                    elif lititle.lower() == 'interior color':\n",
    "                        if livalue:\n",
    "                            int_color = livalue.lower()\n",
    "                        else:\n",
    "                            int_color = livalue\n",
    "                    elif lititle.lower() == 'transmission':\n",
    "                        transmission = livalue\n",
    "                    elif lititle.lower() == 'liters':\n",
    "                        try:\n",
    "                            liters = float(livalue)\n",
    "                        except:\n",
    "                            if livalue:\n",
    "                                str1 = re.search('^(.*?)L',livalue).group(0).strip().replace(' ','')\n",
    "                                if re.search('^(\\D+)',str1):\n",
    "                                    idxend = re.search('^(\\D+)',str1).end()\n",
    "                                    livalue = str1[idxend:-1]\n",
    "                                    if re.search('(\\D+)',livalue): # check if still other pollutants\n",
    "                                        idxend = re.search('(\\D+)',livalue).end()\n",
    "                                        livalue = livalue[idxend:]\n",
    "                                else:\n",
    "                                    livalue = str1[:-1]\n",
    "                                try:\n",
    "                                    livalue = float(livalue)\n",
    "                                except:\n",
    "                                    print(url)\n",
    "                                    print('****')\n",
    "                                    print(link)\n",
    "                            else:\n",
    "                                liters = livalue\n",
    "                    elif lititle.lower() == 'cylinders':\n",
    "                        if livalue:\n",
    "                            cylinders = int(livalue)\n",
    "                        else:\n",
    "                            cylinders = livalue\n",
    "                    elif lititle.lower() == 'fuel type':\n",
    "                        fuel_type = livalue\n",
    "                    elif lititle.lower() == 'number of doors':\n",
    "                        if livalue:\n",
    "                            n_doors = int(livalue)\n",
    "                        else:\n",
    "                            n_doors = livalue\n",
    "                    elif lititle.lower() == 'exterior condition':\n",
    "                        ext_condition = livalue\n",
    "                    elif lititle.lower() == 'interior condition':\n",
    "                        int_condition = livalue\n",
    "                    elif lititle.lower() == 'drive type':\n",
    "                        drive_type = livalue\n",
    "                    elif (lititle.lower() == 'vin') | (lititle.lower() == 'stock number') | (lititle.lower() == 'dealer license'):\n",
    "                        None # Don't want to save these\n",
    "                    else:\n",
    "                        None\n",
    "                        print(f'Unmatched param {lititle}: {livalue}') # <-- could take advantage of some or all of these\n",
    "\n",
    "                curr_car = pd.DataFrame({\"timestamp\":[tstamp],\n",
    "                                         \"price\":[price],\n",
    "                                         \"year\":[year],\n",
    "                                         \"make\":[make],\n",
    "                                         \"model\":[model],\n",
    "                                         \"body\":[body],\n",
    "                                         \"mileage\":[mileage],\n",
    "                                         \"title_type\":[title_type],\n",
    "                                         \"city\":[city],\n",
    "                                         \"state\":[state],\n",
    "                                         \"seller\":[seller],\n",
    "                                         \"trim\":[trim],\n",
    "                                         \"ext_color\":[ext_color],\n",
    "                                         \"int_color\":[int_color],\n",
    "                                         \"transmission\":[transmission],\n",
    "                                         \"liters\":[liters],\n",
    "                                         \"cylinders\":[cylinders],\n",
    "                                         \"fuel_type\":[fuel_type],\n",
    "                                         \"n_doors\":[n_doors],\n",
    "                                         \"ext_condition\":[ext_condition],\n",
    "                                         \"int_condition\":[int_condition],\n",
    "                                         \"drive_type\":[drive_type],\n",
    "                                         \"n_pics\":[n_pics]})\n",
    "                try:\n",
    "                    all_cars = pd.concat([curr_car, all_cars])\n",
    "                except:\n",
    "                    all_cars = curr_car\n",
    "\n",
    "                bar.update(idx)\n",
    "\n",
    "    if type(all_cars) is pd.core.frame.DataFrame: # make sure that some data was actually scraped\n",
    "        all_cars = all_cars.reset_index()\n",
    "        del all_cars['index']\n",
    "        all_cars.fillna(value=pd.np.nan, inplace=True)\n",
    "    return all_cars, moreresults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with using proxy IPs from above\n",
    "\n",
    "# url = \"https://cars.ksl.com/search/newUsed/Used;Certified/perPage/96/page/0\"\n",
    "url = \"https://httpbin.org/ip\"\n",
    "\n",
    "user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'\n",
    "\n",
    "for idx, proxy in enumerate(proxies):\n",
    "#     user_agent = UAlist[idx]\n",
    "    try:\n",
    "        resp = requests.get(url,proxies={\"http\":proxy, \"https\":proxy},headers={'User-Agent': user_agent}, timeout=10)\n",
    "        print(resp.content)\n",
    "        print(f'Success! proxy used: {proxy}')\n",
    "    except:\n",
    "        print(f'Proxy error! proxy used: {proxy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "######### MIGHT GET BY WITH JUST PROXY IPs #########\n",
    "####################################################\n",
    "\n",
    "### Revisit this in the future if it becomes an issue\n",
    "### For time being, just use a handful of user-agents\n",
    "\n",
    "# Get list of user-agents\n",
    "\n",
    "### Need to use API at some point rather than crawl/scrape since you can get 500 user-agents for free per month...and my IP got banned\n",
    "### username: automodeals\n",
    "### pw: kslclass123\n",
    "### API documentation: https://developers.whatismybrowser.com/api/docs/v2/\n",
    "\n",
    "API_key = '5ecab60888f7aebfbc4aad5850de52fa'\n",
    "\n",
    "UAurl = \"https://developers.whatismybrowser.com/useragents/explore/software_name/chrome/\"\n",
    "\n",
    "resp = requests.get(UAurl)\n",
    "UAhtml = resp.content\n",
    "UAsoup = BeautifulSoup(UAhtml)\n",
    "\n",
    "UAlist = []\n",
    "matches = UAsoup.select(\"table.table-useragents td.useragent\")\n",
    "for match in matches[:len(proxies)]: # only get as many user-agents are there are proxies. Dangerous to use more than one user-agent per IP\n",
    "    UAlist.append(match.find('a').text.strip())\n",
    "random.shuffle(UAlist)\n",
    "UAlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use user-agents API (example from https://github.com/whatismybrowser/api-v2-sample-code/blob/master/sample-code/python-3.6/user_agent_parse.py)\n",
    "\n",
    "API_key = '5ecab60888f7aebfbc4aad5850de52fa'\n",
    "\n",
    "headers = {'X-API-KEY': API_key}\n",
    "# UAurl = \"https://api.whatismybrowser.com/api/v2/user_agent_database_dump_url\"\n",
    "# UAurl = \"https://api.whatismybrowser.com/api/v2/user_agent_database_search\"\n",
    "\n",
    "# The code below works for POSTing data, but we want to GET data\n",
    "\n",
    "UAurl = \"https://api.whatismybrowser.com/api/v2/user_agent_parse\"\n",
    "\n",
    "post_data = {\n",
    "    \"user_agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3282.167 Safari/537.36\",\n",
    "}\n",
    "\n",
    "result = requests.post(UAurl, data=json.dumps(post_data), headers=headers)\n",
    "result # if result is 200, then success!\n",
    "result.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of working live html parser without crawler block and user-agent spoof\n",
    "\n",
    "# url = \"http://www.python.org\"\n",
    "# resp = requests.get(url)\n",
    "# html = resp.content\n",
    "# print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "####### DEPRECATED AS OF MARCH 12, 2020 #########\n",
    "#################################################\n",
    "\n",
    "# TCH: Many functionalities implemented in the carscraper function have not been copied over to this cell block\n",
    "\n",
    "\n",
    "### Working example for a SINGLE KSL search results page\n",
    "\n",
    "# maxresults = 20 # Set max number of listings to parse (per search results page)\n",
    "\n",
    "# # Define root url for KSL cars\n",
    "# rooturl = \"https://cars.ksl.com\"\n",
    "\n",
    "# # Note the url below specifies that we're looking for 96 per page and the default sort of newest to oldest posting\n",
    "# # This note about newest to oldest is useful so that we can avoid scraping repeat listings based on their timestamps\n",
    "# url = \"https://cars.ksl.com/search/newUsed/Used;Certified/perPage/96/page/0\"\n",
    "\n",
    "# # Need to spoof a user-agent in order to get past crawler block\n",
    "# user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36'\n",
    "\n",
    "# # Note: The above user_agent might need to be rotated (along with IP) to avoid IP ban\n",
    "# # Example found on https://www.scrapehero.com/how-to-fake-and-rotate-user-agents-using-python-3/\n",
    "\n",
    "# all_cars = []\n",
    "\n",
    "# # Open live page (as opposed to downloaded)\n",
    "# resp = requests.get(url, headers = {'User-Agent': user_agent})\n",
    "# html = resp.content\n",
    "# pgsoup = BeautifulSoup(html)\n",
    "# lastpg = int(pgsoup.find(attrs={\"title\": \"Go to last page\"}).text.strip()) # Note that this is 1 more than number from href for this page\n",
    "# # print(f'Total number of search results pages: {lastpg}')\n",
    "# # print()\n",
    "\n",
    "# links = pgsoup.select(\"div.title > a.link\") # grab all 96 (or up to 96) links\n",
    "# # print(f'Total number of links found on current page: {len(links)}')\n",
    "# tstamps = pgsoup.select(\"div.listing-detail-line script\") # grab all 96 (or up to 96) timestamps\n",
    "# # print(f'Total number of timestamps found on current page: {len(tstamps)}')\n",
    "\n",
    "# # print()\n",
    "\n",
    "# # for tstamp in tstamps:\n",
    "# #     print(int(re.search('(\\d+)',tstamp.text).group(0))) # <-- This is WORKING code to extract timestamp for each listing from search page\n",
    "# # print()\n",
    "\n",
    "# print(f'Limiting Subsequent Listing Results to {maxresults}')\n",
    "\n",
    "# # Loop through links and scrape data for each new listing\n",
    "# with progressbar.ProgressBar(max_value=maxresults) as bar:\n",
    "#     for idx, link in enumerate(links[:maxresults]): # *** only load first x results for now to avoid ban before implementing spoofing\n",
    "\n",
    "#         # Reset all fields to None before next loop\n",
    "#         price=year=make=model=body=mileage=title_type=city=state=seller=None\n",
    "#         trim=ext_color=int_color=transmission=liters=cylinders=fuel_type=n_doors=ext_condition=int_condition=drive_type=None\n",
    "        \n",
    "#         # We're going to want to strip the \"?ad_cid=[number]\" from the end of these links as they're not needed to load the page properly\n",
    "#         # Regular expressions should come in handy here\n",
    "\n",
    "#         cutidx = re.search('(\\?ad_cid=.+)',link['href']).start()\n",
    "#         currlink = link['href'][:cutidx]\n",
    "\n",
    "#         # Somewhere here we should do a check to make sure that the timestamp for currlink is newer than our newest file in our repository\n",
    "#         # That is, compare the timestamps with a simple conditional, where if the conditional is not met, this loop breaks to avoid useless computation time\n",
    "\n",
    "#         # Open listing link and pull html from it\n",
    "#         fulllink = '/'.join([rooturl.rstrip('/'), currlink.lstrip('/')])\n",
    "\n",
    "#         resp = requests.get(fulllink, headers = {'User-Agent': user_agent})\n",
    "#         lsthtml = resp.content\n",
    "#         lstsoup = BeautifulSoup(lsthtml)\n",
    "\n",
    "#         # Get listing price\n",
    "#         price = lstsoup.select('h3.price')[0].text.strip().replace('$','').replace(',','')\n",
    "\n",
    "#         # Get seller's location\n",
    "#         location = lstsoup.select('h2.location > a')[0].text.strip()\n",
    "#         city, state = location.split(',')\n",
    "#         city = city.strip()\n",
    "#         state = state.strip()\n",
    "\n",
    "#         # Get seller type (dealer or owner)\n",
    "#         sellerstr = lstsoup.select('div.fsbo')[0].text.strip()\n",
    "#         if re.search('(Dealer)', sellerstr):\n",
    "#             seller = 'Dealer'\n",
    "#         elif re.search('(Owner)', sellerstr):\n",
    "#             seller = 'Owner'\n",
    "\n",
    "#         # Get timestamp\n",
    "#         tstamp = int(re.search('(\\d+)',tstamps[idx].text).group(0))\n",
    "\n",
    "#         # Get table of car specs\n",
    "#         specs = lstsoup.select('ul.listing-specifications')\n",
    "\n",
    "#         for li in specs[0].find_all('li'):\n",
    "#             lititle = li.select('span.title')[0].text.strip().strip(':')\n",
    "#             livalue = li.select('span.value')[0].text.strip().strip(':')\n",
    "            \n",
    "#             if livalue.lower() == 'not specified':\n",
    "#                 livalue = None\n",
    "\n",
    "#             # Now a bunch of if-else statements to determine which column to add data to\n",
    "#             # There might be a more sophisticated way to do this, perhaps with a tuple or a dictionary?\n",
    "#             if lititle.lower() == 'year':\n",
    "#                 if livalue:\n",
    "#                     year = int(livalue)\n",
    "#                 else:\n",
    "#                     year = livalue\n",
    "#             elif lititle.lower() == 'make':\n",
    "#                 make = livalue\n",
    "#             elif lititle.lower() == 'model':\n",
    "#                 model = livalue\n",
    "#             elif lititle.lower() == 'body':\n",
    "#                 body = livalue\n",
    "#             elif lititle.lower() == 'mileage':\n",
    "#                 if livalue:\n",
    "#                     mileage = int(livalue.replace(',',''))\n",
    "#                 else:\n",
    "#                     mileage = livalue\n",
    "#             elif lititle.lower() == 'title type':\n",
    "#                 title_type = livalue\n",
    "                \n",
    "#             # Below this are non-required specs    \n",
    "#             elif lititle.lower() == 'trim':\n",
    "#                 trim = livalue\n",
    "#             elif lititle.lower() == 'exterior color':\n",
    "#                 if livalue:\n",
    "#                     ext_color = livalue.lower()\n",
    "#                 else:\n",
    "#                     ext_color = livalue\n",
    "#             elif lititle.lower() == 'interior color':\n",
    "#                 if livalue:\n",
    "#                     int_color = livalue.lower()\n",
    "#                 else:\n",
    "#                     int_color = livalue\n",
    "#             elif lititle.lower() == 'transmission':\n",
    "#                 transmission = livalue\n",
    "#             elif lititle.lower() == 'liters':\n",
    "#                 try:\n",
    "#                     liters = float(livalue)\n",
    "#                 except:\n",
    "#                     if livalue:\n",
    "#                         str1 = re.search('^(.*?)L',livalue).group(0).strip().replace(' ','')\n",
    "#                         if re.search('^(\\D+)',str1):\n",
    "#                             idxend = re.search('^(\\D+)',str1).end()\n",
    "#                             livalue = str1[idxend:-1]\n",
    "#                         else:\n",
    "#                             livalue = str1[:-1]\n",
    "#                         livalue = float(livalue)\n",
    "#                     else:\n",
    "#                         liters = livalue\n",
    "#             elif lititle.lower() == 'cylinders':\n",
    "#                 if livalue:\n",
    "#                     cylinders = int(livalue)\n",
    "#                 else:\n",
    "#                     cylinders = livalue\n",
    "#             elif lititle.lower() == 'fuel type':\n",
    "#                 fuel_type = livalue\n",
    "#             elif lititle.lower() == 'number of doors':\n",
    "#                 if livalue:\n",
    "#                     n_doors = int(livalue)\n",
    "#                 else:\n",
    "#                     n_doors = livalue\n",
    "#             elif lititle.lower() == 'exterior condition':\n",
    "#                 ext_condition = livalue\n",
    "#             elif lititle.lower() == 'interior condition':\n",
    "#                 int_condition = livalue\n",
    "#             elif lititle.lower() == 'drive type':\n",
    "#                 drive_type = livalue\n",
    "#             elif (lititle.lower() == 'vin') | (lititle.lower() == 'stock number') | (lititle.lower() == 'dealer license'):\n",
    "#                 None # Don't want to save these\n",
    "#             else:\n",
    "#                 None\n",
    "#                 print(f'Unmatched param {lititle}: {livalue}') # <-- could take advantage of some or all of these\n",
    "\n",
    "#         curr_car = pd.DataFrame({\"timestamp\":[tstamp],\n",
    "#                                  \"price\":[price],\n",
    "#                                  \"year\":[year],\n",
    "#                                  \"make\":[make],\n",
    "#                                  \"model\":[model],\n",
    "#                                  \"body\":[body],\n",
    "#                                  \"mileage\":[mileage],\n",
    "#                                  \"title_type\":[title_type],\n",
    "#                                  \"city\":[city],\n",
    "#                                  \"state\":[state],\n",
    "#                                  \"seller\":[seller],\n",
    "#                                  \"trim\":[trim],\n",
    "#                                  \"ext_color\":[ext_color],\n",
    "#                                  \"int_color\":[int_color],\n",
    "#                                  \"transmission\":[transmission],\n",
    "#                                  \"liters\":[liters],\n",
    "#                                  \"cylinders\":[cylinders],\n",
    "#                                  \"fuel_type\":[fuel_type],\n",
    "#                                  \"n_doors\":[n_doors],\n",
    "#                                  \"ext_condition\":[ext_condition],\n",
    "#                                  \"int_condition\":[int_condition],\n",
    "#                                  \"drive_type\":[drive_type]})\n",
    "#         try:\n",
    "#             all_cars = pd.concat([all_cars, curr_car])\n",
    "#         except:\n",
    "#             all_cars = curr_car\n",
    "\n",
    "#         bar.update(idx)\n",
    "        \n",
    "# all_cars = all_cars.reset_index()\n",
    "# del all_cars['index']\n",
    "# all_cars.fillna(value=pd.np.nan, inplace=True)\n",
    "# all_cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "######## USE SELENIUM INSTEAD TO GET BIGGER LIST ########\n",
    "#########################################################\n",
    "\n",
    "# # Get list of proxy IPs\n",
    "\n",
    "# IPurl = \"https://free-proxy-list.net\"\n",
    "\n",
    "# resp = requests.get(IPurl)\n",
    "# IPhtml = resp.content\n",
    "# IPsoup = BeautifulSoup(IPhtml)\n",
    "\n",
    "# proxies = []\n",
    "# for tr in IPsoup.find(id='proxylisttable').find('tbody').find_all('tr'):\n",
    "#     tds = tr.find_all('td')\n",
    "#     if (tds[2].text.strip() == 'US') & (tds[6].text.strip() == 'yes') & (tds[4].text.strip() != 'transparent'):\n",
    "#         proxies.append(''.join(['http://', ':'.join([tds[0].text.strip(), tds[1].text.strip()])])) # grab the IP addresses matching the above criteria\n",
    "# random.shuffle(proxies)\n",
    "# proxies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
