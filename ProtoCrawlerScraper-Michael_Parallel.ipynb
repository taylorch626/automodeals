{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do:\n",
    "* Carve up carscraper() function into subfunctions where possible:\n",
    "    * Checking for validity of \\*\\*kwargs could be a separate function\n",
    "    * Checking for lititle and livalue type could be a separate function\n",
    "* Get page views (after certain time period?)\n",
    "* Get number of times listing has been favorited?\n",
    "* Keep a database or dictionary of IP:user-agent combos to avoid using different user-agents for the same IP?\n",
    "* Figure out how to use API for getting user-agents (to avoid IP ban from user-agent website)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # if this isn't installed, use pip install beautifulsoup4\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "import progressbar # if this isn't installed, use pip install progressbar2\n",
    "import random\n",
    "from selenium import webdriver # if not installed, do pip install selenium\n",
    "from itertools import cycle\n",
    "# import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure chromedriver.exe has been added to the `PATH` before running the generateProxies() function. You can do so using [this guide](https://zwbetz.com/download-chromedriver-binary-and-add-to-your-path-for-automated-functional-testing/). The driver itself is located in the repository at `~/automodeals/selenium/chromedriver.exe`\n",
    "\n",
    "**Importantly**, make sure that the chromedriver version used (e.g. 80) is the same as the full Chrome version you have installed (e.g. 80).\n",
    "\n",
    "Chromedriver can be downloaded [here](https://sites.google.com/a/chromium.org/chromedriver/downloads)\n",
    "\n",
    "Chrome version can be found [here](https://www.whatismybrowser.com/detect/what-version-of-chrome-do-i-have)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateProxies():\n",
    "    # Get list of US-based proxy IPs and ports using selenium\n",
    "\n",
    "    IPurl = \"https://www.us-proxy.org/\" # <-- the robots.txt file for this site allows full access for all user-agents\n",
    "\n",
    "    # Specify incognito options for Chrome\n",
    "    option = webdriver.ChromeOptions()\n",
    "    option.add_argument(\"--incognito\")\n",
    "\n",
    "    # Create new Chrome instance\n",
    "    browser = webdriver.Chrome(options=option)\n",
    "\n",
    "    # Minimize window\n",
    "    browser.minimize_window()\n",
    "\n",
    "    # Go to desired website\n",
    "    IPurl = \"https://www.us-proxy.org/\" # <-- the robots.txt file for this site allows full access for all user-agents\n",
    "    browser.get(IPurl)\n",
    "\n",
    "    # Filter by https only\n",
    "    https_button = browser.find_elements_by_xpath(\"//*[@id='proxylisttable']/tfoot/tr/th[7]/select/option[3]\")[0]\n",
    "    https_button.click()\n",
    "\n",
    "    # Set to 80 results\n",
    "    maxnum_button = browser.find_elements_by_xpath(\"//*[@id='proxylisttable_length']/label/select/option[3]\")[0]\n",
    "    maxnum_button.click()\n",
    "\n",
    "    # Grab IP's and Ports from the resulting table\n",
    "    rows = browser.find_elements_by_xpath(\"//*[@id='proxylisttable']/tbody/tr\")\n",
    "\n",
    "    proxies = set() # using a set ensures there aren't duplicates\n",
    "    for row in rows:\n",
    "        row = row.text.split(' ')\n",
    "\n",
    "        if row[3].strip().lower() != 'transparent': # don't want to include our real proxy when navigating KSL\n",
    "            proxies.add(''.join(['http://', ':'.join([row[0].strip(), row[1].strip()])]))\n",
    "\n",
    "    # Close browser when done\n",
    "    browser.close()\n",
    "\n",
    "    return proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function for the scraping done for each search page\n",
    "\n",
    "# def carscraper(url, rooturl, maxts, **kwargs):\n",
    "def carscraper(**kwargs):\n",
    "    '''VARIABLE INPUTS:\n",
    "    url: should be of the form \"https://cars.ksl.com/search/newUsed/Used;Certified/perPage/96/page/0\"\n",
    "    rooturl: should be something like \"https://cars.ksl.com\"\n",
    "    maxts: the maximum timestamp of the all_cars repository\n",
    "    use_proxy: a boolean or binary to indicate if a proxy should be used\n",
    "    curr_proxy: a string indicating the current proxy IP from last function call\n",
    "    proxydict: a dictionary of proxy IPs and associated user-agents to cycle through\n",
    "    refreshmin: the number of minutes to wait before updating the proxy pool\n",
    "    \n",
    "    ***NOTE: This function is meant to work with a pool of proxy IPs and a various spoofed user-agents'''\n",
    "    \n",
    "    # Need to spoof a user-agent in order to get past crawler block\n",
    "    user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36'\n",
    "    \n",
    "    # the following were pulled manually on 3/12/20 from https://www.whatismybrowser.com/guides/the-latest-user-agent/\n",
    "    user_agents = ['Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36',\n",
    "                   'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36',\n",
    "                   'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36',\n",
    "                   'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:54.0) Gecko/20100101 Firefox/74.0',\n",
    "                   'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.13; rv:61.0) Gecko/20100101 Firefox/74.0',\n",
    "                   'Mozilla/5.0 (X11; Linux i586; rv:31.0) Gecko/20100101 Firefox/74.0',\n",
    "                   'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0 Safari/605.1.15',\n",
    "                   'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36 Edg/80.0.361.62',\n",
    "                   'Mozilla/5.0 (Windows NT 10.0; Trident/7.0; rv:11.0) like Gecko']    \n",
    "    \n",
    "    \n",
    "    # Parse the kwargs\n",
    "    \n",
    "    \n",
    "    if 'url' in kwargs.keys():\n",
    "        if isinstance(kwargs['url'],str):\n",
    "            url = kwargs['url']\n",
    "        else:\n",
    "            raise TypeError(f'Expected string for url but got {type(kwargs[\"url\"])}.')\n",
    "    else:\n",
    "        raise ValueError('url is a required input for carscraper().')\n",
    "        \n",
    "    if 'rooturl' in kwargs.keys():\n",
    "        if isinstance(kwargs['rooturl'],str):\n",
    "            rooturl = kwargs['rooturl']\n",
    "        else:\n",
    "            raise TypeError(f'Expected string for rooturl but got {type(kwargs[\"rooturl\"])}.')\n",
    "    else:\n",
    "        raise ValueError('rooturl is a required input for carscraper().')\n",
    "        \n",
    "    if 'maxts' in kwargs.keys():\n",
    "        if isinstance(kwargs['maxts'],np.int64) or isinstance(kwargs['maxts'],int):\n",
    "            maxts = kwargs['maxts']\n",
    "        else:\n",
    "            raise TypeError(f'Expected np.int64 or int for maxts but got {type(kwargs[\"maxts\"])}.')\n",
    "    else:\n",
    "        raise ValueError('maxts is a required input for carscraper().')\n",
    "        \n",
    "    if 'use_proxy' in kwargs.keys():\n",
    "        if isinstance(kwargs['use_proxy'],int) or isinstance(kwargs['use_proxy'],bool):\n",
    "            use_proxy = kwargs['use_proxy']\n",
    "        else:\n",
    "            raise TypeError(f'Expected int or bool for use_proxy but got {type(kwargs[\"use_proxy\"])}.')\n",
    "    else:\n",
    "        # default is to NOT use proxy\n",
    "        use_proxy = False\n",
    "        \n",
    "    if use_proxy:\n",
    "        # The following inputs are only useful when using a proxy\n",
    "        \n",
    "        if 'proxydict' in kwargs.keys():\n",
    "            if isinstance(kwargs['proxydict'],dict):\n",
    "                proxydict = kwargs['proxydict']\n",
    "            else:\n",
    "                print(f'Expected dict type for proxydict but got {type(kwargs[\"proxydict\"])}. Generating new proxydict...')\n",
    "                newproxies = generateProxies()\n",
    "                proxydict = {i:random.choice(user_agents) for i in newproxies}\n",
    "        else:\n",
    "            print('No proxydict found. Generating...')\n",
    "            newproxies = generateProxies()\n",
    "            proxydict = {i:random.choice(user_agents) for i in newproxies}\n",
    "\n",
    "        if 'refreshmin' in kwargs.keys():\n",
    "            if isinstance(kwargs['refreshmin'],int) or isinstance(kwargs['refreshmin'],float):\n",
    "                refreshmin = kwargs['refreshmin']\n",
    "            else:\n",
    "                refreshmin = 15\n",
    "                print(f'Expected int or float for refreshmin but got {type(kwargs[\"refreshmin\"])}. Set to default value of {refreshmin}.')\n",
    "        else:\n",
    "            refreshmin = 15\n",
    "            print(f'No refreshmin found. Set to default value of {refreshmin}.')\n",
    "        \n",
    "    \n",
    "    \n",
    "    if use_proxy:\n",
    "        tstart = time.time() # set a start time to use for refreshing proxy list (if needed)    \n",
    "\n",
    "        if 'currproxy' in kwargs.keys():\n",
    "            if isinstance(kwargs['currproxy'],str):\n",
    "                currproxy = kwargs['currproxy']\n",
    "            else:\n",
    "                proxy_pool = cycle(proxydict) # make a pool of proxies \n",
    "                currproxy = next(proxy_pool) # grab the next proxy in cycle\n",
    "        else:\n",
    "            proxy_pool = cycle(proxydict) # make a pool of proxies \n",
    "            currproxy = next(proxy_pool) # grab the next proxy in cycle                \n",
    "\n",
    "\n",
    "        attempts = len(proxydict) # for now, limit the total number of attempts to one per proxy. This will prevent endless while loop\n",
    "        chkproxy = 1\n",
    "        while chkproxy and attempts:\n",
    "            if (time.time() - tstart) > 60*refreshmin: # check if it's been more than refreshmin minutes since proxy_pool updated\n",
    "                print('Refreshing proxy pool...')\n",
    "\n",
    "                currproxies = set(proxydict.keys())\n",
    "                newproxies = generateProxies()\n",
    "                newproxies = newproxies.difference(currproxies)\n",
    "\n",
    "                if newproxies:\n",
    "                    newdict = {i:random.choice(user_agents) for i in newproxies}\n",
    "                    proxydict.update(newdict)\n",
    "                    proxy_pool = cycle(proxydict)\n",
    "                    currproxy = next(proxy_pool)\n",
    "                    print('Proxy pool updated!')\n",
    "\n",
    "            try:\n",
    "                resp = requests.get(url,proxies={\"http\":currproxy, \"https\":currproxy},headers={'User-Agent': proxydict[currproxy]}, timeout=20)\n",
    "                print(f'Proxy success for {currproxy}')\n",
    "                print()\n",
    "                chkproxy = 0\n",
    "                attempts += 1\n",
    "            except:\n",
    "                prevproxy = currproxy\n",
    "                currproxy = next(proxy_pool)\n",
    "                print(f'Proxy error for {prevproxy}! Next up is {currproxy}')\n",
    "                attempts -= 1\n",
    "                print(f'Attempts remaining: {attempts}')\n",
    "                \n",
    "    else:\n",
    "        # don't use the proxy\n",
    "        resp = requests.get(url, headers = {'User-Agent': user_agent})\n",
    "        \n",
    "    html = resp.content\n",
    "    pgsoup = BeautifulSoup(html)\n",
    "    \n",
    "    # Check if there are additional pages of results\n",
    "    if pgsoup.find(\"a\", {\"title\" : \"Go forward 1 page\"}):\n",
    "        moreresults = 1\n",
    "    else:\n",
    "        moreresults = 0\n",
    "    \n",
    "    links = pgsoup.select(\"div.title > a.link\") # grab all 96 (or up to 96) links\n",
    "    tstamps = pgsoup.select(\"div.listing-detail-line script\") # grab all 96 (or up to 96) timestamps\n",
    "\n",
    "    # Loop through links and scrape data for each new listing\n",
    "    all_cars = []\n",
    "    with progressbar.ProgressBar(max_value=len(links)) as bar:\n",
    "        for idx, link in enumerate(links): # *** only load first x results for now to avoid ban before implementing spoofing\n",
    "\n",
    "            # Reset all fields to None before next loop\n",
    "            price=year=make=model=body=mileage=title_type=city=state=seller=None\n",
    "            trim=ext_color=int_color=transmission=liters=cylinders=fuel_type=n_doors=ext_condition=int_condition=drive_type=None\n",
    "\n",
    "            # We're going to want to strip the \"?ad_cid=[number]\" from the end of these links as they're not needed to load the page properly\n",
    "            # Regular expressions should come in handy here\n",
    "\n",
    "            cutidx = re.search('(\\?ad_cid=.+)',link['href']).start()\n",
    "            currlink = link['href'][:cutidx]\n",
    "\n",
    "            # Somewhere here we should do a check to make sure that the timestamp for currlink is newer than our newest file in our repository\n",
    "            # That is, compare the timestamps with a simple conditional, where if the conditional is not met, this loop breaks to avoid useless computation time\n",
    "\n",
    "            # Generate full link for the current listing\n",
    "            fulllink = '/'.join([rooturl.rstrip('/'), currlink.lstrip('/')])\n",
    "\n",
    "            if use_proxy:\n",
    "                attempts = len(proxydict) # for now, limit the total number of attempts to one per proxy. This will prevent endless while loop\n",
    "                chkproxy = 1\n",
    "                while chkproxy and attempts:\n",
    "                    if (time.time() - tstart) > 60*refreshmin: # check if it's been more than refreshmin minutes since proxy_pool updated\n",
    "                        print('Refreshing proxy pool...')\n",
    "\n",
    "                        currproxies = set(proxydict.keys())\n",
    "                        newproxies = generateProxies()\n",
    "                        newproxies = newproxies.difference(currproxies)\n",
    "\n",
    "                        if newproxies:\n",
    "                            newdict = {i:random.choice(user_agents) for i in newproxies}\n",
    "                            proxydict.update(newdict)\n",
    "                            proxy_pool = cycle(proxydict)\n",
    "                            currproxy = next(proxy_pool)\n",
    "                            print('Proxy pool updated!')\n",
    "\n",
    "                    try:\n",
    "                        resp = requests.get(fulllink,proxies={\"http\":currproxy, \"https\":currproxy},headers={'User-Agent': proxydict[currproxy]}, timeout=20)\n",
    "                        print(f'Proxy success for {currproxy}')\n",
    "                        print()\n",
    "                        chkproxy = 0\n",
    "                        attempts += 1\n",
    "                    except:\n",
    "                        prevproxy = currproxy\n",
    "                        currproxy = next(proxy_pool)\n",
    "                        print(f'Proxy error for {prevproxy}! Next up is {currproxy}')\n",
    "                        attempts -= 1\n",
    "                        print(f'Attempts remaining: {attempts}')\n",
    "                        \n",
    "            else:\n",
    "                # don't use the proxy\n",
    "                resp = requests.get(fulllink, headers = {'User-Agent': user_agent})\n",
    "            \n",
    "            \n",
    "            lsthtml = resp.content\n",
    "            lstsoup = BeautifulSoup(lsthtml)\n",
    "            \n",
    "            # Check if link is still good (i.e. listing is still active)\n",
    "            if lstsoup.title.text.strip().lower() == 'not found':\n",
    "                print('Bad link. Skipping...')\n",
    "                bar.update(idx)\n",
    "            else:\n",
    "\n",
    "                # Get timestamp\n",
    "                tstamp = int(re.search('(\\d+)',tstamps[idx].text).group(0))\n",
    "\n",
    "                # Check if timestamp is newer than maxts\n",
    "                if tstamp <= maxts:\n",
    "                    print('************ Found end of new data ************')\n",
    "#                     print(f'var type of all_cars is: {type(all_cars)}')\n",
    "                    moreresults = 0\n",
    "                    break\n",
    "#                 else:\n",
    "#                     print(f'New car found: {idx} in link {fulllink}')\n",
    "\n",
    "                # Get listing price\n",
    "                price = lstsoup.select('h3.price')[0].text.strip().replace('$','').replace(',','')\n",
    "\n",
    "                # Get seller's location\n",
    "                if lstsoup.select('h2.location > a'):\n",
    "                    location = lstsoup.select('h2.location > a')[0].text.strip()\n",
    "                    city, state = location.split(',')\n",
    "                    city = city.strip()\n",
    "                    state = state.strip()\n",
    "\n",
    "                # Get seller type (dealer or owner)\n",
    "                sellerstr = lstsoup.select('div.fsbo')[0].text.strip()\n",
    "                if re.search('(Dealer)', sellerstr):\n",
    "                    seller = 'Dealer'\n",
    "                elif re.search('(Owner)', sellerstr):\n",
    "                    seller = 'Owner'\n",
    "                    \n",
    "                # Get number of photos\n",
    "                if lstsoup.select('div.slider-uninitialized > p'):\n",
    "                    picstr = lstsoup.select('div.slider-uninitialized > p')[0].text.strip()\n",
    "                    n_pics = int(re.search('(\\d+)',picstr).group())\n",
    "                else:\n",
    "                    if lstsoup.find(id='widgetPhoto').p:\n",
    "                        picstr = lstsoup.find(id='widgetPhoto').p.text.strip()\n",
    "                        n_pics = int(re.search('(\\d+)',picstr).group())\n",
    "                    else:\n",
    "                        n_pics = 0\n",
    "\n",
    "                # Get table of car specs\n",
    "                specs = lstsoup.select('ul.listing-specifications')\n",
    "\n",
    "                for li in specs[0].find_all('li'):\n",
    "                    lititle = li.select('span.title')[0].text.strip().strip(':')\n",
    "                    livalue = li.select('span.value')[0].text.strip().strip(':')\n",
    "\n",
    "                    if livalue.lower() == 'not specified':\n",
    "                        livalue = None\n",
    "\n",
    "                    # Now a bunch of if-else statements to determine which column to add data to\n",
    "                    # There might be a more sophisticated way to do this, perhaps with a tuple or a dictionary?\n",
    "                    if lititle.lower() == 'year':\n",
    "                        if livalue:\n",
    "                            year = int(livalue)\n",
    "                        else:\n",
    "                            year = livalue\n",
    "                    elif lititle.lower() == 'make':\n",
    "                        make = livalue\n",
    "                    elif lititle.lower() == 'model':\n",
    "                        model = livalue\n",
    "                    elif lititle.lower() == 'body':\n",
    "                        body = livalue\n",
    "                    elif lititle.lower() == 'mileage':\n",
    "                        if livalue:\n",
    "                            mileage = int(livalue.replace(',',''))\n",
    "                        else:\n",
    "                            mileage = livalue\n",
    "                    elif lititle.lower() == 'title type':\n",
    "                        title_type = livalue\n",
    "\n",
    "                    # Below this are non-required specs    \n",
    "                    elif lititle.lower() == 'trim':\n",
    "                        trim = livalue\n",
    "                    elif lititle.lower() == 'exterior color':\n",
    "                        if livalue:\n",
    "                            ext_color = livalue.lower()\n",
    "                        else:\n",
    "                            ext_color = livalue\n",
    "                    elif lititle.lower() == 'interior color':\n",
    "                        if livalue:\n",
    "                            int_color = livalue.lower()\n",
    "                        else:\n",
    "                            int_color = livalue\n",
    "                    elif lititle.lower() == 'transmission':\n",
    "                        transmission = livalue\n",
    "                    elif lititle.lower() == 'liters':\n",
    "                        try:\n",
    "                            liters = float(livalue)\n",
    "                        except:\n",
    "                            if livalue:\n",
    "                                str1 = re.search('^(.*?)L',livalue).group(0).strip().replace(' ','')\n",
    "                                if re.search('^(\\D+)',str1):\n",
    "                                    idxend = re.search('^(\\D+)',str1).end()\n",
    "                                    livalue = str1[idxend:-1]\n",
    "                                    if re.search('(\\D+)',livalue): # check if still other pollutants\n",
    "                                        idxend = re.search('(\\D+)',livalue).end()\n",
    "                                        livalue = livalue[idxend:]\n",
    "                                else:\n",
    "                                    livalue = str1[:-1]\n",
    "                                try:\n",
    "                                    livalue = float(livalue)\n",
    "                                except:\n",
    "                                    print(url)\n",
    "                                    print('****')\n",
    "                                    print(link)\n",
    "                            else:\n",
    "                                liters = livalue\n",
    "                    elif lititle.lower() == 'cylinders':\n",
    "                        if livalue:\n",
    "                            cylinders = int(livalue)\n",
    "                        else:\n",
    "                            cylinders = livalue\n",
    "                    elif lititle.lower() == 'fuel type':\n",
    "                        fuel_type = livalue\n",
    "                    elif lititle.lower() == 'number of doors':\n",
    "                        if livalue:\n",
    "                            n_doors = int(livalue)\n",
    "                        else:\n",
    "                            n_doors = livalue\n",
    "                    elif lititle.lower() == 'exterior condition':\n",
    "                        ext_condition = livalue\n",
    "                    elif lititle.lower() == 'interior condition':\n",
    "                        int_condition = livalue\n",
    "                    elif lititle.lower() == 'drive type':\n",
    "                        drive_type = livalue\n",
    "                    elif (lititle.lower() == 'vin') | (lititle.lower() == 'stock number') | (lititle.lower() == 'dealer license'):\n",
    "                        None # Don't want to save these\n",
    "                    else:\n",
    "                        None\n",
    "                        print(f'Unmatched param {lititle}: {livalue}') # <-- could take advantage of some or all of these\n",
    "\n",
    "                curr_car = pd.DataFrame({\"timestamp\":[tstamp],\n",
    "                                         \"price\":[price],\n",
    "                                         \"year\":[year],\n",
    "                                         \"make\":[make],\n",
    "                                         \"model\":[model],\n",
    "                                         \"body\":[body],\n",
    "                                         \"mileage\":[mileage],\n",
    "                                         \"title_type\":[title_type],\n",
    "                                         \"city\":[city],\n",
    "                                         \"state\":[state],\n",
    "                                         \"seller\":[seller],\n",
    "                                         \"trim\":[trim],\n",
    "                                         \"ext_color\":[ext_color],\n",
    "                                         \"int_color\":[int_color],\n",
    "                                         \"transmission\":[transmission],\n",
    "                                         \"liters\":[liters],\n",
    "                                         \"cylinders\":[cylinders],\n",
    "                                         \"fuel_type\":[fuel_type],\n",
    "                                         \"n_doors\":[n_doors],\n",
    "                                         \"ext_condition\":[ext_condition],\n",
    "                                         \"int_condition\":[int_condition],\n",
    "                                         \"drive_type\":[drive_type],\n",
    "                                         \"n_pics\":[n_pics]})\n",
    "                try:\n",
    "                    all_cars = pd.concat([curr_car, all_cars])\n",
    "                except:\n",
    "                    all_cars = curr_car\n",
    "\n",
    "                bar.update(idx)\n",
    "\n",
    "    if type(all_cars) is pd.core.frame.DataFrame: # make sure that some data was actually scraped\n",
    "        all_cars = all_cars.reset_index()\n",
    "        del all_cars['index']\n",
    "        all_cars.fillna(value=pd.np.nan, inplace=True)\n",
    "    if use_proxy:\n",
    "        return all_cars, moreresults, currproxy, proxydict\n",
    "    else:\n",
    "        return all_cars, moreresults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (96 of 96) |########################| Elapsed Time: 0:01:46 Time:  0:01:46\n",
      "100% (96 of 96) |########################| Elapsed Time: 0:02:01 Time:  0:02:01\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>make</th>\n",
       "      <th>model</th>\n",
       "      <th>body</th>\n",
       "      <th>mileage</th>\n",
       "      <th>title_type</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>...</th>\n",
       "      <th>int_color</th>\n",
       "      <th>transmission</th>\n",
       "      <th>liters</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>fuel_type</th>\n",
       "      <th>n_doors</th>\n",
       "      <th>ext_condition</th>\n",
       "      <th>int_condition</th>\n",
       "      <th>drive_type</th>\n",
       "      <th>n_pics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1584384008</td>\n",
       "      <td>21990</td>\n",
       "      <td>2019</td>\n",
       "      <td>Nissan</td>\n",
       "      <td>Frontier</td>\n",
       "      <td>Truck</td>\n",
       "      <td>17667</td>\n",
       "      <td>Clean Title</td>\n",
       "      <td>Tooele</td>\n",
       "      <td>UT</td>\n",
       "      <td>...</td>\n",
       "      <td>gray</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>4-Wheel Drive</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1584384026</td>\n",
       "      <td>11790</td>\n",
       "      <td>2018</td>\n",
       "      <td>Hyundai</td>\n",
       "      <td>Elantra</td>\n",
       "      <td>Sedan</td>\n",
       "      <td>37506</td>\n",
       "      <td>Clean Title</td>\n",
       "      <td>Tooele</td>\n",
       "      <td>UT</td>\n",
       "      <td>...</td>\n",
       "      <td>gray</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>FWD</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1584384045</td>\n",
       "      <td>11990</td>\n",
       "      <td>2014</td>\n",
       "      <td>Volkswagen</td>\n",
       "      <td>Jetta</td>\n",
       "      <td>Sedan</td>\n",
       "      <td>36429</td>\n",
       "      <td>Clean Title</td>\n",
       "      <td>Tooele</td>\n",
       "      <td>UT</td>\n",
       "      <td>...</td>\n",
       "      <td>black</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>FWD</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1584384058</td>\n",
       "      <td>19500</td>\n",
       "      <td>2016</td>\n",
       "      <td>Chevrolet</td>\n",
       "      <td>Express Cargo Van</td>\n",
       "      <td>Van</td>\n",
       "      <td>68000</td>\n",
       "      <td>Clean Title</td>\n",
       "      <td>Bountiful</td>\n",
       "      <td>UT</td>\n",
       "      <td>...</td>\n",
       "      <td>gray</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Flex Fuel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>RWD</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1584384059</td>\n",
       "      <td>14999</td>\n",
       "      <td>2011</td>\n",
       "      <td>Ford</td>\n",
       "      <td>F-150</td>\n",
       "      <td>Truck</td>\n",
       "      <td>103293</td>\n",
       "      <td>Clean Title</td>\n",
       "      <td>American Fork</td>\n",
       "      <td>UT</td>\n",
       "      <td>...</td>\n",
       "      <td>black cloth interior</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4-Wheel Drive</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>1584388175</td>\n",
       "      <td>32995</td>\n",
       "      <td>2016</td>\n",
       "      <td>Ford</td>\n",
       "      <td>F-150</td>\n",
       "      <td>Truck</td>\n",
       "      <td>102306</td>\n",
       "      <td>Clean Title</td>\n",
       "      <td>Idaho Falls</td>\n",
       "      <td>ID</td>\n",
       "      <td>...</td>\n",
       "      <td>black</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4-Wheel Drive</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>1584388214</td>\n",
       "      <td>3150</td>\n",
       "      <td>2005</td>\n",
       "      <td>Pontiac</td>\n",
       "      <td>Grand Prix</td>\n",
       "      <td>Sedan</td>\n",
       "      <td>132807</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Taylorsville</td>\n",
       "      <td>UT</td>\n",
       "      <td>...</td>\n",
       "      <td>black</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>FWD</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>1584388230</td>\n",
       "      <td>21995</td>\n",
       "      <td>2015</td>\n",
       "      <td>BMW</td>\n",
       "      <td>3 Series</td>\n",
       "      <td>Hatchback</td>\n",
       "      <td>22637</td>\n",
       "      <td>Clean Title</td>\n",
       "      <td>American Fork</td>\n",
       "      <td>UT</td>\n",
       "      <td>...</td>\n",
       "      <td>black</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AWD</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1584388396</td>\n",
       "      <td>12499</td>\n",
       "      <td>2013</td>\n",
       "      <td>Ford</td>\n",
       "      <td>Edge</td>\n",
       "      <td>Sport Utility</td>\n",
       "      <td>72818</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Salt Lake City</td>\n",
       "      <td>UT</td>\n",
       "      <td>...</td>\n",
       "      <td>charcoal black</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FWD</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>1584388693</td>\n",
       "      <td>23999</td>\n",
       "      <td>2016</td>\n",
       "      <td>Ram</td>\n",
       "      <td>1500</td>\n",
       "      <td>Truck</td>\n",
       "      <td>21365</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Draper</td>\n",
       "      <td>UT</td>\n",
       "      <td>...</td>\n",
       "      <td>black</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4-Wheel Drive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      timestamp  price  year        make              model           body  \\\n",
       "0    1584384008  21990  2019      Nissan           Frontier          Truck   \n",
       "1    1584384026  11790  2018     Hyundai            Elantra          Sedan   \n",
       "2    1584384045  11990  2014  Volkswagen              Jetta          Sedan   \n",
       "3    1584384058  19500  2016   Chevrolet  Express Cargo Van            Van   \n",
       "4    1584384059  14999  2011        Ford              F-150          Truck   \n",
       "..          ...    ...   ...         ...                ...            ...   \n",
       "187  1584388175  32995  2016        Ford              F-150          Truck   \n",
       "188  1584388214   3150  2005     Pontiac         Grand Prix          Sedan   \n",
       "189  1584388230  21995  2015         BMW           3 Series      Hatchback   \n",
       "190  1584388396  12499  2013        Ford               Edge  Sport Utility   \n",
       "191  1584388693  23999  2016         Ram               1500          Truck   \n",
       "\n",
       "     mileage   title_type            city state  ...             int_color  \\\n",
       "0      17667  Clean Title          Tooele    UT  ...                  gray   \n",
       "1      37506  Clean Title          Tooele    UT  ...                  gray   \n",
       "2      36429  Clean Title          Tooele    UT  ...                 black   \n",
       "3      68000  Clean Title       Bountiful    UT  ...                  gray   \n",
       "4     103293  Clean Title   American Fork    UT  ...  black cloth interior   \n",
       "..       ...          ...             ...   ...  ...                   ...   \n",
       "187   102306  Clean Title     Idaho Falls    ID  ...                 black   \n",
       "188   132807          NaN    Taylorsville    UT  ...                 black   \n",
       "189    22637  Clean Title   American Fork    UT  ...                 black   \n",
       "190    72818          NaN  Salt Lake City    UT  ...        charcoal black   \n",
       "191    21365          NaN          Draper    UT  ...                 black   \n",
       "\n",
       "    transmission liters cylinders  fuel_type  n_doors  ext_condition  \\\n",
       "0      Automatic    4.0       6.0   Gasoline      4.0      Excellent   \n",
       "1      Automatic    2.0       4.0   Gasoline      4.0      Excellent   \n",
       "2      Automatic    NaN       4.0     Diesel      4.0      Excellent   \n",
       "3            NaN    NaN       8.0  Flex Fuel      NaN      Excellent   \n",
       "4      Automatic    NaN       6.0   Gasoline      NaN            NaN   \n",
       "..           ...    ...       ...        ...      ...            ...   \n",
       "187    Automatic    NaN       6.0   Gasoline      NaN            NaN   \n",
       "188    Automatic    NaN       6.0   Gasoline      4.0           Good   \n",
       "189    Automatic    NaN       4.0   Gasoline      4.0            NaN   \n",
       "190    Automatic    NaN       4.0   Gasoline      4.0            NaN   \n",
       "191    Automatic    NaN       8.0   Gasoline      4.0            NaN   \n",
       "\n",
       "    int_condition     drive_type n_pics  \n",
       "0       Excellent  4-Wheel Drive     23  \n",
       "1       Excellent            FWD     24  \n",
       "2       Excellent            FWD     22  \n",
       "3       Excellent            RWD     33  \n",
       "4             NaN  4-Wheel Drive     25  \n",
       "..            ...            ...    ...  \n",
       "187           NaN  4-Wheel Drive     28  \n",
       "188          Good            FWD      7  \n",
       "189           NaN            AWD     30  \n",
       "190           NaN            FWD     19  \n",
       "191           NaN  4-Wheel Drive      2  \n",
       "\n",
       "[192 rows x 23 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try a multi-page test using carscraper function\n",
    "import sys\n",
    "\n",
    "# determine whether or not to use proxy IPs. Can use boolean or int for this\n",
    "use_proxy = False\n",
    "\n",
    "# set cap for number of search pages to load (i.e. pages with up to 96 listings)\n",
    "maxpg = 2\n",
    "\n",
    "# Define root url for KSL cars\n",
    "rooturl = \"https://cars.ksl.com\"\n",
    "\n",
    "# Note the url below specifies that we're looking for 96 per page and the default sort of newest to oldest posting\n",
    "# This note about newest to oldest is useful so that we can avoid scraping repeat listings based on their timestamps\n",
    "# Also note that this url does NOT have a page number associated with it. This is added in the while loop below\n",
    "lurl = \"https://cars.ksl.com/search/newUsed/Used;Certified/perPage/96/page/\"\n",
    "\n",
    "count = 0\n",
    "all_cars = []\n",
    "while count < maxpg:\n",
    "    url = lurl + str(count)\n",
    "    try:\n",
    "        if use_proxy:\n",
    "            curr_cars, moreresults, currproxy, proxydict = carscraper(url=url, rooturl=rooturl, maxts=0, use_proxy=use_proxy, currproxy=currproxy, refreshmin = 15, proxydict = proxydict)\n",
    "        else:\n",
    "            curr_cars, moreresults = carscraper(url=url, rooturl=rooturl, maxts=0, use_proxy=False)\n",
    "        \n",
    "    except:\n",
    "        if use_proxy:\n",
    "            curr_cars, moreresults, currproxy, proxydict = carscraper(url=url, rooturl=rooturl, maxts=0, use_proxy=use_proxy, refreshmin = 15)\n",
    "        else:\n",
    "            print('Unexpected error:', sys.exc_info())\n",
    "            pass\n",
    "        \n",
    "    \n",
    "    count += 1    \n",
    "#     print(f'More results? {moreresults}')\n",
    "    if type(curr_cars) is pd.core.frame.DataFrame: # make sure real data was returned\n",
    "        try:\n",
    "            all_cars = pd.concat([curr_cars, all_cars], ignore_index=True)\n",
    "        except:\n",
    "            all_cars = curr_cars\n",
    "    else:\n",
    "        print('No car data found!')\n",
    "    \n",
    "all_cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe to csv\n",
    "\n",
    "all_cars.to_csv('data/all_cars.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframe\n",
    "all_cars = pd.read_csv('data/all_cars.csv')\n",
    "\n",
    "# get most recent timestamp from the dataframe\n",
    "rep_ts = all_cars['timestamp'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now scrape for more cars and check for timestamp\n",
    "\n",
    "# Define root url for KSL cars\n",
    "rooturl = \"https://cars.ksl.com\"\n",
    "\n",
    "lurl = \"https://cars.ksl.com/search/newUsed/Used;Certified/perPage/96/page/\"\n",
    "\n",
    "count = 0\n",
    "newer_cars = []\n",
    "moreresults = 1\n",
    "while moreresults:\n",
    "    url = lurl + str(count)\n",
    "    \n",
    "    try:\n",
    "        if use_proxy:\n",
    "            curr_cars, moreresults, currproxy, proxydict = carscraper(url=url, rooturl=rooturl, maxts=rep_ts, use_proxy=use_proxy, currproxy=currproxy, refreshmin = 15, proxydict = proxydict)\n",
    "        else:\n",
    "            curr_cars, moreresults = carscraper(url=url, rooturl=rooturl, maxts=rep_ts, use_proxy=use_proxy)\n",
    "    except:\n",
    "        if use_proxy:\n",
    "            curr_cars, moreresults, currproxy, proxydict = carscraper(url=url, rooturl=rooturl, maxts=0, use_proxy=use_proxy, refreshmin = 15)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    count += 1    \n",
    "#     print(f'More results? {moreresults}')\n",
    "    if type(curr_cars) is pd.core.frame.DataFrame: # make sure real data was returned\n",
    "        try:\n",
    "            newer_cars = pd.concat([curr_cars, newer_cars], ignore_index=True)\n",
    "        except:\n",
    "            newer_cars = curr_cars\n",
    "    else:\n",
    "        print('No newer car data found!')\n",
    "    \n",
    "# add newer_cars\n",
    "if type(newer_cars) is pd.core.frame.DataFrame:\n",
    "    all_cars_UPDATED = pd.concat([newer_cars, all_cars], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save updated dataframe to csv\n",
    "\n",
    "all_cars_UPDATED.to_csv('data/all_cars_UPDATED.csv', index=False) # in later iterations, remove the \"_UPDATED\" part of filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load updated dataframe\n",
    "all_cars_UPDATED = pd.read_csv('data/all_cars_UPDATED.csv') # in later iterations, remove the \"_UPDATED\" part of filename\n",
    "all_cars_UPDATED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_listing_info(cars_df):\n",
    "    '''Updates a cars_dataframe with 6 new columns (views, favorites, \n",
    "    workingURL, view_rate, favorite_rate, fav_per_view). \n",
    "    Can easily be modified to accept different parameters in the future'''\n",
    "\n",
    "    # new columns to add\n",
    "    cars_df['views'] = np.NaN\n",
    "    cars_df['favorites'] = np.NaN\n",
    "    cars_df['workingURL'] = 1\n",
    "    cars_df['view_rate'] = np.NaN\n",
    "    cars_df['favorite_rate'] = np.NaN\n",
    "    cars_df['fav_per_view'] = np.NaN\n",
    "\n",
    "    # find ads more than x days old (time.time() is in seconds)\n",
    "    curr_time = int(time.time())\n",
    "    min_days = 3\n",
    "    min_dt = min_days*60*60*24 # time in seconds for use with datetime\n",
    "    old_ads = cars_df['timestamp'] < (curr_time - min_dt)\n",
    "\n",
    "    # find ads that haven't been pulled for more than x days\n",
    "    min_last_pull = 2\n",
    "    min_last_pull_dt = min_last_pull*60*60*24 # time in seconds for use with datetime\n",
    "    no_recent_update = cars_df['lastpull_ts'] < (curr_time - min_last_pull_dt)\n",
    "\n",
    "    # subselect ads that need updating based on previous criteria and having a working URL last time it was checked\n",
    "    cars_need_update = cars_df[old_ads & no_recent_update & cars_df['workingURL']]\n",
    "\n",
    "    user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'\n",
    "\n",
    "    # iterate through, pulling new information from each ad\n",
    "    last_pull = []\n",
    "    views = []\n",
    "    favorites = []\n",
    "    working_url = []\n",
    "    for _, ad in cars_need_update.iterrows():\n",
    "        ad_response = requests.get(ad['link'], headers = {'User-Agent': user_agent})\n",
    "        pull_ts = int(time.time())\n",
    "        last_pull.append(pull_ts)\n",
    "        ad_soup = BeautifulSoup(ad_response.content)\n",
    "\n",
    "        # Check if link is still good (i.e. listing is still active)\n",
    "        if ad_soup.title.text.strip().lower() == 'not found':\n",
    "            working_url.append(0)\n",
    "            views.append(None)\n",
    "            favorites.append(None)\n",
    "        else:\n",
    "            working_url.append(1)\n",
    "\n",
    "            # get views\n",
    "            viewcount = int(ad_soup.select('span.vdp-info-value')[1].text.split()[0])\n",
    "            views.append(viewcount)\n",
    "\n",
    "            # get favorites\n",
    "            favoritecount = int(ad_soup.select('span.vdp-info-value')[2].text.split()[0])\n",
    "            favorites.append(favoritecount)\n",
    "\n",
    "    cars_updated = cars_need_update\n",
    "    cars_updated['views'] = views\n",
    "    cars_updated['favorites'] = favorites\n",
    "    cars_updated['lastpull_ts'] = last_pull\n",
    "    cars_updated['workingURL'] = working_url\n",
    "    cars_updated['fav_per_view'] = cars_updated['favorites'] / cars_updated['views']\n",
    "    # rates calculated per day\n",
    "    cars_updated['view_rate'] = cars_updated['views'] / (cars_updated['lastpull_ts'] - cars_updated['timestamp']) * 60*60*24\n",
    "    cars_updated['favorite_rate'] = cars_updated['favorites'] / (cars_updated['lastpull_ts'] - cars_updated['timestamp']) * 60*60*24\n",
    "\n",
    "    cars_df.update(cars_updated)\n",
    "    \n",
    "    return cars_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_df = pd.read_csv('data/all_cars_UPDATED.csv')\n",
    "cars_df = update_listing_info(cars_df)\n",
    "cars_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sandbox code before implementing in main loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_listing_info(cars_df):\n",
    "    '''Updates a cars_dataframe with 6 new columns (views, favorites, \n",
    "    workingURL, view_rate, favorite_rate, fav_per_view). Can easily be modified to accept different arguments'''\n",
    "\n",
    "    # new columns to add\n",
    "    cars_df['views'] = np.NaN\n",
    "    cars_df['favorites'] = np.NaN\n",
    "    cars_df['workingURL'] = 1\n",
    "    cars_df['view_rate'] = np.NaN\n",
    "    cars_df['favorite_rate'] = np.NaN\n",
    "    cars_df['fav_per_view'] = np.NaN\n",
    "\n",
    "    # find ads more than x days old (time.time() is in seconds)\n",
    "    curr_time = int(time.time())\n",
    "    min_days = 3\n",
    "    min_dt = min_days*60*60*24 # time in seconds for use with datetime\n",
    "    old_ads = cars_df['timestamp'] < (curr_time - min_dt)\n",
    "\n",
    "    # find ads that haven't been pulled for more than x days\n",
    "    min_last_pull = 2\n",
    "    min_last_pull_dt = min_last_pull*60*60*24 # time in seconds for use with datetime\n",
    "    no_recent_update = cars_df['lastpull_ts'] < (curr_time - min_last_pull_dt)\n",
    "\n",
    "    # subselect ads that need updating based on previous criteria and having a working URL last time it was checked\n",
    "    cars_need_update = cars_df[old_ads & no_recent_update & cars_df['workingURL']]\n",
    "#     cars_need_update = cars_need_update[:3] # temporary for development\n",
    "\n",
    "    user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'\n",
    "\n",
    "    # iterate through, pulling new information from each ad\n",
    "    last_pull = []\n",
    "    views = []\n",
    "    favorites = []\n",
    "    working_url = []\n",
    "    for _, ad in cars_need_update.iterrows():\n",
    "        ad_response = requests.get(ad['link'], headers = {'User-Agent': user_agent})\n",
    "        pull_ts = int(time.time())\n",
    "        last_pull.append(pull_ts)\n",
    "        ad_soup = BeautifulSoup(ad_response.content)\n",
    "\n",
    "        # Check if link is still good (i.e. listing is still active)\n",
    "        if ad_soup.title.text.strip().lower() == 'not found':\n",
    "            working_url.append(0)\n",
    "            views.append(None)\n",
    "            favorites.append(None)\n",
    "        else:\n",
    "            working_url.append(1)\n",
    "\n",
    "            # get views\n",
    "            viewcount = int(ad_soup.select('span.vdp-info-value')[1].text.split()[0])\n",
    "            views.append(viewcount)\n",
    "\n",
    "            # get favorites\n",
    "            favoritecount = int(ad_soup.select('span.vdp-info-value')[2].text.split()[0])\n",
    "            favorites.append(favoritecount)\n",
    "\n",
    "    cars_updated = cars_need_update\n",
    "    cars_updated['views'] = views\n",
    "    cars_updated['favorites'] = favorites\n",
    "    cars_updated['lastpull_ts'] = last_pull\n",
    "    cars_updated['workingURL'] = working_url\n",
    "    cars_updated['fav_per_view'] = cars_updated['favorites'] / cars_updated['views']\n",
    "    # rates calculated per day\n",
    "    cars_updated['view_rate'] = cars_updated['views'] / (cars_updated['lastpull_ts'] - cars_updated['timestamp']) * 60*60*24\n",
    "    cars_updated['favorite_rate'] = cars_updated['favorites'] / (cars_updated['lastpull_ts'] - cars_updated['timestamp']) * 60*60*24\n",
    "\n",
    "    cars_df.update(cars_updated)\n",
    "    \n",
    "    return cars_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>lastpull_ts</th>\n",
       "      <th>link</th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>make</th>\n",
       "      <th>model</th>\n",
       "      <th>body</th>\n",
       "      <th>mileage</th>\n",
       "      <th>title_type</th>\n",
       "      <th>...</th>\n",
       "      <th>ext_condition</th>\n",
       "      <th>int_condition</th>\n",
       "      <th>drive_type</th>\n",
       "      <th>n_pics</th>\n",
       "      <th>views</th>\n",
       "      <th>favorites</th>\n",
       "      <th>workingURL</th>\n",
       "      <th>view_rate</th>\n",
       "      <th>favorite_rate</th>\n",
       "      <th>fav_per_view</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1584392093</td>\n",
       "      <td>1584401795</td>\n",
       "      <td>https://cars.ksl.com/listing/6312830</td>\n",
       "      <td>5500</td>\n",
       "      <td>2014</td>\n",
       "      <td>Ford</td>\n",
       "      <td>Focus</td>\n",
       "      <td>Hatchback</td>\n",
       "      <td>93105</td>\n",
       "      <td>Rebuilt/Reconstructed Title</td>\n",
       "      <td>...</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>FWD</td>\n",
       "      <td>7</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>739.146568</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1584391924</td>\n",
       "      <td>1584401797</td>\n",
       "      <td>https://cars.ksl.com/listing/6312828</td>\n",
       "      <td>31500</td>\n",
       "      <td>2017</td>\n",
       "      <td>Dodge</td>\n",
       "      <td>Durango</td>\n",
       "      <td>SUV</td>\n",
       "      <td>49000</td>\n",
       "      <td>Clean Title</td>\n",
       "      <td>...</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>AWD</td>\n",
       "      <td>18</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>507.566089</td>\n",
       "      <td>17.502279</td>\n",
       "      <td>0.034483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1584391895</td>\n",
       "      <td>1584401800</td>\n",
       "      <td>https://cars.ksl.com/listing/6277476</td>\n",
       "      <td>21800</td>\n",
       "      <td>2014</td>\n",
       "      <td>Ford</td>\n",
       "      <td>F-150</td>\n",
       "      <td>Truck</td>\n",
       "      <td>81000</td>\n",
       "      <td>Clean Title</td>\n",
       "      <td>...</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>4-Wheel Drive</td>\n",
       "      <td>4</td>\n",
       "      <td>89.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>776.335184</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1584391700</td>\n",
       "      <td>1584401801</td>\n",
       "      <td>https://cars.ksl.com/listing/6312821</td>\n",
       "      <td>29995</td>\n",
       "      <td>2015</td>\n",
       "      <td>Toyota</td>\n",
       "      <td>Sienna</td>\n",
       "      <td>Van</td>\n",
       "      <td>26346</td>\n",
       "      <td>Clean Title</td>\n",
       "      <td>...</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>AWD</td>\n",
       "      <td>0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>145.411345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1584391680</td>\n",
       "      <td>1584401803</td>\n",
       "      <td>https://cars.ksl.com/listing/6296049</td>\n",
       "      <td>18887</td>\n",
       "      <td>2014</td>\n",
       "      <td>BMW</td>\n",
       "      <td>5 Series</td>\n",
       "      <td>Sedan</td>\n",
       "      <td>54887</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AWD</td>\n",
       "      <td>13</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>221.910501</td>\n",
       "      <td>8.535019</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1584386482</td>\n",
       "      <td>1584401993</td>\n",
       "      <td>https://cars.ksl.com/listing/6312669</td>\n",
       "      <td>6995</td>\n",
       "      <td>2011</td>\n",
       "      <td>Honda</td>\n",
       "      <td>Civic</td>\n",
       "      <td>Sedan</td>\n",
       "      <td>113000</td>\n",
       "      <td>Clean Title</td>\n",
       "      <td>...</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>FWD</td>\n",
       "      <td>22</td>\n",
       "      <td>48.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>267.371543</td>\n",
       "      <td>11.140481</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>1584386463</td>\n",
       "      <td>1584401994</td>\n",
       "      <td>https://cars.ksl.com/listing/5923015</td>\n",
       "      <td>10290</td>\n",
       "      <td>2018</td>\n",
       "      <td>Nissan</td>\n",
       "      <td>Sentra</td>\n",
       "      <td>Sedan</td>\n",
       "      <td>33802</td>\n",
       "      <td>Clean Title</td>\n",
       "      <td>...</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>FWD</td>\n",
       "      <td>21</td>\n",
       "      <td>247.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1374.077651</td>\n",
       "      <td>38.941472</td>\n",
       "      <td>0.028340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>1584386349</td>\n",
       "      <td>1584401995</td>\n",
       "      <td>https://cars.ksl.com/listing/6312659</td>\n",
       "      <td>54000</td>\n",
       "      <td>2018</td>\n",
       "      <td>Jeep</td>\n",
       "      <td>Wrangler</td>\n",
       "      <td>SUV</td>\n",
       "      <td>13000</td>\n",
       "      <td>Clean Title</td>\n",
       "      <td>...</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>4-Wheel Drive</td>\n",
       "      <td>4</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>176.709702</td>\n",
       "      <td>5.522178</td>\n",
       "      <td>0.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>1584386341</td>\n",
       "      <td>1584401997</td>\n",
       "      <td>https://cars.ksl.com/listing/6132549</td>\n",
       "      <td>24553</td>\n",
       "      <td>2016</td>\n",
       "      <td>Ford</td>\n",
       "      <td>F-150</td>\n",
       "      <td>Truck</td>\n",
       "      <td>79525</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4-Wheel Drive</td>\n",
       "      <td>21</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>816.760347</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>1584386341</td>\n",
       "      <td>1584401998</td>\n",
       "      <td>https://cars.ksl.com/listing/6166983</td>\n",
       "      <td>11980</td>\n",
       "      <td>2018</td>\n",
       "      <td>Volkswagen</td>\n",
       "      <td>Jetta</td>\n",
       "      <td>Sedan</td>\n",
       "      <td>27410</td>\n",
       "      <td>Clean Title</td>\n",
       "      <td>...</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>FWD</td>\n",
       "      <td>18</td>\n",
       "      <td>188.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1037.440123</td>\n",
       "      <td>33.109791</td>\n",
       "      <td>0.031915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>195 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      timestamp  lastpull_ts                                  link  price  \\\n",
       "0    1584392093   1584401795  https://cars.ksl.com/listing/6312830   5500   \n",
       "1    1584391924   1584401797  https://cars.ksl.com/listing/6312828  31500   \n",
       "2    1584391895   1584401800  https://cars.ksl.com/listing/6277476  21800   \n",
       "3    1584391700   1584401801  https://cars.ksl.com/listing/6312821  29995   \n",
       "4    1584391680   1584401803  https://cars.ksl.com/listing/6296049  18887   \n",
       "..          ...          ...                                   ...    ...   \n",
       "190  1584386482   1584401993  https://cars.ksl.com/listing/6312669   6995   \n",
       "191  1584386463   1584401994  https://cars.ksl.com/listing/5923015  10290   \n",
       "192  1584386349   1584401995  https://cars.ksl.com/listing/6312659  54000   \n",
       "193  1584386341   1584401997  https://cars.ksl.com/listing/6132549  24553   \n",
       "194  1584386341   1584401998  https://cars.ksl.com/listing/6166983  11980   \n",
       "\n",
       "     year        make     model       body  mileage  \\\n",
       "0    2014        Ford     Focus  Hatchback    93105   \n",
       "1    2017       Dodge   Durango        SUV    49000   \n",
       "2    2014        Ford     F-150      Truck    81000   \n",
       "3    2015      Toyota    Sienna        Van    26346   \n",
       "4    2014         BMW  5 Series      Sedan    54887   \n",
       "..    ...         ...       ...        ...      ...   \n",
       "190  2011       Honda     Civic      Sedan   113000   \n",
       "191  2018      Nissan    Sentra      Sedan    33802   \n",
       "192  2018        Jeep  Wrangler        SUV    13000   \n",
       "193  2016        Ford     F-150      Truck    79525   \n",
       "194  2018  Volkswagen     Jetta      Sedan    27410   \n",
       "\n",
       "                      title_type  ... ext_condition int_condition  \\\n",
       "0    Rebuilt/Reconstructed Title  ...          Good          Good   \n",
       "1                    Clean Title  ...     Excellent     Excellent   \n",
       "2                    Clean Title  ...          Good          Good   \n",
       "3                    Clean Title  ...     Excellent     Excellent   \n",
       "4                            NaN  ...           NaN           NaN   \n",
       "..                           ...  ...           ...           ...   \n",
       "190                  Clean Title  ...     Excellent     Excellent   \n",
       "191                  Clean Title  ...     Excellent     Excellent   \n",
       "192                  Clean Title  ...     Excellent     Excellent   \n",
       "193                          NaN  ...           NaN           NaN   \n",
       "194                  Clean Title  ...     Excellent     Excellent   \n",
       "\n",
       "        drive_type n_pics  views favorites workingURL    view_rate  \\\n",
       "0              FWD      7   83.0       0.0          1   739.146568   \n",
       "1              AWD     18   58.0       2.0          1   507.566089   \n",
       "2    4-Wheel Drive      4   89.0       0.0          1   776.335184   \n",
       "3              AWD      0   17.0       0.0          1   145.411345   \n",
       "4              AWD     13   26.0       1.0          1   221.910501   \n",
       "..             ...    ...    ...       ...        ...          ...   \n",
       "190            FWD     22   48.0       2.0          1   267.371543   \n",
       "191            FWD     21  247.0       7.0          1  1374.077651   \n",
       "192  4-Wheel Drive      4   32.0       1.0          1   176.709702   \n",
       "193  4-Wheel Drive     21  148.0       0.0          1   816.760347   \n",
       "194            FWD     18  188.0       6.0          1  1037.440123   \n",
       "\n",
       "     favorite_rate fav_per_view  \n",
       "0         0.000000     0.000000  \n",
       "1        17.502279     0.034483  \n",
       "2         0.000000     0.000000  \n",
       "3         0.000000     0.000000  \n",
       "4         8.535019     0.038462  \n",
       "..             ...          ...  \n",
       "190      11.140481     0.041667  \n",
       "191      38.941472     0.028340  \n",
       "192       5.522178     0.031250  \n",
       "193       0.000000     0.000000  \n",
       "194      33.109791     0.031915  \n",
       "\n",
       "[195 rows x 31 columns]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars_df = pd.read_csv('data/all_cars_UPDATED.csv')\n",
    "cars_df = update_listing_info(cars_df)\n",
    "cars_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a multi-page test using carscraper function\n",
    "\n",
    "# set cap for number of search pages to load (i.e. pages with up to 96 listings)\n",
    "maxpg = 2\n",
    "\n",
    "# Define root url for KSL cars\n",
    "rooturl = \"https://cars.ksl.com\"\n",
    "\n",
    "# Note the url below specifies that we're looking for 96 per page and the default sort of newest to oldest posting\n",
    "# This note about newest to oldest is useful so that we can avoid scraping repeat listings based on their timestamps\n",
    "# Also note that this url does NOT have a page number associated with it. This is added in the while loop below\n",
    "lurl = \"https://cars.ksl.com/search/newUsed/Used;Certified/perPage/96/page/\"\n",
    "\n",
    "count = 0\n",
    "all_cars = []\n",
    "while count < maxpg:\n",
    "    url = lurl + str(count)\n",
    "#     curr_cars, moreresults = carscraper(url, rooturl, 0)\n",
    "    \n",
    "#     curr_cars, moreresults = carscraperproxy(url, rooturl, 0)\n",
    "    try:\n",
    "        curr_cars, moreresults, proxydict = carscraperproxy(url, rooturl, 0, refreshmin = 15, proxydict = proxydict)\n",
    "    except:\n",
    "        curr_cars, moreresults, proxydict = carscraperproxy(url, rooturl, 0, refreshmin = 15)\n",
    "    \n",
    "    count += 1    \n",
    "#     print(f'More results? {moreresults}')\n",
    "    if type(curr_cars) is pd.core.frame.DataFrame: # make sure real data was returned\n",
    "        try:\n",
    "            all_cars = pd.concat([curr_cars, all_cars], ignore_index=True)\n",
    "        except:\n",
    "            all_cars = curr_cars\n",
    "    else:\n",
    "        print('No car data found!')\n",
    "    \n",
    "all_cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy code to figure out how to find which proxies are new when refreshing list\n",
    "set1 = set(['a','b','c','d','e'])\n",
    "set2 = set(['b','c','e','f','h'])\n",
    "# set2 = set(['a','b','c','d','e'])\n",
    "newset = set2.difference(set1) # return elements in set2 that aren't in set1\n",
    "if newset:\n",
    "    print(newset)\n",
    "else:\n",
    "    print('No new elements in set 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding a way to try out a new proxy without moving on to the next href until success\n",
    "\n",
    "url = \"https://cars.ksl.com/search/newUsed/Used;Certified/perPage/96/page/0\"\n",
    "# url = \"https://httpbin.org/ip\"\n",
    "\n",
    "# user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'\n",
    "\n",
    "# the following were pulled manually on 3/12/20 from https://www.whatismybrowser.com/guides/the-latest-user-agent/\n",
    "user_agents = ['Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36',\n",
    "               'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36',\n",
    "               'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36',\n",
    "               'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:54.0) Gecko/20100101 Firefox/74.0',\n",
    "               'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.13; rv:61.0) Gecko/20100101 Firefox/74.0',\n",
    "               'Mozilla/5.0 (X11; Linux i586; rv:31.0) Gecko/20100101 Firefox/74.0',\n",
    "               'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0 Safari/605.1.15',\n",
    "               'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36 Edg/80.0.361.62',\n",
    "               'Mozilla/5.0 (Windows NT 10.0; Trident/7.0; rv:11.0) like Gecko']\n",
    "\n",
    "if not proxies:\n",
    "    proxies = generateProxies()\n",
    "\n",
    "# make a dictionary of proxies and user-agents using dictionary comprehension\n",
    "proxydict = {i:random.choice(user_agents) for i in proxies}\n",
    "\n",
    "proxy_pool = cycle(proxydict)\n",
    "currproxy = next(proxy_pool)\n",
    "\n",
    "attempts = len(proxies) # for now, limit the total number of attempts to one per proxy. This will prevent endless while loop\n",
    "for i in range(5):\n",
    "    chkproxy = 1\n",
    "    while chkproxy and attempts:\n",
    "        try:\n",
    "            resp = requests.get(url,proxies={\"http\":currproxy, \"https\":currproxy},headers={'User-Agent': proxydict[currproxy]}, timeout=20)\n",
    "            html = resp.content\n",
    "            print()\n",
    "            if url == \"https://httpbin.org/ip\":\n",
    "                print(html)\n",
    "            else:\n",
    "                pgsoup = BeautifulSoup(html)\n",
    "                links = pgsoup.select(\"div.title > a.link\") # grab all 96 (or up to 96) links\n",
    "                print(f'Number of links found: {len(links)}')\n",
    "            chkproxy = 0\n",
    "        except:\n",
    "            prevproxy = currproxy\n",
    "            currproxy = next(proxy_pool)\n",
    "            print(f'Proxy error for {prevproxy}! Next up is {currproxy}')\n",
    "            attempts -= 1\n",
    "            print(f'Attempts remaining: {attempts}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(proxydict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with proxy and user-agent combos\n",
    "\n",
    "url = \"https://cars.ksl.com/search/newUsed/Used;Certified/perPage/96/page/0\"\n",
    "# url = \"https://httpbin.org/ip\"\n",
    "\n",
    "# user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'\n",
    "\n",
    "# the following were pulled manually on 3/12/20 from https://www.whatismybrowser.com/guides/the-latest-user-agent/\n",
    "user_agents = ['Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36',\n",
    "               'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36',\n",
    "               'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36',\n",
    "               'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:54.0) Gecko/20100101 Firefox/74.0',\n",
    "               'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.13; rv:61.0) Gecko/20100101 Firefox/74.0',\n",
    "               'Mozilla/5.0 (X11; Linux i586; rv:31.0) Gecko/20100101 Firefox/74.0',\n",
    "               'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0 Safari/605.1.15',\n",
    "               'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36 Edg/80.0.361.62',\n",
    "               'Mozilla/5.0 (Windows NT 10.0; Trident/7.0; rv:11.0) like Gecko']\n",
    "\n",
    "proxies = generateProxies()\n",
    "\n",
    "# make a dictionary of proxies and user-agents using dictionary comprehension\n",
    "proxydict = {i:random.choice(user_agents) for i in proxies}\n",
    "\n",
    "proxy_pool = cycle(proxydict)\n",
    "for i in range(30):\n",
    "    currproxy = next(proxy_pool)\n",
    "    try:\n",
    "        resp = requests.get(url,proxies={\"http\":currproxy, \"https\":currproxy},headers={'User-Agent': proxydict[currproxy]}, timeout=15)\n",
    "        html = resp.content\n",
    "        print()\n",
    "        if url == \"https://httpbin.org/ip\":\n",
    "            print(html)\n",
    "        else:\n",
    "            pgsoup = BeautifulSoup(html)\n",
    "            links = pgsoup.select(\"div.title > a.link\") # grab all 96 (or up to 96) links\n",
    "            print(f'Number of links found: {len(links)}')\n",
    "        print(f'Success! proxy used: {currproxy}')\n",
    "        print()\n",
    "    except:\n",
    "        print(f'Proxy error! proxy used: {currproxy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figuring out number of photos on listing page\n",
    "# url = 'https://cars.ksl.com/listing/6269343' # 19 photos (seller)\n",
    "# url = 'https://cars.ksl.com/listing/6101875' # 26 photos (dealer)\n",
    "# url = 'https://cars.ksl.com/listing/6304284' # no photos (dealer)\n",
    "url = 'https://cars.ksl.com/listing/6302069' # 1 photo (seller)\n",
    "\n",
    "user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36'\n",
    "\n",
    "resp = requests.get(url, headers = {'User-Agent': user_agent})\n",
    "lsthtml = resp.content\n",
    "lstsoup = BeautifulSoup(lsthtml)\n",
    "    \n",
    "if lstsoup.select('div.slider-uninitialized > p'):\n",
    "    picstr = lstsoup.select('div.slider-uninitialized > p')[0].text.strip()\n",
    "    n_pics = int(re.search('(\\d+)',picstr).group())\n",
    "else:\n",
    "    if lstsoup.find(id='widgetPhoto').p:\n",
    "        picstr = lstsoup.find(id='widgetPhoto').p.text.strip()\n",
    "        n_pics = int(re.search('(\\d+)',picstr).group())\n",
    "    else:\n",
    "        n_pics = 0\n",
    "        \n",
    "print(f'Number of photos found: {n_pics}')\n",
    "print(type(n_pics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Playing around with determining if car listing is still good (or if it's been removed)\n",
    "testurl = \"https://cars.ksl.com/listing/9999999\"\n",
    "\n",
    "user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36'\n",
    "\n",
    "resp = requests.get(testurl, headers = {'User-Agent': user_agent})\n",
    "lsthtml = resp.content\n",
    "lstsoup = BeautifulSoup(lsthtml)\n",
    "\n",
    "if lstsoup.title.text.strip().lower() == 'not found':\n",
    "    print('bad link')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Playing around with checking time to see when to refresh proxy list\n",
    "tstart = time.time()\n",
    "time.sleep(5) # time in seconds\n",
    "tend = time.time() - tstart\n",
    "tend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Playing around with timestamps for use when checking for new data\n",
    "\n",
    "print(datetime.fromtimestamp(all_cars['timestamp'][0]).isoformat())\n",
    "print(datetime.now())\n",
    "currtime = time.time()\n",
    "print(currtime)\n",
    "print(datetime.fromtimestamp(currtime).isoformat())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below this is old code, some of which is no longer useful. Proceed with caution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "### DEPRECATED CODE: All functionality built into new optional proxy-based function ###\n",
    "#######################################################################################\n",
    "\n",
    "# Make a function for the scraping done for each search page\n",
    "\n",
    "def carscraper(url, rooturl, maxts):\n",
    "    '''INPUTS:\n",
    "    url should be of the form \"https://cars.ksl.com/search/newUsed/Used;Certified/perPage/96/page/0\"\n",
    "    rooturl should be something like \"https://cars.ksl.com\"\n",
    "    maxts is the maximum timestamp of the all_cars repository\n",
    "    \n",
    "    ***NOTE: This function is meant to work with original IP address (as opposed to proxy) and a single spoofed user-agent'''\n",
    "    \n",
    "    # Need to spoof a user-agent in order to get past crawler block\n",
    "    user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36'\n",
    "\n",
    "    resp = requests.get(url, headers = {'User-Agent': user_agent})\n",
    "    html = resp.content\n",
    "    pgsoup = BeautifulSoup(html)\n",
    "    \n",
    "    # Check if there are additional pages of results\n",
    "    if pgsoup.find(\"a\", {\"title\" : \"Go forward 1 page\"}):\n",
    "        moreresults = 1\n",
    "    else:\n",
    "        moreresults = 0\n",
    "    \n",
    "    links = pgsoup.select(\"div.title > a.link\") # grab all 96 (or up to 96) links\n",
    "    tstamps = pgsoup.select(\"div.listing-detail-line script\") # grab all 96 (or up to 96) timestamps\n",
    "\n",
    "    # Loop through links and scrape data for each new listing\n",
    "    all_cars = []\n",
    "    with progressbar.ProgressBar(max_value=len(links)) as bar:\n",
    "        for idx, link in enumerate(links): # *** only load first x results for now to avoid ban before implementing spoofing\n",
    "\n",
    "            # Reset all fields to None before next loop\n",
    "            price=year=make=model=body=mileage=title_type=city=state=seller=None\n",
    "            trim=ext_color=int_color=transmission=liters=cylinders=fuel_type=n_doors=ext_condition=int_condition=drive_type=None\n",
    "\n",
    "            # We're going to want to strip the \"?ad_cid=[number]\" from the end of these links as they're not needed to load the page properly\n",
    "            # Regular expressions should come in handy here\n",
    "\n",
    "            cutidx = re.search('(\\?ad_cid=.+)',link['href']).start()\n",
    "            currlink = link['href'][:cutidx]\n",
    "\n",
    "            # Somewhere here we should do a check to make sure that the timestamp for currlink is newer than our newest file in our repository\n",
    "            # That is, compare the timestamps with a simple conditional, where if the conditional is not met, this loop breaks to avoid useless computation time\n",
    "\n",
    "            # Open listing link and pull html from it\n",
    "            fulllink = '/'.join([rooturl.rstrip('/'), currlink.lstrip('/')])\n",
    "\n",
    "            resp = requests.get(fulllink, headers = {'User-Agent': user_agent})\n",
    "            lsthtml = resp.content\n",
    "            lstsoup = BeautifulSoup(lsthtml)\n",
    "            \n",
    "            # Check if link is still good (i.e. listing is still active)\n",
    "            if lstsoup.title.text.strip().lower() == 'not found':\n",
    "                print('Bad link. Skipping...')\n",
    "                bar.update(idx)\n",
    "            else:\n",
    "\n",
    "                # Get timestamp\n",
    "                tstamp = int(re.search('(\\d+)',tstamps[idx].text).group(0))\n",
    "\n",
    "                # Check if timestamp is newer than maxts\n",
    "                if tstamp <= maxts:\n",
    "                    print('************ Found end of new data ************')\n",
    "#                     print(f'var type of all_cars is: {type(all_cars)}')\n",
    "                    moreresults = 0\n",
    "                    break\n",
    "#                 else:\n",
    "#                     print(f'New car found: {idx} in link {fulllink}')\n",
    "\n",
    "                # Get listing price\n",
    "                price = lstsoup.select('h3.price')[0].text.strip().replace('$','').replace(',','')\n",
    "\n",
    "                # Get seller's location\n",
    "                if lstsoup.select('h2.location > a'):\n",
    "                    location = lstsoup.select('h2.location > a')[0].text.strip()\n",
    "                    city, state = location.split(',')\n",
    "                    city = city.strip()\n",
    "                    state = state.strip()\n",
    "\n",
    "                # Get seller type (dealer or owner)\n",
    "                sellerstr = lstsoup.select('div.fsbo')[0].text.strip()\n",
    "                if re.search('(Dealer)', sellerstr):\n",
    "                    seller = 'Dealer'\n",
    "                elif re.search('(Owner)', sellerstr):\n",
    "                    seller = 'Owner'\n",
    "                    \n",
    "                # Get number of photos\n",
    "                if lstsoup.select('div.slider-uninitialized > p'):\n",
    "                    picstr = lstsoup.select('div.slider-uninitialized > p')[0].text.strip()\n",
    "                    n_pics = int(re.search('(\\d+)',picstr).group())\n",
    "                else:\n",
    "                    if lstsoup.find(id='widgetPhoto').p:\n",
    "                        picstr = lstsoup.find(id='widgetPhoto').p.text.strip()\n",
    "                        n_pics = int(re.search('(\\d+)',picstr).group())\n",
    "                    else:\n",
    "                        n_pics = 0\n",
    "\n",
    "                # Get table of car specs\n",
    "                specs = lstsoup.select('ul.listing-specifications')\n",
    "\n",
    "                for li in specs[0].find_all('li'):\n",
    "                    lititle = li.select('span.title')[0].text.strip().strip(':')\n",
    "                    livalue = li.select('span.value')[0].text.strip().strip(':')\n",
    "\n",
    "                    if livalue.lower() == 'not specified':\n",
    "                        livalue = None\n",
    "\n",
    "                    # Now a bunch of if-else statements to determine which column to add data to\n",
    "                    # There might be a more sophisticated way to do this, perhaps with a tuple or a dictionary?\n",
    "                    if lititle.lower() == 'year':\n",
    "                        if livalue:\n",
    "                            year = int(livalue)\n",
    "                        else:\n",
    "                            year = livalue\n",
    "                    elif lititle.lower() == 'make':\n",
    "                        make = livalue\n",
    "                    elif lititle.lower() == 'model':\n",
    "                        model = livalue\n",
    "                    elif lititle.lower() == 'body':\n",
    "                        body = livalue\n",
    "                    elif lititle.lower() == 'mileage':\n",
    "                        if livalue:\n",
    "                            mileage = int(livalue.replace(',',''))\n",
    "                        else:\n",
    "                            mileage = livalue\n",
    "                    elif lititle.lower() == 'title type':\n",
    "                        title_type = livalue\n",
    "\n",
    "                    # Below this are non-required specs    \n",
    "                    elif lititle.lower() == 'trim':\n",
    "                        trim = livalue\n",
    "                    elif lititle.lower() == 'exterior color':\n",
    "                        if livalue:\n",
    "                            ext_color = livalue.lower()\n",
    "                        else:\n",
    "                            ext_color = livalue\n",
    "                    elif lititle.lower() == 'interior color':\n",
    "                        if livalue:\n",
    "                            int_color = livalue.lower()\n",
    "                        else:\n",
    "                            int_color = livalue\n",
    "                    elif lititle.lower() == 'transmission':\n",
    "                        transmission = livalue\n",
    "                    elif lititle.lower() == 'liters':\n",
    "                        try:\n",
    "                            liters = float(livalue)\n",
    "                        except:\n",
    "                            if livalue:\n",
    "                                str1 = re.search('^(.*?)L',livalue).group(0).strip().replace(' ','')\n",
    "                                if re.search('^(\\D+)',str1):\n",
    "                                    idxend = re.search('^(\\D+)',str1).end()\n",
    "                                    livalue = str1[idxend:-1]\n",
    "                                    if re.search('(\\D+)',livalue): # check if still other pollutants\n",
    "                                        idxend = re.search('(\\D+)',livalue).end()\n",
    "                                        livalue = livalue[idxend:]\n",
    "                                else:\n",
    "                                    livalue = str1[:-1]\n",
    "                                try:\n",
    "                                    livalue = float(livalue)\n",
    "                                except:\n",
    "                                    print(url)\n",
    "                                    print('****')\n",
    "                                    print(link)\n",
    "                            else:\n",
    "                                liters = livalue\n",
    "                    elif lititle.lower() == 'cylinders':\n",
    "                        if livalue:\n",
    "                            cylinders = int(livalue)\n",
    "                        else:\n",
    "                            cylinders = livalue\n",
    "                    elif lititle.lower() == 'fuel type':\n",
    "                        fuel_type = livalue\n",
    "                    elif lititle.lower() == 'number of doors':\n",
    "                        if livalue:\n",
    "                            n_doors = int(livalue)\n",
    "                        else:\n",
    "                            n_doors = livalue\n",
    "                    elif lititle.lower() == 'exterior condition':\n",
    "                        ext_condition = livalue\n",
    "                    elif lititle.lower() == 'interior condition':\n",
    "                        int_condition = livalue\n",
    "                    elif lititle.lower() == 'drive type':\n",
    "                        drive_type = livalue\n",
    "                    elif (lititle.lower() == 'vin') | (lititle.lower() == 'stock number') | (lititle.lower() == 'dealer license'):\n",
    "                        None # Don't want to save these\n",
    "                    else:\n",
    "                        None\n",
    "                        print(f'Unmatched param {lititle}: {livalue}') # <-- could take advantage of some or all of these\n",
    "\n",
    "                curr_car = pd.DataFrame({\"timestamp\":[tstamp],\n",
    "                                         \"price\":[price],\n",
    "                                         \"year\":[year],\n",
    "                                         \"make\":[make],\n",
    "                                         \"model\":[model],\n",
    "                                         \"body\":[body],\n",
    "                                         \"mileage\":[mileage],\n",
    "                                         \"title_type\":[title_type],\n",
    "                                         \"city\":[city],\n",
    "                                         \"state\":[state],\n",
    "                                         \"seller\":[seller],\n",
    "                                         \"trim\":[trim],\n",
    "                                         \"ext_color\":[ext_color],\n",
    "                                         \"int_color\":[int_color],\n",
    "                                         \"transmission\":[transmission],\n",
    "                                         \"liters\":[liters],\n",
    "                                         \"cylinders\":[cylinders],\n",
    "                                         \"fuel_type\":[fuel_type],\n",
    "                                         \"n_doors\":[n_doors],\n",
    "                                         \"ext_condition\":[ext_condition],\n",
    "                                         \"int_condition\":[int_condition],\n",
    "                                         \"drive_type\":[drive_type],\n",
    "                                         \"n_pics\":[n_pics]})\n",
    "                try:\n",
    "                    all_cars = pd.concat([curr_car, all_cars])\n",
    "                except:\n",
    "                    all_cars = curr_car\n",
    "\n",
    "                bar.update(idx)\n",
    "\n",
    "    if type(all_cars) is pd.core.frame.DataFrame: # make sure that some data was actually scraped\n",
    "        all_cars = all_cars.reset_index()\n",
    "        del all_cars['index']\n",
    "        all_cars.fillna(value=pd.np.nan, inplace=True)\n",
    "    return all_cars, moreresults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with using proxy IPs from above\n",
    "\n",
    "# url = \"https://cars.ksl.com/search/newUsed/Used;Certified/perPage/96/page/0\"\n",
    "url = \"https://httpbin.org/ip\"\n",
    "\n",
    "user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'\n",
    "\n",
    "for idx, proxy in enumerate(proxies):\n",
    "#     user_agent = UAlist[idx]\n",
    "    try:\n",
    "        resp = requests.get(url,proxies={\"http\":proxy, \"https\":proxy},headers={'User-Agent': user_agent}, timeout=10)\n",
    "        print(resp.content)\n",
    "        print(f'Success! proxy used: {proxy}')\n",
    "    except:\n",
    "        print(f'Proxy error! proxy used: {proxy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "######### MIGHT GET BY WITH JUST PROXY IPs #########\n",
    "####################################################\n",
    "\n",
    "### Revisit this in the future if it becomes an issue\n",
    "### For time being, just use a handful of user-agents\n",
    "\n",
    "# Get list of user-agents\n",
    "\n",
    "### Need to use API at some point rather than crawl/scrape since you can get 500 user-agents for free per month...and my IP got banned\n",
    "### username: automodeals\n",
    "### pw: kslclass123\n",
    "### API documentation: https://developers.whatismybrowser.com/api/docs/v2/\n",
    "\n",
    "API_key = '5ecab60888f7aebfbc4aad5850de52fa'\n",
    "\n",
    "UAurl = \"https://developers.whatismybrowser.com/useragents/explore/software_name/chrome/\"\n",
    "\n",
    "resp = requests.get(UAurl)\n",
    "UAhtml = resp.content\n",
    "UAsoup = BeautifulSoup(UAhtml)\n",
    "\n",
    "UAlist = []\n",
    "matches = UAsoup.select(\"table.table-useragents td.useragent\")\n",
    "for match in matches[:len(proxies)]: # only get as many user-agents are there are proxies. Dangerous to use more than one user-agent per IP\n",
    "    UAlist.append(match.find('a').text.strip())\n",
    "random.shuffle(UAlist)\n",
    "UAlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use user-agents API (example from https://github.com/whatismybrowser/api-v2-sample-code/blob/master/sample-code/python-3.6/user_agent_parse.py)\n",
    "\n",
    "API_key = '5ecab60888f7aebfbc4aad5850de52fa'\n",
    "\n",
    "headers = {'X-API-KEY': API_key}\n",
    "# UAurl = \"https://api.whatismybrowser.com/api/v2/user_agent_database_dump_url\"\n",
    "# UAurl = \"https://api.whatismybrowser.com/api/v2/user_agent_database_search\"\n",
    "\n",
    "# The code below works for POSTing data, but we want to GET data\n",
    "\n",
    "UAurl = \"https://api.whatismybrowser.com/api/v2/user_agent_parse\"\n",
    "\n",
    "post_data = {\n",
    "    \"user_agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3282.167 Safari/537.36\",\n",
    "}\n",
    "\n",
    "result = requests.post(UAurl, data=json.dumps(post_data), headers=headers)\n",
    "result # if result is 200, then success!\n",
    "result.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of working live html parser without crawler block and user-agent spoof\n",
    "\n",
    "# url = \"http://www.python.org\"\n",
    "# resp = requests.get(url)\n",
    "# html = resp.content\n",
    "# print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "####### DEPRECATED AS OF MARCH 12, 2020 #########\n",
    "#################################################\n",
    "\n",
    "# TCH: Many functionalities implemented in the carscraper function have not been copied over to this cell block\n",
    "\n",
    "\n",
    "### Working example for a SINGLE KSL search results page\n",
    "\n",
    "# maxresults = 20 # Set max number of listings to parse (per search results page)\n",
    "\n",
    "# # Define root url for KSL cars\n",
    "# rooturl = \"https://cars.ksl.com\"\n",
    "\n",
    "# # Note the url below specifies that we're looking for 96 per page and the default sort of newest to oldest posting\n",
    "# # This note about newest to oldest is useful so that we can avoid scraping repeat listings based on their timestamps\n",
    "# url = \"https://cars.ksl.com/search/newUsed/Used;Certified/perPage/96/page/0\"\n",
    "\n",
    "# # Need to spoof a user-agent in order to get past crawler block\n",
    "# user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36'\n",
    "\n",
    "# # Note: The above user_agent might need to be rotated (along with IP) to avoid IP ban\n",
    "# # Example found on https://www.scrapehero.com/how-to-fake-and-rotate-user-agents-using-python-3/\n",
    "\n",
    "# all_cars = []\n",
    "\n",
    "# # Open live page (as opposed to downloaded)\n",
    "# resp = requests.get(url, headers = {'User-Agent': user_agent})\n",
    "# html = resp.content\n",
    "# pgsoup = BeautifulSoup(html)\n",
    "# lastpg = int(pgsoup.find(attrs={\"title\": \"Go to last page\"}).text.strip()) # Note that this is 1 more than number from href for this page\n",
    "# # print(f'Total number of search results pages: {lastpg}')\n",
    "# # print()\n",
    "\n",
    "# links = pgsoup.select(\"div.title > a.link\") # grab all 96 (or up to 96) links\n",
    "# # print(f'Total number of links found on current page: {len(links)}')\n",
    "# tstamps = pgsoup.select(\"div.listing-detail-line script\") # grab all 96 (or up to 96) timestamps\n",
    "# # print(f'Total number of timestamps found on current page: {len(tstamps)}')\n",
    "\n",
    "# # print()\n",
    "\n",
    "# # for tstamp in tstamps:\n",
    "# #     print(int(re.search('(\\d+)',tstamp.text).group(0))) # <-- This is WORKING code to extract timestamp for each listing from search page\n",
    "# # print()\n",
    "\n",
    "# print(f'Limiting Subsequent Listing Results to {maxresults}')\n",
    "\n",
    "# # Loop through links and scrape data for each new listing\n",
    "# with progressbar.ProgressBar(max_value=maxresults) as bar:\n",
    "#     for idx, link in enumerate(links[:maxresults]): # *** only load first x results for now to avoid ban before implementing spoofing\n",
    "\n",
    "#         # Reset all fields to None before next loop\n",
    "#         price=year=make=model=body=mileage=title_type=city=state=seller=None\n",
    "#         trim=ext_color=int_color=transmission=liters=cylinders=fuel_type=n_doors=ext_condition=int_condition=drive_type=None\n",
    "        \n",
    "#         # We're going to want to strip the \"?ad_cid=[number]\" from the end of these links as they're not needed to load the page properly\n",
    "#         # Regular expressions should come in handy here\n",
    "\n",
    "#         cutidx = re.search('(\\?ad_cid=.+)',link['href']).start()\n",
    "#         currlink = link['href'][:cutidx]\n",
    "\n",
    "#         # Somewhere here we should do a check to make sure that the timestamp for currlink is newer than our newest file in our repository\n",
    "#         # That is, compare the timestamps with a simple conditional, where if the conditional is not met, this loop breaks to avoid useless computation time\n",
    "\n",
    "#         # Open listing link and pull html from it\n",
    "#         fulllink = '/'.join([rooturl.rstrip('/'), currlink.lstrip('/')])\n",
    "\n",
    "#         resp = requests.get(fulllink, headers = {'User-Agent': user_agent})\n",
    "#         lsthtml = resp.content\n",
    "#         lstsoup = BeautifulSoup(lsthtml)\n",
    "\n",
    "#         # Get listing price\n",
    "#         price = lstsoup.select('h3.price')[0].text.strip().replace('$','').replace(',','')\n",
    "\n",
    "#         # Get seller's location\n",
    "#         location = lstsoup.select('h2.location > a')[0].text.strip()\n",
    "#         city, state = location.split(',')\n",
    "#         city = city.strip()\n",
    "#         state = state.strip()\n",
    "\n",
    "#         # Get seller type (dealer or owner)\n",
    "#         sellerstr = lstsoup.select('div.fsbo')[0].text.strip()\n",
    "#         if re.search('(Dealer)', sellerstr):\n",
    "#             seller = 'Dealer'\n",
    "#         elif re.search('(Owner)', sellerstr):\n",
    "#             seller = 'Owner'\n",
    "\n",
    "#         # Get timestamp\n",
    "#         tstamp = int(re.search('(\\d+)',tstamps[idx].text).group(0))\n",
    "\n",
    "#         # Get table of car specs\n",
    "#         specs = lstsoup.select('ul.listing-specifications')\n",
    "\n",
    "#         for li in specs[0].find_all('li'):\n",
    "#             lititle = li.select('span.title')[0].text.strip().strip(':')\n",
    "#             livalue = li.select('span.value')[0].text.strip().strip(':')\n",
    "            \n",
    "#             if livalue.lower() == 'not specified':\n",
    "#                 livalue = None\n",
    "\n",
    "#             # Now a bunch of if-else statements to determine which column to add data to\n",
    "#             # There might be a more sophisticated way to do this, perhaps with a tuple or a dictionary?\n",
    "#             if lititle.lower() == 'year':\n",
    "#                 if livalue:\n",
    "#                     year = int(livalue)\n",
    "#                 else:\n",
    "#                     year = livalue\n",
    "#             elif lititle.lower() == 'make':\n",
    "#                 make = livalue\n",
    "#             elif lititle.lower() == 'model':\n",
    "#                 model = livalue\n",
    "#             elif lititle.lower() == 'body':\n",
    "#                 body = livalue\n",
    "#             elif lititle.lower() == 'mileage':\n",
    "#                 if livalue:\n",
    "#                     mileage = int(livalue.replace(',',''))\n",
    "#                 else:\n",
    "#                     mileage = livalue\n",
    "#             elif lititle.lower() == 'title type':\n",
    "#                 title_type = livalue\n",
    "                \n",
    "#             # Below this are non-required specs    \n",
    "#             elif lititle.lower() == 'trim':\n",
    "#                 trim = livalue\n",
    "#             elif lititle.lower() == 'exterior color':\n",
    "#                 if livalue:\n",
    "#                     ext_color = livalue.lower()\n",
    "#                 else:\n",
    "#                     ext_color = livalue\n",
    "#             elif lititle.lower() == 'interior color':\n",
    "#                 if livalue:\n",
    "#                     int_color = livalue.lower()\n",
    "#                 else:\n",
    "#                     int_color = livalue\n",
    "#             elif lititle.lower() == 'transmission':\n",
    "#                 transmission = livalue\n",
    "#             elif lititle.lower() == 'liters':\n",
    "#                 try:\n",
    "#                     liters = float(livalue)\n",
    "#                 except:\n",
    "#                     if livalue:\n",
    "#                         str1 = re.search('^(.*?)L',livalue).group(0).strip().replace(' ','')\n",
    "#                         if re.search('^(\\D+)',str1):\n",
    "#                             idxend = re.search('^(\\D+)',str1).end()\n",
    "#                             livalue = str1[idxend:-1]\n",
    "#                         else:\n",
    "#                             livalue = str1[:-1]\n",
    "#                         livalue = float(livalue)\n",
    "#                     else:\n",
    "#                         liters = livalue\n",
    "#             elif lititle.lower() == 'cylinders':\n",
    "#                 if livalue:\n",
    "#                     cylinders = int(livalue)\n",
    "#                 else:\n",
    "#                     cylinders = livalue\n",
    "#             elif lititle.lower() == 'fuel type':\n",
    "#                 fuel_type = livalue\n",
    "#             elif lititle.lower() == 'number of doors':\n",
    "#                 if livalue:\n",
    "#                     n_doors = int(livalue)\n",
    "#                 else:\n",
    "#                     n_doors = livalue\n",
    "#             elif lititle.lower() == 'exterior condition':\n",
    "#                 ext_condition = livalue\n",
    "#             elif lititle.lower() == 'interior condition':\n",
    "#                 int_condition = livalue\n",
    "#             elif lititle.lower() == 'drive type':\n",
    "#                 drive_type = livalue\n",
    "#             elif (lititle.lower() == 'vin') | (lititle.lower() == 'stock number') | (lititle.lower() == 'dealer license'):\n",
    "#                 None # Don't want to save these\n",
    "#             else:\n",
    "#                 None\n",
    "#                 print(f'Unmatched param {lititle}: {livalue}') # <-- could take advantage of some or all of these\n",
    "\n",
    "#         curr_car = pd.DataFrame({\"timestamp\":[tstamp],\n",
    "#                                  \"price\":[price],\n",
    "#                                  \"year\":[year],\n",
    "#                                  \"make\":[make],\n",
    "#                                  \"model\":[model],\n",
    "#                                  \"body\":[body],\n",
    "#                                  \"mileage\":[mileage],\n",
    "#                                  \"title_type\":[title_type],\n",
    "#                                  \"city\":[city],\n",
    "#                                  \"state\":[state],\n",
    "#                                  \"seller\":[seller],\n",
    "#                                  \"trim\":[trim],\n",
    "#                                  \"ext_color\":[ext_color],\n",
    "#                                  \"int_color\":[int_color],\n",
    "#                                  \"transmission\":[transmission],\n",
    "#                                  \"liters\":[liters],\n",
    "#                                  \"cylinders\":[cylinders],\n",
    "#                                  \"fuel_type\":[fuel_type],\n",
    "#                                  \"n_doors\":[n_doors],\n",
    "#                                  \"ext_condition\":[ext_condition],\n",
    "#                                  \"int_condition\":[int_condition],\n",
    "#                                  \"drive_type\":[drive_type]})\n",
    "#         try:\n",
    "#             all_cars = pd.concat([all_cars, curr_car])\n",
    "#         except:\n",
    "#             all_cars = curr_car\n",
    "\n",
    "#         bar.update(idx)\n",
    "        \n",
    "# all_cars = all_cars.reset_index()\n",
    "# del all_cars['index']\n",
    "# all_cars.fillna(value=pd.np.nan, inplace=True)\n",
    "# all_cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "######## USE SELENIUM INSTEAD TO GET BIGGER LIST ########\n",
    "#########################################################\n",
    "\n",
    "# # Get list of proxy IPs\n",
    "\n",
    "# IPurl = \"https://free-proxy-list.net\"\n",
    "\n",
    "# resp = requests.get(IPurl)\n",
    "# IPhtml = resp.content\n",
    "# IPsoup = BeautifulSoup(IPhtml)\n",
    "\n",
    "# proxies = []\n",
    "# for tr in IPsoup.find(id='proxylisttable').find('tbody').find_all('tr'):\n",
    "#     tds = tr.find_all('td')\n",
    "#     if (tds[2].text.strip() == 'US') & (tds[6].text.strip() == 'yes') & (tds[4].text.strip() != 'transparent'):\n",
    "#         proxies.append(''.join(['http://', ':'.join([tds[0].text.strip(), tds[1].text.strip()])])) # grab the IP addresses matching the above criteria\n",
    "# random.shuffle(proxies)\n",
    "# proxies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "580.85px",
    "left": "1550px",
    "right": "20px",
    "top": "121px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
