{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import progressbar # if this isn't installed, use pip install progressbar2\n",
    "import requests\n",
    "from bs4 import BeautifulSoup # if this isn't installed, use pip install beautifulsoup4\n",
    "from selenium import webdriver # if not installed, do pip install selenium\n",
    "import random\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateProxies():\n",
    "    # Get list of US-based proxy IPs and ports using selenium\n",
    "\n",
    "    IPurl = \"https://www.us-proxy.org/\" # <-- the robots.txt file for this site allows full access for all user-agents\n",
    "\n",
    "    # Specify incognito options for Chrome\n",
    "    option = webdriver.ChromeOptions()\n",
    "    option.add_argument(\"--incognito\")\n",
    "    option.add_argument(\"--start-maximized\")\n",
    "\n",
    "    # Create new Chrome instance\n",
    "    browser = webdriver.Chrome(options=option)\n",
    "\n",
    "    # Minimize window\n",
    "#     browser.minimize_window()\n",
    "\n",
    "    # Go to desired website\n",
    "    IPurl = \"https://www.us-proxy.org/\" # <-- the robots.txt file for this site allows full access for all user-agents\n",
    "    browser.get(IPurl)\n",
    "\n",
    "    # Filter by https only\n",
    "    https_button = browser.find_elements_by_xpath(\"//*[@id='proxylisttable']/tfoot/tr/th[7]/select/option[3]\")[0]\n",
    "    https_button.click()\n",
    "\n",
    "    # Set to 80 results\n",
    "    maxnum_button = browser.find_elements_by_xpath(\"//*[@id='proxylisttable_length']/label/select/option[3]\")[0]\n",
    "    maxnum_button.click()\n",
    "\n",
    "    # Grab IP's and Ports from the resulting table\n",
    "    rows = browser.find_elements_by_xpath(\"//*[@id='proxylisttable']/tbody/tr\")\n",
    "\n",
    "    proxies = set() # using a set ensures there aren't duplicates\n",
    "    for row in rows:\n",
    "        row = row.text.split(' ')\n",
    "\n",
    "        if row[3].strip().lower() != 'transparent': # don't want to include our real proxy when navigating KSL\n",
    "            proxies.add(''.join(['http://', ':'.join([row[0].strip(), row[1].strip()])]))\n",
    "\n",
    "    # Close browser when done\n",
    "    browser.close()\n",
    "\n",
    "    return proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_listing_info(cars_df, **kwargs):\n",
    "    '''Updates a cars_dataframe with 6 new columns (views, favorites, \n",
    "    workingURL, view_rate, favorite_rate, fav_per_view). \n",
    "    REQUIRED INPUTS:\n",
    "    cars_df: data frame with information as pulled from carscraper()\n",
    "    VARIABLE INPUTS:\n",
    "    min_age: int specifying minimum age (days) listing must be before updating information. Default 3\n",
    "    min_last_pull: int specifying minimum time (days) since last pull for new information. Default 1\n",
    "    use_proxy: bool indicating to use a proxy. Default 0\n",
    "    proxy_dict: dictionary of proxy IPs and user agents'''\n",
    "    \n",
    "    # parse kwargs/set defaults\n",
    "    if 'min_age' in kwargs.keys():\n",
    "        if isinstance(kwargs['min_age'],int):\n",
    "            min_age = kwargs['min_age']\n",
    "        else:\n",
    "            raise TypeError(f'Expected int for min_age but got {type(kwargs[\"min_age\"])}.')\n",
    "    else:\n",
    "        min_age = 3\n",
    "    if 'min_last_pull' in kwargs.keys():\n",
    "        if isinstance(kwargs['min_last_pull'],int):\n",
    "            min_last_pull = kwargs['min_last_pull']\n",
    "        else:\n",
    "            raise TypeError(f'Expected int for min_last_pull but got {type(kwargs[\"min_last_pull\"])}.')\n",
    "    else:\n",
    "        min_last_pull = 1\n",
    "    if 'use_proxy' in kwargs.keys():\n",
    "        if isinstance(kwargs['use_proxy'],int) or isinstance(kwargs['use_proxy'],bool):\n",
    "            use_proxy = kwargs['use_proxy']\n",
    "        else:\n",
    "            raise TypeError(f'Expected int or bool for use_proxy but got {type(kwargs[\"use_proxy\"])}.')\n",
    "    else:\n",
    "        # default is to NOT use proxy\n",
    "        use_proxy = False\n",
    "    \n",
    "    # the following were pulled manually on 3/12/20 from https://www.whatismybrowser.com/guides/the-latest-user-agent/\n",
    "    user_agents = ['Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36',\n",
    "                   'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36',\n",
    "                   'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36',\n",
    "                   'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:54.0) Gecko/20100101 Firefox/74.0',\n",
    "                   'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.13; rv:61.0) Gecko/20100101 Firefox/74.0',\n",
    "                   'Mozilla/5.0 (X11; Linux i586; rv:31.0) Gecko/20100101 Firefox/74.0',\n",
    "                   'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0 Safari/605.1.15',\n",
    "                   'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36 Edg/80.0.361.62',\n",
    "                   'Mozilla/5.0 (Windows NT 10.0; Trident/7.0; rv:11.0) like Gecko']\n",
    "    if use_proxy:\n",
    "        # The following inputs are only useful when using a proxy\n",
    "        \n",
    "        if 'proxydict' in kwargs.keys():\n",
    "            if isinstance(kwargs['proxydict'],dict):\n",
    "                proxydict = kwargs['proxydict']\n",
    "            else:\n",
    "                print(f'Expected dict type for proxydict but got {type(kwargs[\"proxydict\"])}. Generating new proxydict...')\n",
    "                newproxies = generateProxies()\n",
    "                proxydict = {i:random.choice(user_agents) for i in newproxies}\n",
    "        else:\n",
    "            print('No proxydict found. Generating...')\n",
    "            newproxies = generateProxies()\n",
    "            proxydict = {i:random.choice(user_agents) for i in newproxies}\n",
    "\n",
    "        if 'refreshmin' in kwargs.keys():\n",
    "            if isinstance(kwargs['refreshmin'],int) or isinstance(kwargs['refreshmin'],float):\n",
    "                refreshmin = kwargs['refreshmin']\n",
    "            else:\n",
    "                refreshmin = 15\n",
    "                print(f'Expected int or float for refreshmin but got {type(kwargs[\"refreshmin\"])}. Set to default value of {refreshmin}.')\n",
    "        else:\n",
    "            refreshmin = 15\n",
    "            print(f'No refreshmin found. Set to default value of {refreshmin}.')\n",
    "                      \n",
    "    if use_proxy:\n",
    "        tstart = time.time() # set a start time to use for refreshing proxy list (if needed)    \n",
    "\n",
    "        if 'currproxy' in kwargs.keys():\n",
    "            if isinstance(kwargs['currproxy'],str):\n",
    "                currproxy = kwargs['currproxy']\n",
    "            else:\n",
    "                proxy_pool = cycle(proxydict) # make a pool of proxies \n",
    "                currproxy = next(proxy_pool) # grab the next proxy in cycle\n",
    "        else:\n",
    "            proxy_pool = cycle(proxydict) # make a pool of proxies \n",
    "            currproxy = next(proxy_pool) # grab the next proxy in cycle     \n",
    "\n",
    "    # new columns to add\n",
    "    cars_df['views'] = np.NaN\n",
    "    cars_df['favorites'] = np.NaN\n",
    "    cars_df['workingURL'] = 1\n",
    "    cars_df['view_rate'] = np.NaN\n",
    "    cars_df['favorite_rate'] = np.NaN\n",
    "    cars_df['fav_per_view'] = np.NaN\n",
    "    \n",
    "    # conversions to datetime\n",
    "    orig_dates = cars_df['post_date']\n",
    "    cars_df['post_date'] = pd.to_datetime(cars_df['post_date'])\n",
    "    cars_df['lastpull_ts'] = pd.to_datetime(cars_df['lastpull_ts'], unit = 's')\n",
    "\n",
    "    # find ads more than x days old (time.time() is in seconds)\n",
    "    curr_time = pd.to_datetime(time.time(),unit='s')\n",
    "    min_dt = pd.to_timedelta(min_age*60*60*24, unit='seconds') # time in seconds for use with datetime\n",
    "\n",
    "    old_ads = cars_df['post_date'] < (curr_time - min_dt)\n",
    "\n",
    "    # find ads that haven't been pulled for more than x days\n",
    "    min_last_pull_dt = pd.to_timedelta(min_last_pull*60*60*24, unit='seconds') # time in seconds for use with datetime\n",
    "    no_recent_update = cars_df['lastpull_ts'] < (curr_time - min_last_pull_dt)\n",
    "\n",
    "    # subselect ads that need updating based on previous criteria and having a working URL last time it was checked\n",
    "    cars_need_update = cars_df[old_ads & no_recent_update & cars_df['workingURL']]\n",
    "\n",
    "    user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'\n",
    "\n",
    "    # iterate through, pulling new information from each ad\n",
    "    last_pull = []\n",
    "    views = []\n",
    "    favorites = []\n",
    "    working_url = []\n",
    "    with progressbar.ProgressBar(max_value=len(cars_need_update.index)) as bar:\n",
    "        for i, ad in cars_need_update.iterrows():\n",
    "            if use_proxy:\n",
    "                attempts = len(proxydict) # for now, limit the total number of attempts to one per proxy. This will prevent endless while loop\n",
    "                chkproxy = 1\n",
    "                while chkproxy:\n",
    "                    if (time.time() - tstart) > 60*refreshmin: # check if it's been more than refreshmin minutes since proxy_pool updated\n",
    "                        print('Refreshing proxy pool...')\n",
    "                        tstart = time.time()\n",
    "\n",
    "                        currproxies = set(proxydict.keys())\n",
    "                        newproxies = generateProxies()\n",
    "                        newproxies = newproxies.difference(currproxies)\n",
    "\n",
    "                        if newproxies:\n",
    "                            newdict = {i:random.choice(user_agents) for i in newproxies}\n",
    "                            proxydict.update(newdict)\n",
    "                            proxy_pool = cycle(proxydict)\n",
    "                            currproxy = next(proxy_pool)\n",
    "                            print('Proxy pool updated!')\n",
    "\n",
    "                    try:\n",
    "                        ad_response = requests.get(ad['link'],proxies={\"http\":currproxy, \"https\":currproxy},headers={'User-Agent': proxydict[currproxy]}, timeout=20)\n",
    "                        print(f'Proxy success for {currproxy}')\n",
    "                        print()\n",
    "                        chkproxy = 0\n",
    "                        attempts += 1\n",
    "                    except:\n",
    "                        prevproxy = currproxy\n",
    "                        currproxy = next(proxy_pool)\n",
    "                        print(f'Proxy error for {prevproxy}! Next up is {currproxy}')\n",
    "                        attempts -= 1\n",
    "                        print(f'Attempts remaining: {attempts}')\n",
    "            else:\n",
    "                ad_response = requests.get(ad['link'], headers = {'User-Agent': user_agent})\n",
    "            \n",
    "            pull_ts = pd.to_datetime(time.time(), unit='s')\n",
    "            last_pull.append(pull_ts)\n",
    "            ad_soup = BeautifulSoup(ad_response.content)\n",
    "\n",
    "            # Check if link is still good (i.e. listing is still active)\n",
    "            if ad_soup.title.text.strip().lower() == 'not found':\n",
    "                working_url.append(0)\n",
    "                views.append(None)\n",
    "                favorites.append(None)\n",
    "            else:\n",
    "                working_url.append(1)\n",
    "\n",
    "                # get views\n",
    "                viewcount = int(ad_soup.select('span.vdp-info-value')[1].text.split()[0])\n",
    "                views.append(viewcount)\n",
    "\n",
    "                # get favorites\n",
    "                favoritecount = int(ad_soup.select('span.vdp-info-value')[2].text.split()[0])\n",
    "                favorites.append(favoritecount)\n",
    "            bar.update(i)\n",
    "            \n",
    "    cars_updated = cars_need_update\n",
    "    cars_updated['views'] = views\n",
    "    cars_updated['favorites'] = favorites\n",
    "    cars_updated['lastpull_ts'] = last_pull\n",
    "    cars_updated['workingURL'] = working_url\n",
    "    cars_updated['fav_per_view'] = cars_updated['favorites'] / cars_updated['views']\n",
    "    # rates calculated per day\n",
    "    cars_updated['view_rate'] = cars_updated['views'] / ((cars_updated['lastpull_ts'] - cars_updated['post_date']).dt.total_seconds()*60*60*24)\n",
    "    cars_updated['favorite_rate'] = cars_updated['favorites'] / ((cars_updated['lastpull_ts'] - cars_updated['post_date']).dt.total_seconds()*60*60*24)\n",
    "\n",
    "    cars_df.update(cars_updated)\n",
    "    \n",
    "    # update timestamps to replicate original state\n",
    "    cars_df['lastpull_ts'] = (cars_df['lastpull_ts'] - datetime.datetime(1970,1,1)).dt.total_seconds().astype(int)\n",
    "    cars_df['post_date'] = orig_dates\n",
    "    \n",
    "    return cars_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_df = pd.read_csv('data/all_cars.csv')\n",
    "\n",
    "# cars_df = cars_df.iloc[:5,:]\n",
    "cars_df = update_listing_info(cars_df,min_age=0,min_last_pull=0,use_proxy=True)\n",
    "cars_df.to_csv('data/all_cars_view_fav.csv',index=False)\n",
    "\n",
    "cars_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "337.85px",
    "left": "1550px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
